{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Alternate_train-val-test.ipynb","provenance":[{"file_id":"13Lyv-U5K86wr9D9ZhW9qnUJ4FU---gHI","timestamp":1652046214252}],"collapsed_sections":[],"authorship_tag":"ABX9TyPTgraPMvP/SpEGsXfkqGmF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":379},"id":"BxvyBn79x4HJ","outputId":"8ac60656-c73c-4f3e-bf7b-43854d9dd3d8","executionInfo":{"status":"ok","timestamp":1652124319729,"user_tz":240,"elapsed":2541423,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a41a5e56-8f25-498a-bf1e-dce6f985f81c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a41a5e56-8f25-498a-bf1e-dce6f985f81c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Y_cleaned_attempt_new.npy to Y_cleaned_attempt_new.npy\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-cd142c6d-a13f-4571-afcf-0146b0af2ec2\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-cd142c6d-a13f-4571-afcf-0146b0af2ec2\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving X_cleaned_attempt_new.npy to X_cleaned_attempt_new.npy\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-7d4e0a97-6654-461e-b4e1-605227153f53\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-7d4e0a97-6654-461e-b4e1-605227153f53\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Y_test.npy to Y_test.npy\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3b810adf-3d4d-4d50-8808-639d4349cd30\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3b810adf-3d4d-4d50-8808-639d4349cd30\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving X_test.npy to X_test.npy\n","(9349, 520)\n","(9349,)\n","(40249, 520)\n","(40249,)\n","[3.0 3.0 3.0 ... 4.0 0.0 array(0., dtype=float32)]\n","[3.0 6.0 0.0 ... 0.0 0.0 array(0., dtype=float32)]\n","(49598, 520)\n","(49598,)\n"]}],"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","# import libraries\n","%matplotlib inline\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# TensorFlow and tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.models import Sequential \n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers\n","# Helper libraries\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from google.colab import files\n","import io\n","from sklearn.utils import shuffle\n","\n","# load mostly preprocessed data (preprocessing accomplished before 3/25 proposal)\n","\n","# would use regular numpy syntax to load locally:\n","#X = np.load('<path>/X_cleaned_attempt_new.npy',allow_pickle=True)\n","#Y = np.load('<path>/Y_cleaned_attempt_new.npy',allow_pickle=True)\n","#X = np.asarray(X).astype('float32')\n","#Y = np.asarray(Y).astype('float32')\n","\n","# i'm using colab, so it's slightly more complicated\n","ytrainval_byteseq = files.upload() # choose 'Y_cleaned_attempt_new.npy' from local system\n","ytrainval_filelike = io.BytesIO(ytrainval_byteseq['Y_cleaned_attempt_new.npy']) # create file-like object\n","ytrainval = np.load(ytrainval_filelike,allow_pickle=True) # create regular numpy array\n","xtrainval_byteseq = files.upload() # choose 'X_cleaned_attempt_new.npy' from local system\n","xtrainval_filelike = io.BytesIO(xtrainval_byteseq['X_cleaned_attempt_new.npy']) # create file-like object\n","xtrainval = np.load(xtrainval_filelike,allow_pickle=True) # create regular numpy array\n","ytest_byteseq = files.upload() # choose 'Y_test.npy' from local system\n","ytest_filelike = io.BytesIO(ytest_byteseq['Y_test.npy']) # create file-like object\n","ytest = np.load(ytest_filelike,allow_pickle=True) # create regular numpy array\n","xtest_byteseq = files.upload() # choose 'X_test.npy' from local system\n","xtest_filelike = io.BytesIO(xtest_byteseq['X_test.npy']) # create file-like object\n","xtest = np.load(xtest_filelike,allow_pickle=True) # create regular numpy array\n","\n","#check shapes\n","print(xtrainval.shape)\n","print(ytrainval.shape)\n","print(xtest.shape)\n","print(ytest.shape)\n","\n","#ytrainval needs to be converted to number representation of labels\n","feature_map = {'Rock':0,'Electronic':1,'Experimental':2,'Hip-Hop':3,'Folk':4,'Instrumental':5,'Pop':6,\n","              'International':7,'Classical':8,'Old-Time / Historic':9, 'Jazz':10,'Country':11,'Soul-RnB':12,\n","              'Spoken':13,'Blues':14,'Easy Listening':15}\n","for i in range(ytrainval.shape[0]):\n","    ytrainval[i] = float(feature_map[ytrainval[i]])\n","ytrainval[i] = np.asarray(ytrainval[i]).astype('float32')\n","\n","print(ytrainval)\n","print(ytest)\n","\n","# for the preliminary report, i did a roughly 80-20 split, but used the 20% as the training set to improve speed\n","# so will recombine and do a 90-10 split, using the 90% as training/validation\n","xfull = np.concatenate((xtrainval,xtest),axis=0)\n","yfull = np.concatenate((ytrainval,ytest),axis=0)\n","print(xfull.shape)\n","print(yfull.shape)\n","\n"]},{"cell_type":"code","source":["# this is where the strategy changes. We need to do a different train/test/val split\n","# let's see what the overall distribution between the categories is\n","yfull_1hot = to_categorical(yfull)\n","yfull_sum = np.sum(yfull_1hot, axis=0)\n","print(yfull_sum)\n"],"metadata":{"id":"_oicdI3ksFWc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652124332262,"user_tz":240,"elapsed":280,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"e3a2b49a-dc16-44ea-fbdf-4b94cfad3372"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[14182.  9372. 10608.  3552.  2803.  2079.  2332.  1389.  1230.   554.\n","   571.   194.   175.   423.   110.    24.]\n"]}]},{"cell_type":"code","source":["# In an extreme case, we might just take 20 of each class in the train set\n","\n","yfull = yfull.reshape(yfull.shape[0],1)\n","both_full = np.concatenate((xfull, yfull), axis=1)\n","print(both_full.shape)\n","both_full = shuffle(both_full)\n","indices = [] \n","num_counter = []\n","for i in range(16):\n","    num_counter.append(0)\n","train_max_list = [1000,1000,1000,1000,1000,1000,1000,1000,1000,100,100,100,100,100,100,20]\n","for i in range(both_full.shape[0]):\n","    if num_counter[int(both_full[i,-1])] < train_max_list[int(both_full[i,-1])]:\n","        num_counter[int(both_full[i,-1])] += 1\n","        indices.append(i)\n","print(num_counter)\n","both_train_small = both_full[indices]\n","y_train_small = both_train_small[:,-1]\n","y_train_small_1hot = to_categorical(y_train_small).astype('float32')\n","x_train_small = both_train_small[:,:-1]\n","print(y_train_small_1hot.shape)\n","print(y_train_small.shape)\n","print(x_train_small.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHC8x8U6txYx","executionInfo":{"status":"ok","timestamp":1652124340463,"user_tz":240,"elapsed":1442,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"f24e69da-8e90-49c9-a110-4abf2b63f203"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(49598, 521)\n","[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 100, 100, 100, 100, 100, 100, 20]\n","(9620, 16)\n","(9620,)\n","(9620, 520)\n"]}]},{"cell_type":"code","source":["# need to implement k-fold cross validation\n","'''\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.3),\n","          layers.Dense(100,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(256,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(128,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(64,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","# COMPILE NEW MODEL\n","model.compile(loss='categorical_crossentropy',\n","    optimizer=keras.optimizers.Adam(\n","        learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","        metrics=['accuracy'])\n","\n","\n","# see https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","kf = KFold(n_splits = x_trainval_small.shape[0])\n","for train_index, val_index in kf.split(np.zeros(x_trainval_small.shape[0]),y_trainval_small_1hot):\n","\t\n","\t\n","\t\n","\t\n","\t# FIT THE MODEL\n","\thistory = model.fit(x_trainval_small,\n","\t\t\t    epochs=10,,\n","\t\t\t    validation_data=valid_data_generator)\n","\t#PLOT HISTORY\n","\t#\t\t:\n","\t#\t\t:\n","\t\n","\t# LOAD BEST MODEL to evaluate the performance of the model\n","\tmodel.load_weights(\"/saved_models/model_\"+str(fold_var)+\".h5\")\n","\t\n","\tresults = model.evaluate(valid_data_generator)\n","\tresults = dict(zip(model.metrics_names,results))\n","\t\n","\tVALIDATION_ACCURACY.append(results['accuracy'])\n","\tVALIDATION_LOSS.append(results['loss'])\n","\t\n","\ttf.keras.backend.clear_session()\n","\t\n","\tfold_var += 1\n","  '''"],"metadata":{"id":"LgzwCHxWCteI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","# let's give the datasets better names to reflect what i'm actually doing (kind of abandoned above plan)\n","import copy \n","x_trainval_small = x_trainval_small.astype('float32')\n","y_trainval_small_1hot = y_trainval_small_1hot.astype('float32')\n","x_train_small = copy.deepcopy(x_trainval_small)\n","y_train_small_1hot = copy.deepcopy(y_trainval_small_1hot)\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.3),\n","          layers.Dense(100,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(256,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(128,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(64,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_trainval_small, y_trainval_small_1hot, batch_size=32, epochs=100, validation_data=(x_trainval_small, y_trainval_small_1hot))\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-gaXkjgTFXWm","executionInfo":{"status":"ok","timestamp":1651888857167,"user_tz":240,"elapsed":21911,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"1b0a3ccd-e0c1-45d7-adef-9580cd4b4ffc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","10/10 [==============================] - 1s 37ms/step - loss: 2.9122 - accuracy: 0.1406 - val_loss: 21.3840 - val_accuracy: 0.0719\n","Epoch 2/100\n","10/10 [==============================] - 0s 13ms/step - loss: 2.0922 - accuracy: 0.3375 - val_loss: 19.8783 - val_accuracy: 0.0625\n","Epoch 3/100\n","10/10 [==============================] - 0s 13ms/step - loss: 1.7584 - accuracy: 0.4719 - val_loss: 22.5311 - val_accuracy: 0.0625\n","Epoch 4/100\n","10/10 [==============================] - 0s 9ms/step - loss: 1.4792 - accuracy: 0.5562 - val_loss: 22.1548 - val_accuracy: 0.0625\n","Epoch 5/100\n","10/10 [==============================] - 0s 14ms/step - loss: 1.2724 - accuracy: 0.6094 - val_loss: 23.0078 - val_accuracy: 0.0625\n","Epoch 6/100\n","10/10 [==============================] - 0s 10ms/step - loss: 1.1068 - accuracy: 0.6875 - val_loss: 19.2610 - val_accuracy: 0.0625\n","Epoch 7/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.9062 - accuracy: 0.7437 - val_loss: 18.2128 - val_accuracy: 0.0625\n","Epoch 8/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.8289 - accuracy: 0.7563 - val_loss: 17.4694 - val_accuracy: 0.0656\n","Epoch 9/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.7366 - accuracy: 0.7937 - val_loss: 18.0765 - val_accuracy: 0.0656\n","Epoch 10/100\n","10/10 [==============================] - 0s 15ms/step - loss: 0.6537 - accuracy: 0.8000 - val_loss: 18.7777 - val_accuracy: 0.0656\n","Epoch 11/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.6248 - accuracy: 0.8219 - val_loss: 17.3474 - val_accuracy: 0.0625\n","Epoch 12/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.6057 - accuracy: 0.8062 - val_loss: 17.1283 - val_accuracy: 0.0625\n","Epoch 13/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.5131 - accuracy: 0.8562 - val_loss: 17.5887 - val_accuracy: 0.0625\n","Epoch 14/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.4775 - accuracy: 0.8781 - val_loss: 16.9201 - val_accuracy: 0.0625\n","Epoch 15/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.4787 - accuracy: 0.8594 - val_loss: 16.3216 - val_accuracy: 0.0625\n","Epoch 16/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.4158 - accuracy: 0.8781 - val_loss: 17.5319 - val_accuracy: 0.0625\n","Epoch 17/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.3669 - accuracy: 0.9062 - val_loss: 18.4632 - val_accuracy: 0.0625\n","Epoch 18/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.3213 - accuracy: 0.9125 - val_loss: 17.5572 - val_accuracy: 0.1094\n","Epoch 19/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.3715 - accuracy: 0.9000 - val_loss: 15.6237 - val_accuracy: 0.1000\n","Epoch 20/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.3312 - accuracy: 0.8938 - val_loss: 15.1470 - val_accuracy: 0.1187\n","Epoch 21/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.3006 - accuracy: 0.9094 - val_loss: 14.6911 - val_accuracy: 0.1063\n","Epoch 22/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.2702 - accuracy: 0.9219 - val_loss: 13.6780 - val_accuracy: 0.1156\n","Epoch 23/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.2393 - accuracy: 0.9281 - val_loss: 13.7503 - val_accuracy: 0.0969\n","Epoch 24/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.2887 - accuracy: 0.9094 - val_loss: 11.7448 - val_accuracy: 0.1219\n","Epoch 25/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.2096 - accuracy: 0.9406 - val_loss: 11.3765 - val_accuracy: 0.1219\n","Epoch 26/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.2349 - accuracy: 0.9406 - val_loss: 11.1776 - val_accuracy: 0.1187\n","Epoch 27/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.2268 - accuracy: 0.9156 - val_loss: 9.8222 - val_accuracy: 0.1281\n","Epoch 28/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.2068 - accuracy: 0.9344 - val_loss: 8.5403 - val_accuracy: 0.1281\n","Epoch 29/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.2352 - accuracy: 0.9094 - val_loss: 7.8791 - val_accuracy: 0.1406\n","Epoch 30/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1887 - accuracy: 0.9563 - val_loss: 7.7889 - val_accuracy: 0.1469\n","Epoch 31/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1587 - accuracy: 0.9563 - val_loss: 7.2575 - val_accuracy: 0.1688\n","Epoch 32/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1733 - accuracy: 0.9563 - val_loss: 6.7141 - val_accuracy: 0.1813\n","Epoch 33/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1615 - accuracy: 0.9563 - val_loss: 6.2245 - val_accuracy: 0.1875\n","Epoch 34/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.2090 - accuracy: 0.9312 - val_loss: 6.2939 - val_accuracy: 0.2000\n","Epoch 35/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1963 - accuracy: 0.9438 - val_loss: 6.0580 - val_accuracy: 0.1813\n","Epoch 36/100\n","10/10 [==============================] - 0s 11ms/step - loss: 0.1589 - accuracy: 0.9563 - val_loss: 5.8663 - val_accuracy: 0.1969\n","Epoch 37/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1988 - accuracy: 0.9281 - val_loss: 4.7822 - val_accuracy: 0.2688\n","Epoch 38/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1529 - accuracy: 0.9531 - val_loss: 4.0128 - val_accuracy: 0.2937\n","Epoch 39/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1396 - accuracy: 0.9625 - val_loss: 3.0340 - val_accuracy: 0.4031\n","Epoch 40/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1331 - accuracy: 0.9594 - val_loss: 2.4853 - val_accuracy: 0.4812\n","Epoch 41/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1318 - accuracy: 0.9688 - val_loss: 1.7412 - val_accuracy: 0.5719\n","Epoch 42/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1458 - accuracy: 0.9594 - val_loss: 1.0542 - val_accuracy: 0.7000\n","Epoch 43/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1328 - accuracy: 0.9625 - val_loss: 0.6974 - val_accuracy: 0.7188\n","Epoch 44/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1074 - accuracy: 0.9656 - val_loss: 0.8879 - val_accuracy: 0.7219\n","Epoch 45/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1768 - accuracy: 0.9406 - val_loss: 0.6255 - val_accuracy: 0.7875\n","Epoch 46/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.2000 - accuracy: 0.9281 - val_loss: 0.1511 - val_accuracy: 0.9563\n","Epoch 47/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1901 - accuracy: 0.9469 - val_loss: 0.1106 - val_accuracy: 0.9656\n","Epoch 48/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1268 - accuracy: 0.9625 - val_loss: 0.1973 - val_accuracy: 0.9312\n","Epoch 49/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1860 - accuracy: 0.9375 - val_loss: 0.1127 - val_accuracy: 0.9594\n","Epoch 50/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1123 - accuracy: 0.9688 - val_loss: 0.0596 - val_accuracy: 0.9781\n","Epoch 51/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.1054 - accuracy: 0.9688 - val_loss: 0.0453 - val_accuracy: 0.9812\n","Epoch 52/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1175 - accuracy: 0.9500 - val_loss: 0.0400 - val_accuracy: 0.9937\n","Epoch 53/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1469 - accuracy: 0.9438 - val_loss: 0.0170 - val_accuracy: 1.0000\n","Epoch 54/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1090 - accuracy: 0.9625 - val_loss: 0.0152 - val_accuracy: 0.9969\n","Epoch 55/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.0999 - accuracy: 0.9719 - val_loss: 0.0062 - val_accuracy: 1.0000\n","Epoch 56/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1629 - accuracy: 0.9531 - val_loss: 0.0085 - val_accuracy: 1.0000\n","Epoch 57/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1229 - accuracy: 0.9531 - val_loss: 0.0116 - val_accuracy: 1.0000\n","Epoch 58/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1079 - accuracy: 0.9688 - val_loss: 0.0070 - val_accuracy: 1.0000\n","Epoch 59/100\n","10/10 [==============================] - 0s 15ms/step - loss: 0.1400 - accuracy: 0.9594 - val_loss: 0.0057 - val_accuracy: 1.0000\n","Epoch 60/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0797 - accuracy: 0.9688 - val_loss: 0.0115 - val_accuracy: 0.9969\n","Epoch 61/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1448 - accuracy: 0.9656 - val_loss: 0.0039 - val_accuracy: 1.0000\n","Epoch 62/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.0794 - accuracy: 0.9781 - val_loss: 0.0023 - val_accuracy: 1.0000\n","Epoch 63/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1227 - accuracy: 0.9688 - val_loss: 0.0030 - val_accuracy: 1.0000\n","Epoch 64/100\n","10/10 [==============================] - 0s 15ms/step - loss: 0.1430 - accuracy: 0.9563 - val_loss: 0.0042 - val_accuracy: 1.0000\n","Epoch 65/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1013 - accuracy: 0.9656 - val_loss: 0.0047 - val_accuracy: 1.0000\n","Epoch 66/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0826 - accuracy: 0.9688 - val_loss: 0.0031 - val_accuracy: 1.0000\n","Epoch 67/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1001 - accuracy: 0.9656 - val_loss: 0.0031 - val_accuracy: 1.0000\n","Epoch 68/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1123 - accuracy: 0.9531 - val_loss: 0.0022 - val_accuracy: 1.0000\n","Epoch 69/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.0023 - val_accuracy: 1.0000\n","Epoch 70/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0831 - accuracy: 0.9719 - val_loss: 0.0044 - val_accuracy: 1.0000\n","Epoch 71/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.0847 - accuracy: 0.9719 - val_loss: 0.0110 - val_accuracy: 0.9937\n","Epoch 72/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1724 - accuracy: 0.9406 - val_loss: 0.0056 - val_accuracy: 1.0000\n","Epoch 73/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1145 - accuracy: 0.9656 - val_loss: 0.0050 - val_accuracy: 1.0000\n","Epoch 74/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.1217 - accuracy: 0.9594 - val_loss: 0.0042 - val_accuracy: 1.0000\n","Epoch 75/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1420 - accuracy: 0.9531 - val_loss: 0.0028 - val_accuracy: 1.0000\n","Epoch 76/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.0946 - accuracy: 0.9656 - val_loss: 0.0027 - val_accuracy: 1.0000\n","Epoch 77/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.0784 - accuracy: 0.9719 - val_loss: 0.0020 - val_accuracy: 1.0000\n","Epoch 78/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.0755 - accuracy: 0.9812 - val_loss: 0.0030 - val_accuracy: 1.0000\n","Epoch 79/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9781 - val_loss: 0.0023 - val_accuracy: 1.0000\n","Epoch 80/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0978 - accuracy: 0.9625 - val_loss: 0.0027 - val_accuracy: 1.0000\n","Epoch 81/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.0752 - accuracy: 0.9719 - val_loss: 0.0046 - val_accuracy: 1.0000\n","Epoch 82/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0873 - accuracy: 0.9719 - val_loss: 0.0056 - val_accuracy: 1.0000\n","Epoch 83/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1084 - accuracy: 0.9594 - val_loss: 0.0018 - val_accuracy: 1.0000\n","Epoch 84/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1571 - accuracy: 0.9531 - val_loss: 0.0024 - val_accuracy: 1.0000\n","Epoch 85/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1208 - accuracy: 0.9625 - val_loss: 0.0035 - val_accuracy: 1.0000\n","Epoch 86/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1100 - accuracy: 0.9594 - val_loss: 0.0033 - val_accuracy: 1.0000\n","Epoch 87/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0973 - accuracy: 0.9656 - val_loss: 0.0039 - val_accuracy: 1.0000\n","Epoch 88/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1427 - accuracy: 0.9594 - val_loss: 0.0046 - val_accuracy: 0.9969\n","Epoch 89/100\n","10/10 [==============================] - 0s 11ms/step - loss: 0.0387 - accuracy: 0.9906 - val_loss: 0.0016 - val_accuracy: 1.0000\n","Epoch 90/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0872 - accuracy: 0.9594 - val_loss: 0.0016 - val_accuracy: 1.0000\n","Epoch 91/100\n","10/10 [==============================] - 0s 13ms/step - loss: 0.0664 - accuracy: 0.9750 - val_loss: 0.0011 - val_accuracy: 1.0000\n","Epoch 92/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0727 - accuracy: 0.9750 - val_loss: 9.0346e-04 - val_accuracy: 1.0000\n","Epoch 93/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.0787 - accuracy: 0.9750 - val_loss: 0.0011 - val_accuracy: 1.0000\n","Epoch 94/100\n","10/10 [==============================] - 0s 11ms/step - loss: 0.0820 - accuracy: 0.9688 - val_loss: 0.0016 - val_accuracy: 1.0000\n","Epoch 95/100\n","10/10 [==============================] - 0s 9ms/step - loss: 0.1349 - accuracy: 0.9531 - val_loss: 0.0013 - val_accuracy: 1.0000\n","Epoch 96/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0583 - accuracy: 0.9812 - val_loss: 0.0017 - val_accuracy: 1.0000\n","Epoch 97/100\n","10/10 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 1.0000\n","Epoch 98/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.1089 - accuracy: 0.9656 - val_loss: 8.8039e-04 - val_accuracy: 1.0000\n","Epoch 99/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0598 - accuracy: 0.9781 - val_loss: 0.0050 - val_accuracy: 0.9969\n","Epoch 100/100\n","10/10 [==============================] - 0s 14ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.0012 - val_accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["'''\n","y_test = np.array([both_full[i,-1] for i in range(2000) if i not in indices])\n","x_test = np.array([both_full[i,:-1] for i in range(2000) if i not in indices])\n","y_test_1hot = to_categorical(y_test)\n","ytrain_sum = np.sum(y_train_small_1hot, axis=0)\n","print(y_test_1hot.shape)\n","print(x_test.shape)\n","ytest_sum = np.sum(y_test_1hot, axis=0)\n","print(ytest_sum)\n","print(yfull_sum)\n","print(ytrain_sum)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ef8yLrEQP8fH","executionInfo":{"status":"ok","timestamp":1651890334399,"user_tz":240,"elapsed":147,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"d6a79661-c2be-4be0-85f6-17e10e0e162b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1747, 11)\n","(1747, 520)\n","[553. 382. 386. 119.  85.  67.  79.  24.  34.  12.   6.]\n","[14182.  9372. 10608.  3552.  2803.  2079.  2332.  1389.  1230.   554.\n","   571.   194.   175.   423.   110.    24.]\n","[20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20.]\n"]}]},{"cell_type":"code","source":["val_numbers = [(i-train_max_list[c])//2 for c,i in enumerate(yfull_sum)] # number of each instance to put in val set \n","val_indices = []\n","num_counter = []\n","for i in range(16):\n","    num_counter.append(0)\n","for i in range(both_full.shape[0]):\n","    if i not in indices: # don't count if already in training set\n","        if num_counter[int(both_full[i,-1])] < val_numbers[int(both_full[i,-1])]:\n","            num_counter[int(both_full[i,-1])] += 1\n","            val_indices.append(i)\n","\n","#indices and val_indices should be different lists\n","for i in indices:\n","  for j in val_indices:\n","    assert i!=j\n","\n","both_val_small = both_full[val_indices,:]\n","y_val_small = both_val_small[:,-1]\n","y_val_small_1hot = to_categorical(y_val_small).astype('float32')\n","x_val_small = both_val_small[:,:-1]\n","print(y_val_small_1hot.shape)\n","print(y_val_small.shape)\n","print(x_val_small.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Twm9NLSuVwjn","executionInfo":{"status":"ok","timestamp":1652124383757,"user_tz":240,"elapsed":20114,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"36f24e5c-022c-4792-8abb-4785faa22d9b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(19986, 16)\n","(19986,)\n","(19986, 520)\n"]}]},{"cell_type":"code","source":["print(num_counter)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2siKJSRmpf2","executionInfo":{"status":"ok","timestamp":1652124390435,"user_tz":240,"elapsed":305,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"da2dd0a6-425f-4d51-ef54-9083b6be8916"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[6591, 4186, 4804, 1276, 901, 539, 666, 194, 115, 227, 235, 47, 37, 161, 5, 2]\n"]}]},{"cell_type":"code","source":["# now we need to put everything else in the test set\n","test_indices = []\n","for i in range(both_full.shape[0]):\n","    if i not in indices: # don't count if already in training set\n","        if i not in val_indices: # don't count if already in validation set\n","            test_indices.append(i)\n","both_test_small = both_full[test_indices,:]\n","y_test_small = both_test_small[:,-1]\n","y_test_small_1hot = to_categorical(y_test_small).astype('float32')\n","x_test_small = both_test_small[:,:-1]\n","print(y_test_small_1hot.shape)\n","print(y_test_small.shape)\n","print(x_test_small.shape)\n","assert y_test_small.shape[0] + y_val_small.shape[0] + y_train_small.shape[0] == both_full.shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBg14zOwoUoC","executionInfo":{"status":"ok","timestamp":1652124412531,"user_tz":240,"elapsed":15042,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"662df0f8-1236-4f8e-b85b-feac50bbe846"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(19992, 16)\n","(19992,)\n","(19992, 520)\n"]}]},{"cell_type":"code","source":["#from copy import deepcopy\n","#y_train_small_1hot_copy = deepcopy(y_train_small_1hot) float32 copy\n","#y_val_small_1hot_copy = deepcopy(y_val_small_1hot) float32 copy\n","#y_test_small_1hot_copy = deepcopy(y_test_small_1hot) float32 copy\n","#x_train_small_copy = deepcopy(x_train_small) # float copy\n","#x_val_small_copy = deepcopy(x_val_small) # float copy\n","#x_test_small_copy = deepcopy(x_test_small) # float copy\n","\n","x_train_small = x_train_small.astype('float32')\n","x_val_small = x_val_small.astype('float32')\n","x_test_small = x_test_small.astype('float32')\n","\n","y_train_small_1hot = y_train_small_1hot.astype('int')\n","y_val_small_1hot = y_val_small_1hot.astype('int')\n","y_test_small_1hot = y_test_small_1hot.astype('int')"],"metadata":{"id":"AF_XD93Qp_Lm","executionInfo":{"status":"ok","timestamp":1652124427094,"user_tz":240,"elapsed":971,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# now let's train!\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.3),\n","          layers.Dense(100,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(256,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(128,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(64,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tboShY1tpklT","executionInfo":{"status":"ok","timestamp":1652051245271,"user_tz":240,"elapsed":219994,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"f52628b2-bcd6-421d-b133-3c4a89041865"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.8233 - accuracy: 0.4257 - val_loss: 2.9661 - val_accuracy: 0.0697\n","Epoch 2/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5384 - accuracy: 0.5029 - val_loss: 1.5223 - val_accuracy: 0.5067\n","Epoch 3/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4657 - accuracy: 0.5232 - val_loss: 1.3658 - val_accuracy: 0.5642\n","Epoch 4/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4000 - accuracy: 0.5471 - val_loss: 1.5150 - val_accuracy: 0.5067\n","Epoch 5/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3407 - accuracy: 0.5588 - val_loss: 1.4224 - val_accuracy: 0.5406\n","Epoch 6/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3154 - accuracy: 0.5684 - val_loss: 1.4927 - val_accuracy: 0.5197\n","Epoch 7/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.2760 - accuracy: 0.5822 - val_loss: 1.3290 - val_accuracy: 0.5739\n","Epoch 8/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.2350 - accuracy: 0.5924 - val_loss: 1.4146 - val_accuracy: 0.5443\n","Epoch 9/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.2058 - accuracy: 0.6022 - val_loss: 1.4138 - val_accuracy: 0.5529\n","Epoch 10/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.1797 - accuracy: 0.6037 - val_loss: 1.3676 - val_accuracy: 0.5617\n","Epoch 11/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.1507 - accuracy: 0.6139 - val_loss: 1.3320 - val_accuracy: 0.5740\n","Epoch 12/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.1172 - accuracy: 0.6256 - val_loss: 1.3602 - val_accuracy: 0.5724\n","Epoch 13/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.0967 - accuracy: 0.6341 - val_loss: 1.4292 - val_accuracy: 0.5615\n","Epoch 14/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.0755 - accuracy: 0.6369 - val_loss: 1.3309 - val_accuracy: 0.5765\n","Epoch 15/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.0222 - accuracy: 0.6572 - val_loss: 1.3995 - val_accuracy: 0.5653\n","Epoch 16/100\n","301/301 [==============================] - 2s 7ms/step - loss: 1.0211 - accuracy: 0.6565 - val_loss: 1.3772 - val_accuracy: 0.5769\n","Epoch 17/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.9918 - accuracy: 0.6661 - val_loss: 1.3625 - val_accuracy: 0.5733\n","Epoch 18/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.9743 - accuracy: 0.6708 - val_loss: 1.4758 - val_accuracy: 0.5507\n","Epoch 19/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.9438 - accuracy: 0.6812 - val_loss: 1.3397 - val_accuracy: 0.5891\n","Epoch 20/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.9225 - accuracy: 0.6818 - val_loss: 1.5271 - val_accuracy: 0.5280\n","Epoch 21/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.8859 - accuracy: 0.6965 - val_loss: 1.4610 - val_accuracy: 0.5550\n","Epoch 22/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.8763 - accuracy: 0.6949 - val_loss: 1.6061 - val_accuracy: 0.5213\n","Epoch 23/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.8544 - accuracy: 0.7117 - val_loss: 1.4912 - val_accuracy: 0.5499\n","Epoch 24/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.8283 - accuracy: 0.7165 - val_loss: 1.4823 - val_accuracy: 0.5673\n","Epoch 25/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7944 - accuracy: 0.7285 - val_loss: 1.6018 - val_accuracy: 0.5290\n","Epoch 26/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7979 - accuracy: 0.7282 - val_loss: 1.5359 - val_accuracy: 0.5515\n","Epoch 27/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7763 - accuracy: 0.7323 - val_loss: 1.5925 - val_accuracy: 0.5459\n","Epoch 28/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7542 - accuracy: 0.7390 - val_loss: 1.6715 - val_accuracy: 0.5262\n","Epoch 29/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7239 - accuracy: 0.7467 - val_loss: 1.6593 - val_accuracy: 0.5293\n","Epoch 30/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7070 - accuracy: 0.7616 - val_loss: 1.7394 - val_accuracy: 0.5205\n","Epoch 31/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.7063 - accuracy: 0.7608 - val_loss: 1.5476 - val_accuracy: 0.5759\n","Epoch 32/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.6934 - accuracy: 0.7597 - val_loss: 1.6582 - val_accuracy: 0.5329\n","Epoch 33/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.6792 - accuracy: 0.7648 - val_loss: 1.5556 - val_accuracy: 0.5749\n","Epoch 34/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.6682 - accuracy: 0.7684 - val_loss: 1.6195 - val_accuracy: 0.5621\n","Epoch 35/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.6460 - accuracy: 0.7740 - val_loss: 1.6236 - val_accuracy: 0.5572\n","Epoch 36/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.6340 - accuracy: 0.7769 - val_loss: 1.6550 - val_accuracy: 0.5536\n","Epoch 37/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.6173 - accuracy: 0.7865 - val_loss: 1.7281 - val_accuracy: 0.5360\n","Epoch 38/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.6287 - accuracy: 0.7806 - val_loss: 1.7185 - val_accuracy: 0.5326\n","Epoch 39/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.6198 - accuracy: 0.7839 - val_loss: 1.8722 - val_accuracy: 0.5094\n","Epoch 40/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5993 - accuracy: 0.7916 - val_loss: 1.7380 - val_accuracy: 0.5490\n","Epoch 41/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5851 - accuracy: 0.7964 - val_loss: 1.7295 - val_accuracy: 0.5562\n","Epoch 42/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5846 - accuracy: 0.7991 - val_loss: 1.6666 - val_accuracy: 0.5670\n","Epoch 43/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5713 - accuracy: 0.8072 - val_loss: 1.7733 - val_accuracy: 0.5359\n","Epoch 44/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5648 - accuracy: 0.8094 - val_loss: 1.7860 - val_accuracy: 0.5410\n","Epoch 45/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5463 - accuracy: 0.8112 - val_loss: 1.8536 - val_accuracy: 0.5279\n","Epoch 46/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5526 - accuracy: 0.8083 - val_loss: 1.7926 - val_accuracy: 0.5531\n","Epoch 47/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5466 - accuracy: 0.8127 - val_loss: 1.8972 - val_accuracy: 0.5278\n","Epoch 48/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5286 - accuracy: 0.8176 - val_loss: 1.8816 - val_accuracy: 0.5283\n","Epoch 49/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5178 - accuracy: 0.8187 - val_loss: 1.8370 - val_accuracy: 0.5513\n","Epoch 50/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.5184 - accuracy: 0.8207 - val_loss: 1.9593 - val_accuracy: 0.5273\n","Epoch 51/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4892 - accuracy: 0.8354 - val_loss: 1.9006 - val_accuracy: 0.5347\n","Epoch 52/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4874 - accuracy: 0.8320 - val_loss: 1.7939 - val_accuracy: 0.5676\n","Epoch 53/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4955 - accuracy: 0.8305 - val_loss: 1.9777 - val_accuracy: 0.5102\n","Epoch 54/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4808 - accuracy: 0.8344 - val_loss: 1.9709 - val_accuracy: 0.5333\n","Epoch 55/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4774 - accuracy: 0.8319 - val_loss: 1.9388 - val_accuracy: 0.5404\n","Epoch 56/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4797 - accuracy: 0.8360 - val_loss: 1.9629 - val_accuracy: 0.5434\n","Epoch 57/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4800 - accuracy: 0.8308 - val_loss: 1.9179 - val_accuracy: 0.5462\n","Epoch 58/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4594 - accuracy: 0.8445 - val_loss: 1.9665 - val_accuracy: 0.5384\n","Epoch 59/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4611 - accuracy: 0.8443 - val_loss: 1.8828 - val_accuracy: 0.5511\n","Epoch 60/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4477 - accuracy: 0.8481 - val_loss: 1.9912 - val_accuracy: 0.5310\n","Epoch 61/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4397 - accuracy: 0.8484 - val_loss: 2.0869 - val_accuracy: 0.5144\n","Epoch 62/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.4630 - accuracy: 0.8412 - val_loss: 1.9884 - val_accuracy: 0.5343\n","Epoch 63/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4261 - accuracy: 0.8562 - val_loss: 2.0253 - val_accuracy: 0.5417\n","Epoch 64/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4190 - accuracy: 0.8553 - val_loss: 1.9891 - val_accuracy: 0.5463\n","Epoch 65/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4406 - accuracy: 0.8525 - val_loss: 1.9902 - val_accuracy: 0.5346\n","Epoch 66/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4270 - accuracy: 0.8529 - val_loss: 2.0408 - val_accuracy: 0.5320\n","Epoch 67/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4191 - accuracy: 0.8574 - val_loss: 2.2262 - val_accuracy: 0.4931\n","Epoch 68/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4106 - accuracy: 0.8599 - val_loss: 2.0466 - val_accuracy: 0.5459\n","Epoch 69/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4003 - accuracy: 0.8628 - val_loss: 2.1055 - val_accuracy: 0.5356\n","Epoch 70/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4090 - accuracy: 0.8602 - val_loss: 2.0177 - val_accuracy: 0.5472\n","Epoch 71/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4025 - accuracy: 0.8624 - val_loss: 2.1531 - val_accuracy: 0.5066\n","Epoch 72/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4007 - accuracy: 0.8626 - val_loss: 2.0722 - val_accuracy: 0.5308\n","Epoch 73/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.4080 - accuracy: 0.8613 - val_loss: 2.1264 - val_accuracy: 0.5193\n","Epoch 74/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3985 - accuracy: 0.8637 - val_loss: 2.1360 - val_accuracy: 0.5039\n","Epoch 75/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.3837 - accuracy: 0.8675 - val_loss: 2.1592 - val_accuracy: 0.5227\n","Epoch 76/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3795 - accuracy: 0.8748 - val_loss: 2.1165 - val_accuracy: 0.5326\n","Epoch 77/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.3944 - accuracy: 0.8656 - val_loss: 2.1145 - val_accuracy: 0.5233\n","Epoch 78/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3999 - accuracy: 0.8650 - val_loss: 2.1939 - val_accuracy: 0.5127\n","Epoch 79/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3795 - accuracy: 0.8722 - val_loss: 2.2295 - val_accuracy: 0.5168\n","Epoch 80/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3733 - accuracy: 0.8738 - val_loss: 2.0296 - val_accuracy: 0.5516\n","Epoch 81/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3658 - accuracy: 0.8748 - val_loss: 2.0620 - val_accuracy: 0.5436\n","Epoch 82/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3657 - accuracy: 0.8779 - val_loss: 2.1062 - val_accuracy: 0.5330\n","Epoch 83/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3735 - accuracy: 0.8739 - val_loss: 2.1967 - val_accuracy: 0.5231\n","Epoch 84/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3707 - accuracy: 0.8740 - val_loss: 2.0769 - val_accuracy: 0.5370\n","Epoch 85/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3695 - accuracy: 0.8764 - val_loss: 2.3085 - val_accuracy: 0.5087\n","Epoch 86/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3478 - accuracy: 0.8808 - val_loss: 2.1355 - val_accuracy: 0.5361\n","Epoch 87/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3578 - accuracy: 0.8775 - val_loss: 2.1865 - val_accuracy: 0.5283\n","Epoch 88/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3497 - accuracy: 0.8806 - val_loss: 2.2083 - val_accuracy: 0.5223\n","Epoch 89/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3474 - accuracy: 0.8839 - val_loss: 2.2036 - val_accuracy: 0.5221\n","Epoch 90/100\n","301/301 [==============================] - 2s 8ms/step - loss: 0.3517 - accuracy: 0.8799 - val_loss: 2.1852 - val_accuracy: 0.5232\n","Epoch 91/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3484 - accuracy: 0.8814 - val_loss: 2.3212 - val_accuracy: 0.5060\n","Epoch 92/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3491 - accuracy: 0.8844 - val_loss: 2.1683 - val_accuracy: 0.5318\n","Epoch 93/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3437 - accuracy: 0.8820 - val_loss: 2.1673 - val_accuracy: 0.5460\n","Epoch 94/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3340 - accuracy: 0.8846 - val_loss: 2.2517 - val_accuracy: 0.5334\n","Epoch 95/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3458 - accuracy: 0.8814 - val_loss: 2.1864 - val_accuracy: 0.5388\n","Epoch 96/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3397 - accuracy: 0.8844 - val_loss: 2.2946 - val_accuracy: 0.5114\n","Epoch 97/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3390 - accuracy: 0.8847 - val_loss: 2.2651 - val_accuracy: 0.5236\n","Epoch 98/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3326 - accuracy: 0.8883 - val_loss: 2.3004 - val_accuracy: 0.5141\n","Epoch 99/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3367 - accuracy: 0.8856 - val_loss: 2.0702 - val_accuracy: 0.5576\n","Epoch 100/100\n","301/301 [==============================] - 2s 7ms/step - loss: 0.3223 - accuracy: 0.8933 - val_loss: 2.2230 - val_accuracy: 0.5238\n"]}]},{"cell_type":"code","source":["# clearly we need some strong regularization with this small training set\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(64,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'),\n","          #layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=10, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3Rmw8A_r91v","executionInfo":{"status":"ok","timestamp":1652051359987,"user_tz":240,"elapsed":25587,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"ad41202f-3c30-497c-dcae-064aac4b714e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","301/301 [==============================] - 4s 8ms/step - loss: 1.9173 - accuracy: 0.4063 - val_loss: 2.2535 - val_accuracy: 0.3110\n","Epoch 2/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.5435 - accuracy: 0.5089 - val_loss: 1.4879 - val_accuracy: 0.5375\n","Epoch 3/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4410 - accuracy: 0.5402 - val_loss: 1.4222 - val_accuracy: 0.5583\n","Epoch 4/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3812 - accuracy: 0.5553 - val_loss: 1.3982 - val_accuracy: 0.5727\n","Epoch 5/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3481 - accuracy: 0.5705 - val_loss: 1.4050 - val_accuracy: 0.5660\n","Epoch 6/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3311 - accuracy: 0.5664 - val_loss: 1.4290 - val_accuracy: 0.5497\n","Epoch 7/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3034 - accuracy: 0.5791 - val_loss: 1.4087 - val_accuracy: 0.5561\n","Epoch 8/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2909 - accuracy: 0.5808 - val_loss: 1.3911 - val_accuracy: 0.5666\n","Epoch 9/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2674 - accuracy: 0.5874 - val_loss: 1.4307 - val_accuracy: 0.5504\n","Epoch 10/10\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2506 - accuracy: 0.5954 - val_loss: 1.3430 - val_accuracy: 0.5847\n"]}]},{"cell_type":"code","source":["# clearly we need some strong regularization with this small training set\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=200, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gzxusPl7sekI","executionInfo":{"status":"ok","timestamp":1652051836036,"user_tz":240,"elapsed":476097,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"129fedd4-7257-4c12-f9ea-32b819391090"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","301/301 [==============================] - 3s 8ms/step - loss: 2.0008 - accuracy: 0.3985 - val_loss: 5.3959 - val_accuracy: 0.0290\n","Epoch 2/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.6014 - accuracy: 0.4995 - val_loss: 1.5146 - val_accuracy: 0.5485\n","Epoch 3/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4771 - accuracy: 0.5347 - val_loss: 1.4771 - val_accuracy: 0.5510\n","Epoch 4/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4077 - accuracy: 0.5568 - val_loss: 1.4125 - val_accuracy: 0.5773\n","Epoch 5/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3845 - accuracy: 0.5639 - val_loss: 1.4821 - val_accuracy: 0.5394\n","Epoch 6/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3528 - accuracy: 0.5710 - val_loss: 1.4361 - val_accuracy: 0.5610\n","Epoch 7/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3294 - accuracy: 0.5735 - val_loss: 1.4488 - val_accuracy: 0.5538\n","Epoch 8/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3044 - accuracy: 0.5836 - val_loss: 1.4120 - val_accuracy: 0.5710\n","Epoch 9/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2948 - accuracy: 0.5880 - val_loss: 1.3881 - val_accuracy: 0.5719\n","Epoch 10/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2920 - accuracy: 0.5851 - val_loss: 1.4171 - val_accuracy: 0.5631\n","Epoch 11/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2749 - accuracy: 0.5907 - val_loss: 1.3852 - val_accuracy: 0.5794\n","Epoch 12/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2695 - accuracy: 0.5906 - val_loss: 1.4034 - val_accuracy: 0.5715\n","Epoch 13/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2584 - accuracy: 0.5949 - val_loss: 1.4099 - val_accuracy: 0.5656\n","Epoch 14/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2468 - accuracy: 0.5983 - val_loss: 1.4130 - val_accuracy: 0.5621\n","Epoch 15/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2442 - accuracy: 0.5956 - val_loss: 1.4317 - val_accuracy: 0.5556\n","Epoch 16/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2375 - accuracy: 0.5973 - val_loss: 1.4199 - val_accuracy: 0.5608\n","Epoch 17/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2466 - accuracy: 0.5959 - val_loss: 1.4395 - val_accuracy: 0.5555\n","Epoch 18/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2297 - accuracy: 0.6084 - val_loss: 1.4241 - val_accuracy: 0.5572\n","Epoch 19/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2235 - accuracy: 0.5998 - val_loss: 1.3971 - val_accuracy: 0.5695\n","Epoch 20/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2141 - accuracy: 0.6049 - val_loss: 1.4429 - val_accuracy: 0.5533\n","Epoch 21/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2162 - accuracy: 0.6065 - val_loss: 1.4482 - val_accuracy: 0.5478\n","Epoch 22/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2123 - accuracy: 0.6070 - val_loss: 1.4022 - val_accuracy: 0.5709\n","Epoch 23/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2030 - accuracy: 0.6079 - val_loss: 1.4012 - val_accuracy: 0.5668\n","Epoch 24/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2096 - accuracy: 0.6038 - val_loss: 1.4018 - val_accuracy: 0.5683\n","Epoch 25/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.2055 - accuracy: 0.6065 - val_loss: 1.3965 - val_accuracy: 0.5716\n","Epoch 26/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1986 - accuracy: 0.6080 - val_loss: 1.4185 - val_accuracy: 0.5626\n","Epoch 27/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1866 - accuracy: 0.6112 - val_loss: 1.4344 - val_accuracy: 0.5562\n","Epoch 28/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1931 - accuracy: 0.6124 - val_loss: 1.4018 - val_accuracy: 0.5676\n","Epoch 29/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1879 - accuracy: 0.6102 - val_loss: 1.3871 - val_accuracy: 0.5699\n","Epoch 30/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1878 - accuracy: 0.6094 - val_loss: 1.4697 - val_accuracy: 0.5415\n","Epoch 31/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1855 - accuracy: 0.6115 - val_loss: 1.4070 - val_accuracy: 0.5652\n","Epoch 32/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1836 - accuracy: 0.6152 - val_loss: 1.4280 - val_accuracy: 0.5557\n","Epoch 33/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1780 - accuracy: 0.6140 - val_loss: 1.4428 - val_accuracy: 0.5542\n","Epoch 34/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1719 - accuracy: 0.6195 - val_loss: 1.4130 - val_accuracy: 0.5668\n","Epoch 35/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1747 - accuracy: 0.6140 - val_loss: 1.4240 - val_accuracy: 0.5621\n","Epoch 36/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1721 - accuracy: 0.6178 - val_loss: 1.3882 - val_accuracy: 0.5752\n","Epoch 37/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1681 - accuracy: 0.6199 - val_loss: 1.4257 - val_accuracy: 0.5562\n","Epoch 38/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1712 - accuracy: 0.6114 - val_loss: 1.4268 - val_accuracy: 0.5596\n","Epoch 39/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1697 - accuracy: 0.6200 - val_loss: 1.4558 - val_accuracy: 0.5494\n","Epoch 40/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1605 - accuracy: 0.6237 - val_loss: 1.4181 - val_accuracy: 0.5636\n","Epoch 41/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1558 - accuracy: 0.6179 - val_loss: 1.4165 - val_accuracy: 0.5637\n","Epoch 42/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1613 - accuracy: 0.6200 - val_loss: 1.4105 - val_accuracy: 0.5648\n","Epoch 43/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1567 - accuracy: 0.6213 - val_loss: 1.4552 - val_accuracy: 0.5528\n","Epoch 44/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1488 - accuracy: 0.6230 - val_loss: 1.4419 - val_accuracy: 0.5530\n","Epoch 45/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1575 - accuracy: 0.6214 - val_loss: 1.4314 - val_accuracy: 0.5527\n","Epoch 46/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1443 - accuracy: 0.6208 - val_loss: 1.4549 - val_accuracy: 0.5465\n","Epoch 47/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1431 - accuracy: 0.6265 - val_loss: 1.4105 - val_accuracy: 0.5658\n","Epoch 48/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1413 - accuracy: 0.6253 - val_loss: 1.4669 - val_accuracy: 0.5430\n","Epoch 49/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1531 - accuracy: 0.6193 - val_loss: 1.4442 - val_accuracy: 0.5533\n","Epoch 50/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1454 - accuracy: 0.6194 - val_loss: 1.4295 - val_accuracy: 0.5563\n","Epoch 51/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1331 - accuracy: 0.6293 - val_loss: 1.3933 - val_accuracy: 0.5713\n","Epoch 52/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1440 - accuracy: 0.6193 - val_loss: 1.4392 - val_accuracy: 0.5536\n","Epoch 53/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1364 - accuracy: 0.6326 - val_loss: 1.4303 - val_accuracy: 0.5622\n","Epoch 54/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1388 - accuracy: 0.6274 - val_loss: 1.4359 - val_accuracy: 0.5568\n","Epoch 55/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1332 - accuracy: 0.6253 - val_loss: 1.4323 - val_accuracy: 0.5591\n","Epoch 56/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1353 - accuracy: 0.6258 - val_loss: 1.4379 - val_accuracy: 0.5546\n","Epoch 57/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1275 - accuracy: 0.6313 - val_loss: 1.4270 - val_accuracy: 0.5611\n","Epoch 58/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1221 - accuracy: 0.6333 - val_loss: 1.4917 - val_accuracy: 0.5387\n","Epoch 59/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1323 - accuracy: 0.6283 - val_loss: 1.4171 - val_accuracy: 0.5675\n","Epoch 60/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1265 - accuracy: 0.6325 - val_loss: 1.4470 - val_accuracy: 0.5536\n","Epoch 61/200\n","301/301 [==============================] - 3s 8ms/step - loss: 1.1336 - accuracy: 0.6288 - val_loss: 1.4210 - val_accuracy: 0.5602\n","Epoch 62/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1256 - accuracy: 0.6314 - val_loss: 1.4257 - val_accuracy: 0.5662\n","Epoch 63/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1218 - accuracy: 0.6332 - val_loss: 1.4229 - val_accuracy: 0.5603\n","Epoch 64/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1292 - accuracy: 0.6243 - val_loss: 1.4065 - val_accuracy: 0.5692\n","Epoch 65/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1194 - accuracy: 0.6300 - val_loss: 1.4237 - val_accuracy: 0.5623\n","Epoch 66/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1244 - accuracy: 0.6304 - val_loss: 1.4198 - val_accuracy: 0.5637\n","Epoch 67/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1250 - accuracy: 0.6292 - val_loss: 1.4833 - val_accuracy: 0.5402\n","Epoch 68/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1198 - accuracy: 0.6295 - val_loss: 1.4364 - val_accuracy: 0.5517\n","Epoch 69/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1214 - accuracy: 0.6277 - val_loss: 1.4176 - val_accuracy: 0.5647\n","Epoch 70/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1190 - accuracy: 0.6277 - val_loss: 1.4347 - val_accuracy: 0.5566\n","Epoch 71/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1178 - accuracy: 0.6351 - val_loss: 1.4405 - val_accuracy: 0.5548\n","Epoch 72/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1140 - accuracy: 0.6338 - val_loss: 1.4294 - val_accuracy: 0.5573\n","Epoch 73/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1098 - accuracy: 0.6353 - val_loss: 1.4137 - val_accuracy: 0.5663\n","Epoch 74/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1154 - accuracy: 0.6310 - val_loss: 1.4307 - val_accuracy: 0.5608\n","Epoch 75/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1154 - accuracy: 0.6322 - val_loss: 1.4080 - val_accuracy: 0.5664\n","Epoch 76/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1066 - accuracy: 0.6418 - val_loss: 1.4310 - val_accuracy: 0.5631\n","Epoch 77/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1093 - accuracy: 0.6386 - val_loss: 1.4660 - val_accuracy: 0.5448\n","Epoch 78/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1076 - accuracy: 0.6340 - val_loss: 1.4559 - val_accuracy: 0.5485\n","Epoch 79/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1206 - accuracy: 0.6342 - val_loss: 1.4618 - val_accuracy: 0.5490\n","Epoch 80/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1074 - accuracy: 0.6386 - val_loss: 1.3974 - val_accuracy: 0.5714\n","Epoch 81/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1068 - accuracy: 0.6364 - val_loss: 1.4425 - val_accuracy: 0.5543\n","Epoch 82/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1107 - accuracy: 0.6334 - val_loss: 1.4374 - val_accuracy: 0.5583\n","Epoch 83/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1131 - accuracy: 0.6339 - val_loss: 1.4571 - val_accuracy: 0.5493\n","Epoch 84/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1047 - accuracy: 0.6346 - val_loss: 1.4404 - val_accuracy: 0.5570\n","Epoch 85/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1046 - accuracy: 0.6356 - val_loss: 1.4509 - val_accuracy: 0.5548\n","Epoch 86/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1062 - accuracy: 0.6383 - val_loss: 1.4345 - val_accuracy: 0.5610\n","Epoch 87/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1082 - accuracy: 0.6297 - val_loss: 1.4589 - val_accuracy: 0.5490\n","Epoch 88/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1004 - accuracy: 0.6356 - val_loss: 1.4616 - val_accuracy: 0.5485\n","Epoch 89/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1043 - accuracy: 0.6361 - val_loss: 1.3937 - val_accuracy: 0.5717\n","Epoch 90/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1047 - accuracy: 0.6375 - val_loss: 1.4492 - val_accuracy: 0.5491\n","Epoch 91/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1151 - accuracy: 0.6357 - val_loss: 1.4367 - val_accuracy: 0.5579\n","Epoch 92/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0977 - accuracy: 0.6325 - val_loss: 1.4419 - val_accuracy: 0.5571\n","Epoch 93/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0998 - accuracy: 0.6388 - val_loss: 1.4178 - val_accuracy: 0.5649\n","Epoch 94/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0990 - accuracy: 0.6396 - val_loss: 1.4584 - val_accuracy: 0.5506\n","Epoch 95/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0916 - accuracy: 0.6417 - val_loss: 1.4548 - val_accuracy: 0.5514\n","Epoch 96/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0869 - accuracy: 0.6429 - val_loss: 1.4790 - val_accuracy: 0.5472\n","Epoch 97/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0997 - accuracy: 0.6359 - val_loss: 1.4299 - val_accuracy: 0.5616\n","Epoch 98/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0944 - accuracy: 0.6456 - val_loss: 1.4400 - val_accuracy: 0.5599\n","Epoch 99/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0897 - accuracy: 0.6413 - val_loss: 1.4687 - val_accuracy: 0.5438\n","Epoch 100/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.1001 - accuracy: 0.6374 - val_loss: 1.4704 - val_accuracy: 0.5504\n","Epoch 101/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0905 - accuracy: 0.6466 - val_loss: 1.4460 - val_accuracy: 0.5533\n","Epoch 102/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0884 - accuracy: 0.6435 - val_loss: 1.4330 - val_accuracy: 0.5609\n","Epoch 103/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0875 - accuracy: 0.6439 - val_loss: 1.4409 - val_accuracy: 0.5567\n","Epoch 104/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0819 - accuracy: 0.6442 - val_loss: 1.4361 - val_accuracy: 0.5611\n","Epoch 105/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0826 - accuracy: 0.6466 - val_loss: 1.4529 - val_accuracy: 0.5508\n","Epoch 106/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0860 - accuracy: 0.6424 - val_loss: 1.4044 - val_accuracy: 0.5727\n","Epoch 107/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0920 - accuracy: 0.6422 - val_loss: 1.4609 - val_accuracy: 0.5495\n","Epoch 108/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0893 - accuracy: 0.6418 - val_loss: 1.4602 - val_accuracy: 0.5520\n","Epoch 109/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0872 - accuracy: 0.6440 - val_loss: 1.4266 - val_accuracy: 0.5662\n","Epoch 110/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0924 - accuracy: 0.6421 - val_loss: 1.4453 - val_accuracy: 0.5542\n","Epoch 111/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0899 - accuracy: 0.6454 - val_loss: 1.4355 - val_accuracy: 0.5637\n","Epoch 112/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0841 - accuracy: 0.6474 - val_loss: 1.5089 - val_accuracy: 0.5307\n","Epoch 113/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0851 - accuracy: 0.6393 - val_loss: 1.4602 - val_accuracy: 0.5517\n","Epoch 114/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0834 - accuracy: 0.6450 - val_loss: 1.4413 - val_accuracy: 0.5602\n","Epoch 115/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0819 - accuracy: 0.6445 - val_loss: 1.4409 - val_accuracy: 0.5553\n","Epoch 116/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0864 - accuracy: 0.6402 - val_loss: 1.4594 - val_accuracy: 0.5502\n","Epoch 117/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0878 - accuracy: 0.6389 - val_loss: 1.4346 - val_accuracy: 0.5629\n","Epoch 118/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0819 - accuracy: 0.6421 - val_loss: 1.4618 - val_accuracy: 0.5486\n","Epoch 119/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0804 - accuracy: 0.6432 - val_loss: 1.4593 - val_accuracy: 0.5537\n","Epoch 120/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0817 - accuracy: 0.6428 - val_loss: 1.4586 - val_accuracy: 0.5532\n","Epoch 121/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0851 - accuracy: 0.6429 - val_loss: 1.4539 - val_accuracy: 0.5499\n","Epoch 122/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0881 - accuracy: 0.6396 - val_loss: 1.4699 - val_accuracy: 0.5437\n","Epoch 123/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0725 - accuracy: 0.6421 - val_loss: 1.4893 - val_accuracy: 0.5390\n","Epoch 124/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0768 - accuracy: 0.6443 - val_loss: 1.4296 - val_accuracy: 0.5645\n","Epoch 125/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0747 - accuracy: 0.6420 - val_loss: 1.4651 - val_accuracy: 0.5492\n","Epoch 126/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0829 - accuracy: 0.6474 - val_loss: 1.4239 - val_accuracy: 0.5648\n","Epoch 127/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0775 - accuracy: 0.6474 - val_loss: 1.4676 - val_accuracy: 0.5492\n","Epoch 128/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0779 - accuracy: 0.6444 - val_loss: 1.4670 - val_accuracy: 0.5470\n","Epoch 129/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0795 - accuracy: 0.6460 - val_loss: 1.4410 - val_accuracy: 0.5575\n","Epoch 130/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0787 - accuracy: 0.6423 - val_loss: 1.4592 - val_accuracy: 0.5511\n","Epoch 131/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0718 - accuracy: 0.6444 - val_loss: 1.4745 - val_accuracy: 0.5451\n","Epoch 132/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0741 - accuracy: 0.6416 - val_loss: 1.4428 - val_accuracy: 0.5605\n","Epoch 133/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0733 - accuracy: 0.6421 - val_loss: 1.4657 - val_accuracy: 0.5503\n","Epoch 134/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0677 - accuracy: 0.6486 - val_loss: 1.4761 - val_accuracy: 0.5467\n","Epoch 135/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0688 - accuracy: 0.6457 - val_loss: 1.4596 - val_accuracy: 0.5528\n","Epoch 136/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0812 - accuracy: 0.6449 - val_loss: 1.4645 - val_accuracy: 0.5540\n","Epoch 137/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0751 - accuracy: 0.6402 - val_loss: 1.4575 - val_accuracy: 0.5546\n","Epoch 138/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0753 - accuracy: 0.6439 - val_loss: 1.4602 - val_accuracy: 0.5505\n","Epoch 139/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0661 - accuracy: 0.6478 - val_loss: 1.4690 - val_accuracy: 0.5493\n","Epoch 140/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0662 - accuracy: 0.6495 - val_loss: 1.4800 - val_accuracy: 0.5462\n","Epoch 141/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0712 - accuracy: 0.6479 - val_loss: 1.4732 - val_accuracy: 0.5444\n","Epoch 142/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0765 - accuracy: 0.6449 - val_loss: 1.4566 - val_accuracy: 0.5549\n","Epoch 143/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0678 - accuracy: 0.6492 - val_loss: 1.4523 - val_accuracy: 0.5529\n","Epoch 144/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0703 - accuracy: 0.6481 - val_loss: 1.4540 - val_accuracy: 0.5583\n","Epoch 145/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0658 - accuracy: 0.6437 - val_loss: 1.4425 - val_accuracy: 0.5609\n","Epoch 146/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0701 - accuracy: 0.6496 - val_loss: 1.4489 - val_accuracy: 0.5545\n","Epoch 147/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0691 - accuracy: 0.6462 - val_loss: 1.4604 - val_accuracy: 0.5548\n","Epoch 148/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0676 - accuracy: 0.6492 - val_loss: 1.4711 - val_accuracy: 0.5482\n","Epoch 149/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0618 - accuracy: 0.6526 - val_loss: 1.4533 - val_accuracy: 0.5568\n","Epoch 150/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0705 - accuracy: 0.6491 - val_loss: 1.4810 - val_accuracy: 0.5470\n","Epoch 151/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0619 - accuracy: 0.6493 - val_loss: 1.4534 - val_accuracy: 0.5570\n","Epoch 152/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0675 - accuracy: 0.6522 - val_loss: 1.4650 - val_accuracy: 0.5517\n","Epoch 153/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0614 - accuracy: 0.6542 - val_loss: 1.4747 - val_accuracy: 0.5508\n","Epoch 154/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0631 - accuracy: 0.6446 - val_loss: 1.4449 - val_accuracy: 0.5601\n","Epoch 155/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0724 - accuracy: 0.6458 - val_loss: 1.4792 - val_accuracy: 0.5447\n","Epoch 156/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0704 - accuracy: 0.6488 - val_loss: 1.4718 - val_accuracy: 0.5478\n","Epoch 157/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0539 - accuracy: 0.6535 - val_loss: 1.4666 - val_accuracy: 0.5484\n","Epoch 158/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0635 - accuracy: 0.6473 - val_loss: 1.4656 - val_accuracy: 0.5514\n","Epoch 159/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0626 - accuracy: 0.6514 - val_loss: 1.4705 - val_accuracy: 0.5468\n","Epoch 160/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0683 - accuracy: 0.6445 - val_loss: 1.4639 - val_accuracy: 0.5524\n","Epoch 161/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0603 - accuracy: 0.6474 - val_loss: 1.4635 - val_accuracy: 0.5540\n","Epoch 162/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0682 - accuracy: 0.6448 - val_loss: 1.4685 - val_accuracy: 0.5533\n","Epoch 163/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0678 - accuracy: 0.6452 - val_loss: 1.4538 - val_accuracy: 0.5548\n","Epoch 164/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0637 - accuracy: 0.6489 - val_loss: 1.4527 - val_accuracy: 0.5550\n","Epoch 165/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0554 - accuracy: 0.6524 - val_loss: 1.4270 - val_accuracy: 0.5688\n","Epoch 166/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0573 - accuracy: 0.6459 - val_loss: 1.4357 - val_accuracy: 0.5643\n","Epoch 167/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0568 - accuracy: 0.6504 - val_loss: 1.4390 - val_accuracy: 0.5632\n","Epoch 168/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0484 - accuracy: 0.6527 - val_loss: 1.4722 - val_accuracy: 0.5488\n","Epoch 169/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0614 - accuracy: 0.6484 - val_loss: 1.4839 - val_accuracy: 0.5458\n","Epoch 170/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0647 - accuracy: 0.6508 - val_loss: 1.4658 - val_accuracy: 0.5510\n","Epoch 171/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0538 - accuracy: 0.6528 - val_loss: 1.4860 - val_accuracy: 0.5458\n","Epoch 172/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0539 - accuracy: 0.6503 - val_loss: 1.4773 - val_accuracy: 0.5512\n","Epoch 173/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0541 - accuracy: 0.6527 - val_loss: 1.4895 - val_accuracy: 0.5401\n","Epoch 174/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0604 - accuracy: 0.6517 - val_loss: 1.5047 - val_accuracy: 0.5373\n","Epoch 175/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0541 - accuracy: 0.6505 - val_loss: 1.4807 - val_accuracy: 0.5445\n","Epoch 176/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0494 - accuracy: 0.6545 - val_loss: 1.4747 - val_accuracy: 0.5540\n","Epoch 177/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0559 - accuracy: 0.6537 - val_loss: 1.4528 - val_accuracy: 0.5619\n","Epoch 178/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0629 - accuracy: 0.6484 - val_loss: 1.4613 - val_accuracy: 0.5530\n","Epoch 179/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0643 - accuracy: 0.6479 - val_loss: 1.4847 - val_accuracy: 0.5456\n","Epoch 180/200\n","301/301 [==============================] - 3s 8ms/step - loss: 1.0520 - accuracy: 0.6521 - val_loss: 1.4526 - val_accuracy: 0.5560\n","Epoch 181/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0586 - accuracy: 0.6510 - val_loss: 1.4809 - val_accuracy: 0.5515\n","Epoch 182/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0527 - accuracy: 0.6531 - val_loss: 1.4648 - val_accuracy: 0.5538\n","Epoch 183/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0592 - accuracy: 0.6489 - val_loss: 1.4608 - val_accuracy: 0.5534\n","Epoch 184/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0560 - accuracy: 0.6490 - val_loss: 1.4755 - val_accuracy: 0.5465\n","Epoch 185/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0515 - accuracy: 0.6503 - val_loss: 1.4520 - val_accuracy: 0.5616\n","Epoch 186/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0498 - accuracy: 0.6502 - val_loss: 1.4865 - val_accuracy: 0.5476\n","Epoch 187/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0535 - accuracy: 0.6524 - val_loss: 1.4489 - val_accuracy: 0.5578\n","Epoch 188/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0620 - accuracy: 0.6477 - val_loss: 1.4618 - val_accuracy: 0.5535\n","Epoch 189/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0627 - accuracy: 0.6515 - val_loss: 1.4660 - val_accuracy: 0.5520\n","Epoch 190/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0632 - accuracy: 0.6501 - val_loss: 1.4704 - val_accuracy: 0.5516\n","Epoch 191/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0461 - accuracy: 0.6530 - val_loss: 1.4656 - val_accuracy: 0.5552\n","Epoch 192/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0512 - accuracy: 0.6476 - val_loss: 1.4925 - val_accuracy: 0.5418\n","Epoch 193/200\n","301/301 [==============================] - 3s 8ms/step - loss: 1.0532 - accuracy: 0.6504 - val_loss: 1.4957 - val_accuracy: 0.5403\n","Epoch 194/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0516 - accuracy: 0.6544 - val_loss: 1.4405 - val_accuracy: 0.5621\n","Epoch 195/200\n","301/301 [==============================] - 3s 8ms/step - loss: 1.0448 - accuracy: 0.6505 - val_loss: 1.4561 - val_accuracy: 0.5565\n","Epoch 196/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0539 - accuracy: 0.6536 - val_loss: 1.4542 - val_accuracy: 0.5574\n","Epoch 197/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0582 - accuracy: 0.6486 - val_loss: 1.4722 - val_accuracy: 0.5525\n","Epoch 198/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0419 - accuracy: 0.6468 - val_loss: 1.4675 - val_accuracy: 0.5528\n","Epoch 199/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0518 - accuracy: 0.6531 - val_loss: 1.5084 - val_accuracy: 0.5358\n","Epoch 200/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.0503 - accuracy: 0.6511 - val_loss: 1.4872 - val_accuracy: 0.5444\n"]}]},{"cell_type":"code","source":["# clearly we need some strong regularization with this small training set\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=200, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvMApe85ttHM","executionInfo":{"status":"ok","timestamp":1652052278619,"user_tz":240,"elapsed":442634,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"9461898e-3e86-4234-c44e-5fe0d4d2a1e0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2433 - accuracy: 0.3251 - val_loss: 3.9729 - val_accuracy: 0.0607\n","Epoch 2/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8108 - accuracy: 0.4452 - val_loss: 1.6598 - val_accuracy: 0.5053\n","Epoch 3/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.6657 - accuracy: 0.4802 - val_loss: 1.5559 - val_accuracy: 0.5199\n","Epoch 4/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.6015 - accuracy: 0.4943 - val_loss: 1.4788 - val_accuracy: 0.5499\n","Epoch 5/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.5610 - accuracy: 0.5060 - val_loss: 1.4376 - val_accuracy: 0.5594\n","Epoch 6/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.5520 - accuracy: 0.5082 - val_loss: 1.4373 - val_accuracy: 0.5554\n","Epoch 7/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5282 - accuracy: 0.5187 - val_loss: 1.4270 - val_accuracy: 0.5567\n","Epoch 8/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5195 - accuracy: 0.5123 - val_loss: 1.4052 - val_accuracy: 0.5662\n","Epoch 9/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5124 - accuracy: 0.5140 - val_loss: 1.4222 - val_accuracy: 0.5584\n","Epoch 10/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4982 - accuracy: 0.5231 - val_loss: 1.4001 - val_accuracy: 0.5699\n","Epoch 11/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4835 - accuracy: 0.5291 - val_loss: 1.4260 - val_accuracy: 0.5527\n","Epoch 12/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4895 - accuracy: 0.5203 - val_loss: 1.4376 - val_accuracy: 0.5518\n","Epoch 13/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4844 - accuracy: 0.5249 - val_loss: 1.4109 - val_accuracy: 0.5613\n","Epoch 14/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4811 - accuracy: 0.5230 - val_loss: 1.4268 - val_accuracy: 0.5559\n","Epoch 15/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4691 - accuracy: 0.5275 - val_loss: 1.4250 - val_accuracy: 0.5563\n","Epoch 16/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4724 - accuracy: 0.5251 - val_loss: 1.4212 - val_accuracy: 0.5563\n","Epoch 17/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4712 - accuracy: 0.5265 - val_loss: 1.4142 - val_accuracy: 0.5661\n","Epoch 18/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4626 - accuracy: 0.5310 - val_loss: 1.4272 - val_accuracy: 0.5570\n","Epoch 19/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4694 - accuracy: 0.5290 - val_loss: 1.4333 - val_accuracy: 0.5525\n","Epoch 20/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4678 - accuracy: 0.5265 - val_loss: 1.4190 - val_accuracy: 0.5634\n","Epoch 21/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4623 - accuracy: 0.5325 - val_loss: 1.4185 - val_accuracy: 0.5601\n","Epoch 22/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4610 - accuracy: 0.5244 - val_loss: 1.4026 - val_accuracy: 0.5680\n","Epoch 23/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4525 - accuracy: 0.5351 - val_loss: 1.4035 - val_accuracy: 0.5682\n","Epoch 24/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4502 - accuracy: 0.5312 - val_loss: 1.4105 - val_accuracy: 0.5648\n","Epoch 25/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4572 - accuracy: 0.5291 - val_loss: 1.3966 - val_accuracy: 0.5678\n","Epoch 26/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4445 - accuracy: 0.5383 - val_loss: 1.3944 - val_accuracy: 0.5719\n","Epoch 27/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4665 - accuracy: 0.5291 - val_loss: 1.4031 - val_accuracy: 0.5681\n","Epoch 28/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4435 - accuracy: 0.5359 - val_loss: 1.4028 - val_accuracy: 0.5702\n","Epoch 29/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4402 - accuracy: 0.5323 - val_loss: 1.3940 - val_accuracy: 0.5747\n","Epoch 30/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4429 - accuracy: 0.5349 - val_loss: 1.4101 - val_accuracy: 0.5642\n","Epoch 31/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4415 - accuracy: 0.5310 - val_loss: 1.4193 - val_accuracy: 0.5580\n","Epoch 32/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4515 - accuracy: 0.5298 - val_loss: 1.4098 - val_accuracy: 0.5626\n","Epoch 33/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4413 - accuracy: 0.5372 - val_loss: 1.4011 - val_accuracy: 0.5674\n","Epoch 34/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4456 - accuracy: 0.5306 - val_loss: 1.3918 - val_accuracy: 0.5737\n","Epoch 35/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4454 - accuracy: 0.5339 - val_loss: 1.4025 - val_accuracy: 0.5671\n","Epoch 36/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4430 - accuracy: 0.5364 - val_loss: 1.3985 - val_accuracy: 0.5675\n","Epoch 37/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4401 - accuracy: 0.5311 - val_loss: 1.4075 - val_accuracy: 0.5617\n","Epoch 38/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4410 - accuracy: 0.5282 - val_loss: 1.4179 - val_accuracy: 0.5579\n","Epoch 39/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4416 - accuracy: 0.5289 - val_loss: 1.3998 - val_accuracy: 0.5662\n","Epoch 40/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4448 - accuracy: 0.5350 - val_loss: 1.3976 - val_accuracy: 0.5679\n","Epoch 41/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4306 - accuracy: 0.5417 - val_loss: 1.3844 - val_accuracy: 0.5696\n","Epoch 42/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4324 - accuracy: 0.5400 - val_loss: 1.4005 - val_accuracy: 0.5661\n","Epoch 43/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4342 - accuracy: 0.5383 - val_loss: 1.3977 - val_accuracy: 0.5671\n","Epoch 44/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4358 - accuracy: 0.5369 - val_loss: 1.3916 - val_accuracy: 0.5698\n","Epoch 45/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4305 - accuracy: 0.5307 - val_loss: 1.3838 - val_accuracy: 0.5732\n","Epoch 46/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4374 - accuracy: 0.5367 - val_loss: 1.4008 - val_accuracy: 0.5669\n","Epoch 47/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4280 - accuracy: 0.5387 - val_loss: 1.3778 - val_accuracy: 0.5728\n","Epoch 48/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4356 - accuracy: 0.5278 - val_loss: 1.3875 - val_accuracy: 0.5713\n","Epoch 49/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4328 - accuracy: 0.5369 - val_loss: 1.3790 - val_accuracy: 0.5748\n","Epoch 50/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4326 - accuracy: 0.5365 - val_loss: 1.4036 - val_accuracy: 0.5660\n","Epoch 51/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4229 - accuracy: 0.5375 - val_loss: 1.4061 - val_accuracy: 0.5633\n","Epoch 52/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4272 - accuracy: 0.5369 - val_loss: 1.3952 - val_accuracy: 0.5658\n","Epoch 53/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4238 - accuracy: 0.5405 - val_loss: 1.4002 - val_accuracy: 0.5649\n","Epoch 54/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4330 - accuracy: 0.5324 - val_loss: 1.4098 - val_accuracy: 0.5634\n","Epoch 55/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4252 - accuracy: 0.5372 - val_loss: 1.4088 - val_accuracy: 0.5603\n","Epoch 56/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4185 - accuracy: 0.5385 - val_loss: 1.3752 - val_accuracy: 0.5751\n","Epoch 57/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4315 - accuracy: 0.5352 - val_loss: 1.3958 - val_accuracy: 0.5710\n","Epoch 58/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4255 - accuracy: 0.5389 - val_loss: 1.3985 - val_accuracy: 0.5661\n","Epoch 59/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4104 - accuracy: 0.5462 - val_loss: 1.3837 - val_accuracy: 0.5681\n","Epoch 60/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4236 - accuracy: 0.5362 - val_loss: 1.3928 - val_accuracy: 0.5654\n","Epoch 61/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4312 - accuracy: 0.5371 - val_loss: 1.3880 - val_accuracy: 0.5715\n","Epoch 62/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4290 - accuracy: 0.5348 - val_loss: 1.4051 - val_accuracy: 0.5654\n","Epoch 63/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4209 - accuracy: 0.5385 - val_loss: 1.4012 - val_accuracy: 0.5605\n","Epoch 64/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4199 - accuracy: 0.5373 - val_loss: 1.3844 - val_accuracy: 0.5691\n","Epoch 65/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4292 - accuracy: 0.5392 - val_loss: 1.4070 - val_accuracy: 0.5606\n","Epoch 66/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4204 - accuracy: 0.5365 - val_loss: 1.3910 - val_accuracy: 0.5677\n","Epoch 67/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4271 - accuracy: 0.5407 - val_loss: 1.4064 - val_accuracy: 0.5622\n","Epoch 68/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4206 - accuracy: 0.5362 - val_loss: 1.3999 - val_accuracy: 0.5623\n","Epoch 69/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4106 - accuracy: 0.5447 - val_loss: 1.3953 - val_accuracy: 0.5647\n","Epoch 70/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4143 - accuracy: 0.5448 - val_loss: 1.3956 - val_accuracy: 0.5674\n","Epoch 71/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4288 - accuracy: 0.5411 - val_loss: 1.3872 - val_accuracy: 0.5711\n","Epoch 72/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4251 - accuracy: 0.5373 - val_loss: 1.3755 - val_accuracy: 0.5712\n","Epoch 73/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4222 - accuracy: 0.5407 - val_loss: 1.3958 - val_accuracy: 0.5657\n","Epoch 74/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4201 - accuracy: 0.5445 - val_loss: 1.3895 - val_accuracy: 0.5676\n","Epoch 75/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4140 - accuracy: 0.5416 - val_loss: 1.3924 - val_accuracy: 0.5633\n","Epoch 76/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4198 - accuracy: 0.5365 - val_loss: 1.3703 - val_accuracy: 0.5759\n","Epoch 77/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4239 - accuracy: 0.5388 - val_loss: 1.3829 - val_accuracy: 0.5698\n","Epoch 78/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4214 - accuracy: 0.5346 - val_loss: 1.3882 - val_accuracy: 0.5650\n","Epoch 79/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4269 - accuracy: 0.5403 - val_loss: 1.3779 - val_accuracy: 0.5704\n","Epoch 80/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4093 - accuracy: 0.5442 - val_loss: 1.4003 - val_accuracy: 0.5641\n","Epoch 81/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4079 - accuracy: 0.5404 - val_loss: 1.3856 - val_accuracy: 0.5677\n","Epoch 82/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4146 - accuracy: 0.5371 - val_loss: 1.3945 - val_accuracy: 0.5664\n","Epoch 83/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4221 - accuracy: 0.5371 - val_loss: 1.3902 - val_accuracy: 0.5671\n","Epoch 84/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4350 - accuracy: 0.5326 - val_loss: 1.4061 - val_accuracy: 0.5645\n","Epoch 85/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4184 - accuracy: 0.5427 - val_loss: 1.3889 - val_accuracy: 0.5657\n","Epoch 86/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4256 - accuracy: 0.5374 - val_loss: 1.4065 - val_accuracy: 0.5600\n","Epoch 87/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4244 - accuracy: 0.5419 - val_loss: 1.3882 - val_accuracy: 0.5662\n","Epoch 88/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4107 - accuracy: 0.5470 - val_loss: 1.3983 - val_accuracy: 0.5634\n","Epoch 89/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4222 - accuracy: 0.5363 - val_loss: 1.3771 - val_accuracy: 0.5731\n","Epoch 90/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4057 - accuracy: 0.5463 - val_loss: 1.3920 - val_accuracy: 0.5635\n","Epoch 91/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4130 - accuracy: 0.5443 - val_loss: 1.3997 - val_accuracy: 0.5648\n","Epoch 92/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4118 - accuracy: 0.5370 - val_loss: 1.3897 - val_accuracy: 0.5658\n","Epoch 93/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4142 - accuracy: 0.5385 - val_loss: 1.3798 - val_accuracy: 0.5733\n","Epoch 94/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4132 - accuracy: 0.5455 - val_loss: 1.3840 - val_accuracy: 0.5672\n","Epoch 95/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4056 - accuracy: 0.5400 - val_loss: 1.3907 - val_accuracy: 0.5663\n","Epoch 96/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4254 - accuracy: 0.5331 - val_loss: 1.3899 - val_accuracy: 0.5666\n","Epoch 97/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4092 - accuracy: 0.5384 - val_loss: 1.3810 - val_accuracy: 0.5695\n","Epoch 98/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4116 - accuracy: 0.5402 - val_loss: 1.3898 - val_accuracy: 0.5683\n","Epoch 99/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4084 - accuracy: 0.5442 - val_loss: 1.3919 - val_accuracy: 0.5629\n","Epoch 100/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4160 - accuracy: 0.5376 - val_loss: 1.3895 - val_accuracy: 0.5658\n","Epoch 101/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4084 - accuracy: 0.5438 - val_loss: 1.3904 - val_accuracy: 0.5664\n","Epoch 102/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4084 - accuracy: 0.5486 - val_loss: 1.3846 - val_accuracy: 0.5663\n","Epoch 103/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4106 - accuracy: 0.5427 - val_loss: 1.3834 - val_accuracy: 0.5685\n","Epoch 104/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4116 - accuracy: 0.5365 - val_loss: 1.3956 - val_accuracy: 0.5670\n","Epoch 105/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4129 - accuracy: 0.5407 - val_loss: 1.3997 - val_accuracy: 0.5612\n","Epoch 106/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4134 - accuracy: 0.5425 - val_loss: 1.4038 - val_accuracy: 0.5641\n","Epoch 107/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4074 - accuracy: 0.5411 - val_loss: 1.3948 - val_accuracy: 0.5681\n","Epoch 108/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4145 - accuracy: 0.5435 - val_loss: 1.3855 - val_accuracy: 0.5693\n","Epoch 109/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4160 - accuracy: 0.5453 - val_loss: 1.3902 - val_accuracy: 0.5704\n","Epoch 110/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4134 - accuracy: 0.5415 - val_loss: 1.3833 - val_accuracy: 0.5691\n","Epoch 111/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4188 - accuracy: 0.5353 - val_loss: 1.3808 - val_accuracy: 0.5676\n","Epoch 112/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4185 - accuracy: 0.5406 - val_loss: 1.3806 - val_accuracy: 0.5717\n","Epoch 113/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4163 - accuracy: 0.5394 - val_loss: 1.3959 - val_accuracy: 0.5638\n","Epoch 114/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4237 - accuracy: 0.5388 - val_loss: 1.3944 - val_accuracy: 0.5631\n","Epoch 115/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4083 - accuracy: 0.5442 - val_loss: 1.3814 - val_accuracy: 0.5678\n","Epoch 116/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4068 - accuracy: 0.5401 - val_loss: 1.3815 - val_accuracy: 0.5707\n","Epoch 117/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4023 - accuracy: 0.5478 - val_loss: 1.3904 - val_accuracy: 0.5652\n","Epoch 118/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4104 - accuracy: 0.5443 - val_loss: 1.3847 - val_accuracy: 0.5675\n","Epoch 119/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4083 - accuracy: 0.5422 - val_loss: 1.3857 - val_accuracy: 0.5698\n","Epoch 120/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4184 - accuracy: 0.5416 - val_loss: 1.3790 - val_accuracy: 0.5711\n","Epoch 121/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4043 - accuracy: 0.5457 - val_loss: 1.3815 - val_accuracy: 0.5710\n","Epoch 122/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4196 - accuracy: 0.5375 - val_loss: 1.3766 - val_accuracy: 0.5705\n","Epoch 123/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4051 - accuracy: 0.5437 - val_loss: 1.3878 - val_accuracy: 0.5683\n","Epoch 124/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4036 - accuracy: 0.5440 - val_loss: 1.3869 - val_accuracy: 0.5676\n","Epoch 125/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4040 - accuracy: 0.5455 - val_loss: 1.3925 - val_accuracy: 0.5652\n","Epoch 126/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4052 - accuracy: 0.5429 - val_loss: 1.3708 - val_accuracy: 0.5747\n","Epoch 127/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4057 - accuracy: 0.5445 - val_loss: 1.3819 - val_accuracy: 0.5678\n","Epoch 128/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4104 - accuracy: 0.5435 - val_loss: 1.3819 - val_accuracy: 0.5708\n","Epoch 129/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4048 - accuracy: 0.5431 - val_loss: 1.4016 - val_accuracy: 0.5607\n","Epoch 130/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4119 - accuracy: 0.5405 - val_loss: 1.3874 - val_accuracy: 0.5691\n","Epoch 131/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4114 - accuracy: 0.5481 - val_loss: 1.3852 - val_accuracy: 0.5665\n","Epoch 132/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4092 - accuracy: 0.5478 - val_loss: 1.3801 - val_accuracy: 0.5698\n","Epoch 133/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4074 - accuracy: 0.5433 - val_loss: 1.3779 - val_accuracy: 0.5709\n","Epoch 134/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4185 - accuracy: 0.5413 - val_loss: 1.3817 - val_accuracy: 0.5684\n","Epoch 135/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4050 - accuracy: 0.5438 - val_loss: 1.3958 - val_accuracy: 0.5625\n","Epoch 136/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4092 - accuracy: 0.5433 - val_loss: 1.3780 - val_accuracy: 0.5704\n","Epoch 137/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4138 - accuracy: 0.5446 - val_loss: 1.3832 - val_accuracy: 0.5650\n","Epoch 138/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4012 - accuracy: 0.5420 - val_loss: 1.3783 - val_accuracy: 0.5698\n","Epoch 139/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4013 - accuracy: 0.5466 - val_loss: 1.3852 - val_accuracy: 0.5676\n","Epoch 140/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4109 - accuracy: 0.5428 - val_loss: 1.3839 - val_accuracy: 0.5671\n","Epoch 141/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4052 - accuracy: 0.5415 - val_loss: 1.3758 - val_accuracy: 0.5733\n","Epoch 142/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4065 - accuracy: 0.5476 - val_loss: 1.3818 - val_accuracy: 0.5699\n","Epoch 143/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4079 - accuracy: 0.5416 - val_loss: 1.3887 - val_accuracy: 0.5671\n","Epoch 144/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3990 - accuracy: 0.5429 - val_loss: 1.3868 - val_accuracy: 0.5704\n","Epoch 145/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4105 - accuracy: 0.5452 - val_loss: 1.3889 - val_accuracy: 0.5671\n","Epoch 146/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4021 - accuracy: 0.5460 - val_loss: 1.3749 - val_accuracy: 0.5725\n","Epoch 147/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4053 - accuracy: 0.5427 - val_loss: 1.3961 - val_accuracy: 0.5625\n","Epoch 148/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4142 - accuracy: 0.5432 - val_loss: 1.3857 - val_accuracy: 0.5656\n","Epoch 149/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4159 - accuracy: 0.5385 - val_loss: 1.3686 - val_accuracy: 0.5748\n","Epoch 150/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4021 - accuracy: 0.5459 - val_loss: 1.3756 - val_accuracy: 0.5714\n","Epoch 151/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4093 - accuracy: 0.5447 - val_loss: 1.3698 - val_accuracy: 0.5740\n","Epoch 152/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3961 - accuracy: 0.5488 - val_loss: 1.3795 - val_accuracy: 0.5723\n","Epoch 153/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3974 - accuracy: 0.5472 - val_loss: 1.3760 - val_accuracy: 0.5741\n","Epoch 154/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3898 - accuracy: 0.5492 - val_loss: 1.3741 - val_accuracy: 0.5728\n","Epoch 155/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4082 - accuracy: 0.5424 - val_loss: 1.3719 - val_accuracy: 0.5714\n","Epoch 156/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4010 - accuracy: 0.5460 - val_loss: 1.3823 - val_accuracy: 0.5656\n","Epoch 157/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4041 - accuracy: 0.5443 - val_loss: 1.3867 - val_accuracy: 0.5639\n","Epoch 158/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3961 - accuracy: 0.5499 - val_loss: 1.3912 - val_accuracy: 0.5626\n","Epoch 159/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4040 - accuracy: 0.5471 - val_loss: 1.3849 - val_accuracy: 0.5680\n","Epoch 160/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4026 - accuracy: 0.5465 - val_loss: 1.3775 - val_accuracy: 0.5706\n","Epoch 161/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4069 - accuracy: 0.5430 - val_loss: 1.3878 - val_accuracy: 0.5675\n","Epoch 162/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3935 - accuracy: 0.5470 - val_loss: 1.3878 - val_accuracy: 0.5674\n","Epoch 163/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4057 - accuracy: 0.5454 - val_loss: 1.3840 - val_accuracy: 0.5687\n","Epoch 164/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4062 - accuracy: 0.5473 - val_loss: 1.3889 - val_accuracy: 0.5686\n","Epoch 165/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4066 - accuracy: 0.5460 - val_loss: 1.3770 - val_accuracy: 0.5712\n","Epoch 166/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3950 - accuracy: 0.5469 - val_loss: 1.3762 - val_accuracy: 0.5701\n","Epoch 167/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3983 - accuracy: 0.5472 - val_loss: 1.3790 - val_accuracy: 0.5674\n","Epoch 168/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4092 - accuracy: 0.5444 - val_loss: 1.3746 - val_accuracy: 0.5693\n","Epoch 169/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4001 - accuracy: 0.5426 - val_loss: 1.3776 - val_accuracy: 0.5651\n","Epoch 170/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3939 - accuracy: 0.5458 - val_loss: 1.3827 - val_accuracy: 0.5689\n","Epoch 171/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4025 - accuracy: 0.5470 - val_loss: 1.3904 - val_accuracy: 0.5642\n","Epoch 172/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4140 - accuracy: 0.5443 - val_loss: 1.3896 - val_accuracy: 0.5653\n","Epoch 173/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4024 - accuracy: 0.5464 - val_loss: 1.3968 - val_accuracy: 0.5613\n","Epoch 174/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3998 - accuracy: 0.5430 - val_loss: 1.3739 - val_accuracy: 0.5724\n","Epoch 175/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3986 - accuracy: 0.5495 - val_loss: 1.3988 - val_accuracy: 0.5626\n","Epoch 176/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3964 - accuracy: 0.5495 - val_loss: 1.3837 - val_accuracy: 0.5665\n","Epoch 177/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4024 - accuracy: 0.5444 - val_loss: 1.3832 - val_accuracy: 0.5675\n","Epoch 178/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3896 - accuracy: 0.5494 - val_loss: 1.3721 - val_accuracy: 0.5728\n","Epoch 179/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4015 - accuracy: 0.5468 - val_loss: 1.3874 - val_accuracy: 0.5671\n","Epoch 180/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4025 - accuracy: 0.5455 - val_loss: 1.3858 - val_accuracy: 0.5704\n","Epoch 181/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4113 - accuracy: 0.5447 - val_loss: 1.3734 - val_accuracy: 0.5725\n","Epoch 182/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4033 - accuracy: 0.5451 - val_loss: 1.3902 - val_accuracy: 0.5640\n","Epoch 183/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3973 - accuracy: 0.5498 - val_loss: 1.3755 - val_accuracy: 0.5719\n","Epoch 184/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4004 - accuracy: 0.5469 - val_loss: 1.3806 - val_accuracy: 0.5701\n","Epoch 185/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4008 - accuracy: 0.5447 - val_loss: 1.3870 - val_accuracy: 0.5671\n","Epoch 186/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4063 - accuracy: 0.5410 - val_loss: 1.3710 - val_accuracy: 0.5750\n","Epoch 187/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4003 - accuracy: 0.5415 - val_loss: 1.3864 - val_accuracy: 0.5661\n","Epoch 188/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4134 - accuracy: 0.5439 - val_loss: 1.3822 - val_accuracy: 0.5675\n","Epoch 189/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3955 - accuracy: 0.5490 - val_loss: 1.3916 - val_accuracy: 0.5624\n","Epoch 190/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4099 - accuracy: 0.5410 - val_loss: 1.3792 - val_accuracy: 0.5719\n","Epoch 191/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4040 - accuracy: 0.5466 - val_loss: 1.3782 - val_accuracy: 0.5675\n","Epoch 192/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3967 - accuracy: 0.5406 - val_loss: 1.3865 - val_accuracy: 0.5680\n","Epoch 193/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4004 - accuracy: 0.5433 - val_loss: 1.3808 - val_accuracy: 0.5685\n","Epoch 194/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3898 - accuracy: 0.5484 - val_loss: 1.3855 - val_accuracy: 0.5643\n","Epoch 195/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4000 - accuracy: 0.5433 - val_loss: 1.3801 - val_accuracy: 0.5695\n","Epoch 196/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3981 - accuracy: 0.5483 - val_loss: 1.3731 - val_accuracy: 0.5718\n","Epoch 197/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3949 - accuracy: 0.5482 - val_loss: 1.3791 - val_accuracy: 0.5687\n","Epoch 198/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4023 - accuracy: 0.5423 - val_loss: 1.3765 - val_accuracy: 0.5695\n","Epoch 199/200\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3951 - accuracy: 0.5537 - val_loss: 1.3675 - val_accuracy: 0.5725\n","Epoch 200/200\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4030 - accuracy: 0.5419 - val_loss: 1.3868 - val_accuracy: 0.5667\n"]}]},{"cell_type":"code","source":["# the previous model is starting to look like a real neural network, but one that's overfitting\n","# maybe more complexity could also help it get toward 100% also, but overfitting is the main concern\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(8,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-o3QVBFb0YRQ","executionInfo":{"status":"ok","timestamp":1652052530456,"user_tz":240,"elapsed":251886,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"faac26d4-fa59-4b9d-e351-2ad511077892"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 4s 9ms/step - loss: 14.2773 - accuracy: 0.3241 - val_loss: 4.5125 - val_accuracy: 0.0301\n","Epoch 2/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.6859 - accuracy: 0.3810 - val_loss: 3.5226 - val_accuracy: 0.3690\n","Epoch 3/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.4205 - accuracy: 0.4102 - val_loss: 3.2661 - val_accuracy: 0.4357\n","Epoch 4/100\n","301/301 [==============================] - 3s 8ms/step - loss: 3.2009 - accuracy: 0.4407 - val_loss: 3.0143 - val_accuracy: 0.4581\n","Epoch 5/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.0635 - accuracy: 0.4562 - val_loss: 2.8303 - val_accuracy: 0.5124\n","Epoch 6/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.9203 - accuracy: 0.4682 - val_loss: 2.8386 - val_accuracy: 0.4967\n","Epoch 7/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.8021 - accuracy: 0.4780 - val_loss: 2.7403 - val_accuracy: 0.5176\n","Epoch 8/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.7553 - accuracy: 0.4800 - val_loss: 2.6357 - val_accuracy: 0.4960\n","Epoch 9/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6710 - accuracy: 0.4897 - val_loss: 2.8057 - val_accuracy: 0.4856\n","Epoch 10/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6508 - accuracy: 0.4938 - val_loss: 2.5822 - val_accuracy: 0.5113\n","Epoch 11/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5692 - accuracy: 0.5009 - val_loss: 2.5637 - val_accuracy: 0.5354\n","Epoch 12/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5318 - accuracy: 0.5057 - val_loss: 2.4852 - val_accuracy: 0.5158\n","Epoch 13/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4714 - accuracy: 0.5135 - val_loss: 2.4996 - val_accuracy: 0.4997\n","Epoch 14/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4415 - accuracy: 0.5105 - val_loss: 2.4319 - val_accuracy: 0.5239\n","Epoch 15/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4101 - accuracy: 0.5166 - val_loss: 2.3202 - val_accuracy: 0.5373\n","Epoch 16/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3731 - accuracy: 0.5175 - val_loss: 2.2813 - val_accuracy: 0.5291\n","Epoch 17/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3178 - accuracy: 0.5277 - val_loss: 2.3363 - val_accuracy: 0.5281\n","Epoch 18/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3133 - accuracy: 0.5236 - val_loss: 2.3241 - val_accuracy: 0.5417\n","Epoch 19/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2936 - accuracy: 0.5256 - val_loss: 2.2041 - val_accuracy: 0.5509\n","Epoch 20/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2508 - accuracy: 0.5285 - val_loss: 2.1846 - val_accuracy: 0.5465\n","Epoch 21/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2644 - accuracy: 0.5229 - val_loss: 2.1652 - val_accuracy: 0.5597\n","Epoch 22/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2506 - accuracy: 0.5273 - val_loss: 2.2409 - val_accuracy: 0.5382\n","Epoch 23/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2052 - accuracy: 0.5336 - val_loss: 2.2180 - val_accuracy: 0.5247\n","Epoch 24/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1810 - accuracy: 0.5252 - val_loss: 2.1763 - val_accuracy: 0.5358\n","Epoch 25/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1608 - accuracy: 0.5290 - val_loss: 2.2075 - val_accuracy: 0.5327\n","Epoch 26/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1573 - accuracy: 0.5321 - val_loss: 2.1279 - val_accuracy: 0.5587\n","Epoch 27/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1696 - accuracy: 0.5320 - val_loss: 2.1788 - val_accuracy: 0.5290\n","Epoch 28/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1735 - accuracy: 0.5347 - val_loss: 2.1127 - val_accuracy: 0.5448\n","Epoch 29/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.1262 - accuracy: 0.5389 - val_loss: 2.0698 - val_accuracy: 0.5558\n","Epoch 30/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1079 - accuracy: 0.5383 - val_loss: 2.1012 - val_accuracy: 0.5422\n","Epoch 31/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1020 - accuracy: 0.5381 - val_loss: 2.1422 - val_accuracy: 0.5390\n","Epoch 32/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.1067 - accuracy: 0.5350 - val_loss: 2.1127 - val_accuracy: 0.5403\n","Epoch 33/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0791 - accuracy: 0.5369 - val_loss: 2.1040 - val_accuracy: 0.5309\n","Epoch 34/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.0700 - accuracy: 0.5372 - val_loss: 2.0921 - val_accuracy: 0.5344\n","Epoch 35/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0823 - accuracy: 0.5370 - val_loss: 2.0173 - val_accuracy: 0.5724\n","Epoch 36/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0761 - accuracy: 0.5353 - val_loss: 2.0289 - val_accuracy: 0.5588\n","Epoch 37/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0773 - accuracy: 0.5392 - val_loss: 2.0136 - val_accuracy: 0.5598\n","Epoch 38/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0564 - accuracy: 0.5376 - val_loss: 2.0560 - val_accuracy: 0.5541\n","Epoch 39/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0391 - accuracy: 0.5409 - val_loss: 2.0154 - val_accuracy: 0.5556\n","Epoch 40/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0325 - accuracy: 0.5380 - val_loss: 1.9994 - val_accuracy: 0.5699\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0295 - accuracy: 0.5385 - val_loss: 2.0630 - val_accuracy: 0.5389\n","Epoch 42/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0354 - accuracy: 0.5394 - val_loss: 2.0231 - val_accuracy: 0.5404\n","Epoch 43/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0170 - accuracy: 0.5445 - val_loss: 2.0724 - val_accuracy: 0.5347\n","Epoch 44/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0410 - accuracy: 0.5386 - val_loss: 2.0301 - val_accuracy: 0.5589\n","Epoch 45/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0165 - accuracy: 0.5400 - val_loss: 2.0083 - val_accuracy: 0.5566\n","Epoch 46/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0101 - accuracy: 0.5389 - val_loss: 1.9596 - val_accuracy: 0.5693\n","Epoch 47/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9982 - accuracy: 0.5390 - val_loss: 1.9910 - val_accuracy: 0.5444\n","Epoch 48/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.0078 - accuracy: 0.5432 - val_loss: 2.0167 - val_accuracy: 0.5466\n","Epoch 49/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9968 - accuracy: 0.5466 - val_loss: 2.0068 - val_accuracy: 0.5514\n","Epoch 50/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9938 - accuracy: 0.5450 - val_loss: 2.0075 - val_accuracy: 0.5506\n","Epoch 51/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.0056 - accuracy: 0.5418 - val_loss: 1.9659 - val_accuracy: 0.5612\n","Epoch 52/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9957 - accuracy: 0.5424 - val_loss: 1.9700 - val_accuracy: 0.5602\n","Epoch 53/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9909 - accuracy: 0.5396 - val_loss: 2.0029 - val_accuracy: 0.5374\n","Epoch 54/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9730 - accuracy: 0.5404 - val_loss: 1.9437 - val_accuracy: 0.5582\n","Epoch 55/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9852 - accuracy: 0.5395 - val_loss: 1.9577 - val_accuracy: 0.5747\n","Epoch 56/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9698 - accuracy: 0.5392 - val_loss: 1.9792 - val_accuracy: 0.5506\n","Epoch 57/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9630 - accuracy: 0.5397 - val_loss: 1.9118 - val_accuracy: 0.5788\n","Epoch 58/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9561 - accuracy: 0.5451 - val_loss: 1.9452 - val_accuracy: 0.5632\n","Epoch 59/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9754 - accuracy: 0.5454 - val_loss: 1.9176 - val_accuracy: 0.5721\n","Epoch 60/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9667 - accuracy: 0.5467 - val_loss: 2.0214 - val_accuracy: 0.5403\n","Epoch 61/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9822 - accuracy: 0.5439 - val_loss: 1.9816 - val_accuracy: 0.5630\n","Epoch 62/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9666 - accuracy: 0.5484 - val_loss: 2.0277 - val_accuracy: 0.5437\n","Epoch 63/100\n","301/301 [==============================] - 3s 9ms/step - loss: 1.9765 - accuracy: 0.5436 - val_loss: 1.9529 - val_accuracy: 0.5595\n","Epoch 64/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9609 - accuracy: 0.5463 - val_loss: 1.9902 - val_accuracy: 0.5447\n","Epoch 65/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9490 - accuracy: 0.5440 - val_loss: 2.0240 - val_accuracy: 0.5392\n","Epoch 66/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9567 - accuracy: 0.5406 - val_loss: 1.9872 - val_accuracy: 0.5506\n","Epoch 67/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9555 - accuracy: 0.5445 - val_loss: 1.9497 - val_accuracy: 0.5665\n","Epoch 68/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9487 - accuracy: 0.5464 - val_loss: 1.9515 - val_accuracy: 0.5585\n","Epoch 69/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9166 - accuracy: 0.5496 - val_loss: 1.9763 - val_accuracy: 0.5455\n","Epoch 70/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9438 - accuracy: 0.5416 - val_loss: 1.9448 - val_accuracy: 0.5566\n","Epoch 71/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9436 - accuracy: 0.5524 - val_loss: 1.9437 - val_accuracy: 0.5573\n","Epoch 72/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9294 - accuracy: 0.5451 - val_loss: 1.9902 - val_accuracy: 0.5508\n","Epoch 73/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9332 - accuracy: 0.5459 - val_loss: 1.9252 - val_accuracy: 0.5670\n","Epoch 74/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9225 - accuracy: 0.5446 - val_loss: 1.9648 - val_accuracy: 0.5563\n","Epoch 75/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9394 - accuracy: 0.5468 - val_loss: 1.9356 - val_accuracy: 0.5579\n","Epoch 76/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9202 - accuracy: 0.5469 - val_loss: 2.0179 - val_accuracy: 0.5390\n","Epoch 77/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9394 - accuracy: 0.5436 - val_loss: 1.9404 - val_accuracy: 0.5453\n","Epoch 78/100\n","301/301 [==============================] - 3s 9ms/step - loss: 1.9235 - accuracy: 0.5441 - val_loss: 1.9208 - val_accuracy: 0.5710\n","Epoch 79/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9457 - accuracy: 0.5442 - val_loss: 1.9671 - val_accuracy: 0.5550\n","Epoch 80/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9319 - accuracy: 0.5486 - val_loss: 1.9992 - val_accuracy: 0.5433\n","Epoch 81/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9259 - accuracy: 0.5421 - val_loss: 1.9857 - val_accuracy: 0.5453\n","Epoch 82/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9178 - accuracy: 0.5514 - val_loss: 1.9205 - val_accuracy: 0.5618\n","Epoch 83/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9247 - accuracy: 0.5496 - val_loss: 1.9651 - val_accuracy: 0.5432\n","Epoch 84/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9278 - accuracy: 0.5493 - val_loss: 1.9588 - val_accuracy: 0.5510\n","Epoch 85/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9158 - accuracy: 0.5511 - val_loss: 1.9225 - val_accuracy: 0.5680\n","Epoch 86/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9100 - accuracy: 0.5507 - val_loss: 1.8959 - val_accuracy: 0.5676\n","Epoch 87/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9237 - accuracy: 0.5495 - val_loss: 1.9420 - val_accuracy: 0.5588\n","Epoch 88/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9131 - accuracy: 0.5483 - val_loss: 1.9229 - val_accuracy: 0.5621\n","Epoch 89/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9088 - accuracy: 0.5474 - val_loss: 1.9564 - val_accuracy: 0.5462\n","Epoch 90/100\n","301/301 [==============================] - 3s 9ms/step - loss: 1.9034 - accuracy: 0.5531 - val_loss: 1.9362 - val_accuracy: 0.5564\n","Epoch 91/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9039 - accuracy: 0.5480 - val_loss: 1.9297 - val_accuracy: 0.5603\n","Epoch 92/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9166 - accuracy: 0.5507 - val_loss: 1.9261 - val_accuracy: 0.5558\n","Epoch 93/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9152 - accuracy: 0.5515 - val_loss: 1.9324 - val_accuracy: 0.5595\n","Epoch 94/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.9117 - accuracy: 0.5501 - val_loss: 1.9059 - val_accuracy: 0.5762\n","Epoch 95/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9162 - accuracy: 0.5501 - val_loss: 1.9851 - val_accuracy: 0.5517\n","Epoch 96/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9029 - accuracy: 0.5476 - val_loss: 1.9120 - val_accuracy: 0.5554\n","Epoch 97/100\n","301/301 [==============================] - 3s 8ms/step - loss: 1.8943 - accuracy: 0.5506 - val_loss: 1.9457 - val_accuracy: 0.5552\n","Epoch 98/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9159 - accuracy: 0.5412 - val_loss: 1.9464 - val_accuracy: 0.5590\n","Epoch 99/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.8980 - accuracy: 0.5523 - val_loss: 1.9155 - val_accuracy: 0.5531\n","Epoch 100/100\n","301/301 [==============================] - 2s 8ms/step - loss: 1.9018 - accuracy: 0.5520 - val_loss: 1.9556 - val_accuracy: 0.5504\n"]}]},{"cell_type":"code","source":["# maybe dropout makes a difference\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(8,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ExTieBe2gGz","executionInfo":{"status":"ok","timestamp":1652052793304,"user_tz":240,"elapsed":262897,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"ddf08964-5436-453c-9fc0-1190d8c52cd1"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 4s 9ms/step - loss: 15.2182 - accuracy: 0.3192 - val_loss: 4.8246 - val_accuracy: 0.0284\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 4.0568 - accuracy: 0.3595 - val_loss: 3.7470 - val_accuracy: 0.4569\n","Epoch 3/100\n","301/301 [==============================] - 3s 8ms/step - loss: 3.8686 - accuracy: 0.3760 - val_loss: 3.6492 - val_accuracy: 0.4313\n","Epoch 4/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.6890 - accuracy: 0.3865 - val_loss: 3.4987 - val_accuracy: 0.4091\n","Epoch 5/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.5473 - accuracy: 0.3989 - val_loss: 3.3163 - val_accuracy: 0.4542\n","Epoch 6/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.3914 - accuracy: 0.4157 - val_loss: 3.2098 - val_accuracy: 0.4839\n","Epoch 7/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.2961 - accuracy: 0.4194 - val_loss: 3.1400 - val_accuracy: 0.4716\n","Epoch 8/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.2090 - accuracy: 0.4346 - val_loss: 2.9804 - val_accuracy: 0.4826\n","Epoch 9/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.1223 - accuracy: 0.4409 - val_loss: 2.9733 - val_accuracy: 0.4905\n","Epoch 10/100\n","301/301 [==============================] - 3s 8ms/step - loss: 3.0867 - accuracy: 0.4397 - val_loss: 2.9311 - val_accuracy: 0.4736\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.0160 - accuracy: 0.4474 - val_loss: 2.8324 - val_accuracy: 0.5133\n","Epoch 12/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.9345 - accuracy: 0.4580 - val_loss: 2.7870 - val_accuracy: 0.5027\n","Epoch 13/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.9227 - accuracy: 0.4587 - val_loss: 2.7646 - val_accuracy: 0.5122\n","Epoch 14/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.9257 - accuracy: 0.4544 - val_loss: 2.7599 - val_accuracy: 0.5282\n","Epoch 15/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.8382 - accuracy: 0.4647 - val_loss: 2.7083 - val_accuracy: 0.5358\n","Epoch 16/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.8160 - accuracy: 0.4644 - val_loss: 2.7734 - val_accuracy: 0.5089\n","Epoch 17/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.7913 - accuracy: 0.4642 - val_loss: 2.7503 - val_accuracy: 0.4892\n","Epoch 18/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.7736 - accuracy: 0.4665 - val_loss: 2.5442 - val_accuracy: 0.5339\n","Epoch 19/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.7044 - accuracy: 0.4704 - val_loss: 2.5953 - val_accuracy: 0.5020\n","Epoch 20/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.7346 - accuracy: 0.4718 - val_loss: 2.5464 - val_accuracy: 0.5246\n","Epoch 21/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6861 - accuracy: 0.4720 - val_loss: 2.6343 - val_accuracy: 0.4885\n","Epoch 22/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6848 - accuracy: 0.4756 - val_loss: 2.5517 - val_accuracy: 0.5297\n","Epoch 23/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6056 - accuracy: 0.4767 - val_loss: 2.4761 - val_accuracy: 0.5231\n","Epoch 24/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.5618 - accuracy: 0.4815 - val_loss: 2.4109 - val_accuracy: 0.5494\n","Epoch 25/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5829 - accuracy: 0.4717 - val_loss: 2.4190 - val_accuracy: 0.5335\n","Epoch 26/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5493 - accuracy: 0.4742 - val_loss: 2.4322 - val_accuracy: 0.5390\n","Epoch 27/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5661 - accuracy: 0.4822 - val_loss: 2.4533 - val_accuracy: 0.5232\n","Epoch 28/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5363 - accuracy: 0.4808 - val_loss: 2.4129 - val_accuracy: 0.5372\n","Epoch 29/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4981 - accuracy: 0.4788 - val_loss: 2.3202 - val_accuracy: 0.5330\n","Epoch 30/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.4523 - accuracy: 0.4899 - val_loss: 2.3694 - val_accuracy: 0.5241\n","Epoch 31/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4667 - accuracy: 0.4800 - val_loss: 2.4148 - val_accuracy: 0.5197\n","Epoch 32/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4397 - accuracy: 0.4867 - val_loss: 2.3371 - val_accuracy: 0.5258\n","Epoch 33/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.4553 - accuracy: 0.4798 - val_loss: 2.3830 - val_accuracy: 0.5180\n","Epoch 34/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4694 - accuracy: 0.4834 - val_loss: 2.3614 - val_accuracy: 0.5337\n","Epoch 35/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.4339 - accuracy: 0.4864 - val_loss: 2.3849 - val_accuracy: 0.5204\n","Epoch 36/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4322 - accuracy: 0.4869 - val_loss: 2.3182 - val_accuracy: 0.5320\n","Epoch 37/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4270 - accuracy: 0.4849 - val_loss: 2.3299 - val_accuracy: 0.5347\n","Epoch 38/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4129 - accuracy: 0.4788 - val_loss: 2.3414 - val_accuracy: 0.5192\n","Epoch 39/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.4132 - accuracy: 0.4847 - val_loss: 2.2765 - val_accuracy: 0.5458\n","Epoch 40/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3961 - accuracy: 0.4841 - val_loss: 2.2800 - val_accuracy: 0.5394\n","Epoch 41/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4076 - accuracy: 0.4843 - val_loss: 2.2543 - val_accuracy: 0.5489\n","Epoch 42/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3969 - accuracy: 0.4823 - val_loss: 2.2495 - val_accuracy: 0.5527\n","Epoch 43/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3852 - accuracy: 0.4851 - val_loss: 2.3050 - val_accuracy: 0.5185\n","Epoch 44/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3501 - accuracy: 0.4969 - val_loss: 2.1912 - val_accuracy: 0.5525\n","Epoch 45/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3533 - accuracy: 0.4929 - val_loss: 2.2431 - val_accuracy: 0.5478\n","Epoch 46/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3572 - accuracy: 0.4903 - val_loss: 2.2476 - val_accuracy: 0.5448\n","Epoch 47/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3802 - accuracy: 0.4834 - val_loss: 2.3020 - val_accuracy: 0.5246\n","Epoch 48/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3623 - accuracy: 0.4844 - val_loss: 2.2698 - val_accuracy: 0.5199\n","Epoch 49/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3343 - accuracy: 0.4870 - val_loss: 2.2339 - val_accuracy: 0.5227\n","Epoch 50/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3179 - accuracy: 0.4873 - val_loss: 2.1702 - val_accuracy: 0.5506\n","Epoch 51/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3293 - accuracy: 0.4858 - val_loss: 2.2205 - val_accuracy: 0.5325\n","Epoch 52/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3276 - accuracy: 0.4871 - val_loss: 2.1789 - val_accuracy: 0.5399\n","Epoch 53/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3316 - accuracy: 0.4871 - val_loss: 2.1388 - val_accuracy: 0.5618\n","Epoch 54/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3161 - accuracy: 0.4874 - val_loss: 2.2244 - val_accuracy: 0.5374\n","Epoch 55/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3101 - accuracy: 0.4869 - val_loss: 2.2145 - val_accuracy: 0.5172\n","Epoch 56/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3085 - accuracy: 0.4889 - val_loss: 2.1899 - val_accuracy: 0.5365\n","Epoch 57/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3073 - accuracy: 0.4891 - val_loss: 2.1495 - val_accuracy: 0.5544\n","Epoch 58/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2905 - accuracy: 0.4936 - val_loss: 2.1938 - val_accuracy: 0.5301\n","Epoch 59/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2964 - accuracy: 0.4851 - val_loss: 2.1488 - val_accuracy: 0.5509\n","Epoch 60/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2928 - accuracy: 0.4904 - val_loss: 2.2370 - val_accuracy: 0.5373\n","Epoch 61/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3210 - accuracy: 0.4892 - val_loss: 2.2008 - val_accuracy: 0.5373\n","Epoch 62/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2844 - accuracy: 0.4902 - val_loss: 2.1681 - val_accuracy: 0.5604\n","Epoch 63/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3059 - accuracy: 0.4920 - val_loss: 2.1318 - val_accuracy: 0.5626\n","Epoch 64/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3044 - accuracy: 0.4918 - val_loss: 2.1445 - val_accuracy: 0.5544\n","Epoch 65/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2861 - accuracy: 0.4911 - val_loss: 2.1817 - val_accuracy: 0.5502\n","Epoch 66/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2960 - accuracy: 0.4875 - val_loss: 2.1629 - val_accuracy: 0.5386\n","Epoch 67/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2838 - accuracy: 0.4869 - val_loss: 2.1768 - val_accuracy: 0.5355\n","Epoch 68/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2684 - accuracy: 0.4919 - val_loss: 2.1463 - val_accuracy: 0.5379\n","Epoch 69/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2488 - accuracy: 0.4944 - val_loss: 2.1088 - val_accuracy: 0.5564\n","Epoch 70/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2864 - accuracy: 0.4895 - val_loss: 2.2450 - val_accuracy: 0.5279\n","Epoch 71/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2957 - accuracy: 0.4851 - val_loss: 2.1841 - val_accuracy: 0.5409\n","Epoch 72/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2589 - accuracy: 0.4913 - val_loss: 2.1325 - val_accuracy: 0.5524\n","Epoch 73/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2487 - accuracy: 0.4971 - val_loss: 2.1382 - val_accuracy: 0.5455\n","Epoch 74/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2528 - accuracy: 0.4931 - val_loss: 2.1747 - val_accuracy: 0.5287\n","Epoch 75/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2427 - accuracy: 0.4951 - val_loss: 2.1059 - val_accuracy: 0.5509\n","Epoch 76/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2276 - accuracy: 0.4924 - val_loss: 2.1321 - val_accuracy: 0.5341\n","Epoch 77/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2269 - accuracy: 0.4954 - val_loss: 2.1208 - val_accuracy: 0.5444\n","Epoch 78/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2409 - accuracy: 0.4949 - val_loss: 2.1006 - val_accuracy: 0.5527\n","Epoch 79/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2439 - accuracy: 0.4950 - val_loss: 2.1537 - val_accuracy: 0.5452\n","Epoch 80/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2442 - accuracy: 0.4888 - val_loss: 2.1455 - val_accuracy: 0.5319\n","Epoch 81/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2450 - accuracy: 0.4905 - val_loss: 2.1207 - val_accuracy: 0.5508\n","Epoch 82/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2406 - accuracy: 0.4872 - val_loss: 2.0854 - val_accuracy: 0.5455\n","Epoch 83/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2259 - accuracy: 0.4939 - val_loss: 2.0873 - val_accuracy: 0.5502\n","Epoch 84/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2555 - accuracy: 0.4951 - val_loss: 2.1204 - val_accuracy: 0.5461\n","Epoch 85/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2362 - accuracy: 0.4946 - val_loss: 2.1194 - val_accuracy: 0.5399\n","Epoch 86/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.2487 - accuracy: 0.4909 - val_loss: 2.1003 - val_accuracy: 0.5419\n","Epoch 87/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2311 - accuracy: 0.4938 - val_loss: 2.1337 - val_accuracy: 0.5453\n","Epoch 88/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2535 - accuracy: 0.4971 - val_loss: 2.1758 - val_accuracy: 0.5182\n","Epoch 89/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2267 - accuracy: 0.4948 - val_loss: 2.1458 - val_accuracy: 0.5319\n","Epoch 90/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2539 - accuracy: 0.4910 - val_loss: 2.1334 - val_accuracy: 0.5444\n","Epoch 91/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2441 - accuracy: 0.4943 - val_loss: 2.1314 - val_accuracy: 0.5439\n","Epoch 92/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2255 - accuracy: 0.4950 - val_loss: 2.0929 - val_accuracy: 0.5426\n","Epoch 93/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2026 - accuracy: 0.4951 - val_loss: 2.0826 - val_accuracy: 0.5456\n","Epoch 94/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2129 - accuracy: 0.4980 - val_loss: 2.0828 - val_accuracy: 0.5602\n","Epoch 95/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2204 - accuracy: 0.5002 - val_loss: 2.0898 - val_accuracy: 0.5477\n","Epoch 96/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2054 - accuracy: 0.4958 - val_loss: 2.1389 - val_accuracy: 0.5369\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2116 - accuracy: 0.4958 - val_loss: 2.0752 - val_accuracy: 0.5427\n","Epoch 98/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2165 - accuracy: 0.4925 - val_loss: 2.0841 - val_accuracy: 0.5555\n","Epoch 99/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2133 - accuracy: 0.4935 - val_loss: 2.0983 - val_accuracy: 0.5391\n","Epoch 100/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2007 - accuracy: 0.4975 - val_loss: 2.0968 - val_accuracy: 0.5394\n"]}]},{"cell_type":"code","source":["# maybe dropout makes a difference\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1',bias_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          #layers.Dense(8,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          #layers.BatchNormalization(),\n","          #layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uaUZ2JTf3-RO","executionInfo":{"status":"ok","timestamp":1652053055937,"user_tz":240,"elapsed":262679,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"94c4b49a-a026-48a2-95e3-807560bf5d88"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 3s 8ms/step - loss: 18.0574 - accuracy: 0.3714 - val_loss: 6.2345 - val_accuracy: 0.0717\n","Epoch 2/100\n","301/301 [==============================] - 2s 7ms/step - loss: 4.7320 - accuracy: 0.4021 - val_loss: 4.1372 - val_accuracy: 0.4564\n","Epoch 3/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.9818 - accuracy: 0.4209 - val_loss: 3.8109 - val_accuracy: 0.4463\n","Epoch 4/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.7804 - accuracy: 0.4331 - val_loss: 3.6149 - val_accuracy: 0.4914\n","Epoch 5/100\n","301/301 [==============================] - 2s 7ms/step - loss: 3.6688 - accuracy: 0.4379 - val_loss: 3.4586 - val_accuracy: 0.5213\n","Epoch 6/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.5839 - accuracy: 0.4404 - val_loss: 3.4603 - val_accuracy: 0.4900\n","Epoch 7/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.5181 - accuracy: 0.4525 - val_loss: 3.4175 - val_accuracy: 0.5256\n","Epoch 8/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.4358 - accuracy: 0.4560 - val_loss: 3.4751 - val_accuracy: 0.4358\n","Epoch 9/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.3574 - accuracy: 0.4604 - val_loss: 3.1364 - val_accuracy: 0.5388\n","Epoch 10/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.2859 - accuracy: 0.4611 - val_loss: 3.2349 - val_accuracy: 0.5351\n","Epoch 11/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.2383 - accuracy: 0.4664 - val_loss: 3.1288 - val_accuracy: 0.5195\n","Epoch 12/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.1796 - accuracy: 0.4699 - val_loss: 2.9575 - val_accuracy: 0.4959\n","Epoch 13/100\n","301/301 [==============================] - 2s 7ms/step - loss: 3.0895 - accuracy: 0.4722 - val_loss: 3.1156 - val_accuracy: 0.4875\n","Epoch 14/100\n","301/301 [==============================] - 2s 7ms/step - loss: 3.0567 - accuracy: 0.4727 - val_loss: 2.9014 - val_accuracy: 0.5205\n","Epoch 15/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.9749 - accuracy: 0.4772 - val_loss: 2.8174 - val_accuracy: 0.5553\n","Epoch 16/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.8808 - accuracy: 0.4795 - val_loss: 2.7617 - val_accuracy: 0.5167\n","Epoch 17/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.8564 - accuracy: 0.4813 - val_loss: 2.7340 - val_accuracy: 0.5319\n","Epoch 18/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.7944 - accuracy: 0.4833 - val_loss: 2.7027 - val_accuracy: 0.5240\n","Epoch 19/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.7829 - accuracy: 0.4865 - val_loss: 2.6982 - val_accuracy: 0.5340\n","Epoch 20/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.7398 - accuracy: 0.4891 - val_loss: 2.5884 - val_accuracy: 0.5306\n","Epoch 21/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6957 - accuracy: 0.4876 - val_loss: 2.7216 - val_accuracy: 0.5015\n","Epoch 22/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6869 - accuracy: 0.4822 - val_loss: 2.5569 - val_accuracy: 0.5444\n","Epoch 23/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.6284 - accuracy: 0.4913 - val_loss: 2.5181 - val_accuracy: 0.5585\n","Epoch 24/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6088 - accuracy: 0.4863 - val_loss: 2.4952 - val_accuracy: 0.5550\n","Epoch 25/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6168 - accuracy: 0.4854 - val_loss: 2.5340 - val_accuracy: 0.5271\n","Epoch 26/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.5645 - accuracy: 0.4943 - val_loss: 2.4410 - val_accuracy: 0.5509\n","Epoch 27/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5547 - accuracy: 0.4874 - val_loss: 2.3619 - val_accuracy: 0.5679\n","Epoch 28/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.5502 - accuracy: 0.4946 - val_loss: 2.4438 - val_accuracy: 0.5462\n","Epoch 29/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5393 - accuracy: 0.4910 - val_loss: 2.4573 - val_accuracy: 0.5379\n","Epoch 30/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.5219 - accuracy: 0.4966 - val_loss: 2.4463 - val_accuracy: 0.5296\n","Epoch 31/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.5262 - accuracy: 0.4893 - val_loss: 2.4127 - val_accuracy: 0.5347\n","Epoch 32/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4934 - accuracy: 0.4886 - val_loss: 2.4548 - val_accuracy: 0.5245\n","Epoch 33/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4790 - accuracy: 0.4909 - val_loss: 2.3611 - val_accuracy: 0.5507\n","Epoch 34/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4587 - accuracy: 0.4933 - val_loss: 2.3612 - val_accuracy: 0.5461\n","Epoch 35/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4653 - accuracy: 0.4924 - val_loss: 2.4385 - val_accuracy: 0.5216\n","Epoch 36/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4564 - accuracy: 0.4907 - val_loss: 2.3286 - val_accuracy: 0.5525\n","Epoch 37/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.4344 - accuracy: 0.4951 - val_loss: 2.3265 - val_accuracy: 0.5390\n","Epoch 38/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4220 - accuracy: 0.4954 - val_loss: 2.2981 - val_accuracy: 0.5660\n","Epoch 39/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4081 - accuracy: 0.4959 - val_loss: 2.3853 - val_accuracy: 0.5181\n","Epoch 40/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4011 - accuracy: 0.4922 - val_loss: 2.2308 - val_accuracy: 0.5625\n","Epoch 41/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.3813 - accuracy: 0.4930 - val_loss: 2.2423 - val_accuracy: 0.5530\n","Epoch 42/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3631 - accuracy: 0.4930 - val_loss: 2.2446 - val_accuracy: 0.5545\n","Epoch 43/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.4037 - accuracy: 0.4931 - val_loss: 2.2781 - val_accuracy: 0.5485\n","Epoch 44/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3582 - accuracy: 0.4948 - val_loss: 2.2741 - val_accuracy: 0.5396\n","Epoch 45/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.3838 - accuracy: 0.4995 - val_loss: 2.3196 - val_accuracy: 0.5141\n","Epoch 46/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3395 - accuracy: 0.5025 - val_loss: 2.2987 - val_accuracy: 0.5287\n","Epoch 47/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3617 - accuracy: 0.4901 - val_loss: 2.3025 - val_accuracy: 0.5571\n","Epoch 48/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3685 - accuracy: 0.4955 - val_loss: 2.2691 - val_accuracy: 0.5379\n","Epoch 49/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.3544 - accuracy: 0.4962 - val_loss: 2.2293 - val_accuracy: 0.5545\n","Epoch 50/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3431 - accuracy: 0.4977 - val_loss: 2.2089 - val_accuracy: 0.5387\n","Epoch 51/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.3182 - accuracy: 0.4920 - val_loss: 2.1944 - val_accuracy: 0.5510\n","Epoch 52/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3120 - accuracy: 0.4909 - val_loss: 2.2136 - val_accuracy: 0.5498\n","Epoch 53/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3141 - accuracy: 0.5004 - val_loss: 2.1593 - val_accuracy: 0.5529\n","Epoch 54/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3051 - accuracy: 0.4963 - val_loss: 2.2550 - val_accuracy: 0.5196\n","Epoch 55/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3071 - accuracy: 0.4941 - val_loss: 2.2641 - val_accuracy: 0.5231\n","Epoch 56/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3262 - accuracy: 0.4907 - val_loss: 2.2346 - val_accuracy: 0.5439\n","Epoch 57/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.2936 - accuracy: 0.4969 - val_loss: 2.1828 - val_accuracy: 0.5533\n","Epoch 58/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.2705 - accuracy: 0.4954 - val_loss: 2.1933 - val_accuracy: 0.5582\n","Epoch 59/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2751 - accuracy: 0.5001 - val_loss: 2.2014 - val_accuracy: 0.5363\n","Epoch 60/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2864 - accuracy: 0.4941 - val_loss: 2.1453 - val_accuracy: 0.5548\n","Epoch 61/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2970 - accuracy: 0.4972 - val_loss: 2.2660 - val_accuracy: 0.5343\n","Epoch 62/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3162 - accuracy: 0.4955 - val_loss: 2.1975 - val_accuracy: 0.5434\n","Epoch 63/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3000 - accuracy: 0.5002 - val_loss: 2.2681 - val_accuracy: 0.5451\n","Epoch 64/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2825 - accuracy: 0.4994 - val_loss: 2.2345 - val_accuracy: 0.5377\n","Epoch 65/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2849 - accuracy: 0.4964 - val_loss: 2.1130 - val_accuracy: 0.5565\n","Epoch 66/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2533 - accuracy: 0.4976 - val_loss: 2.1314 - val_accuracy: 0.5561\n","Epoch 67/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2682 - accuracy: 0.4931 - val_loss: 2.2249 - val_accuracy: 0.5343\n","Epoch 68/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2535 - accuracy: 0.5021 - val_loss: 2.1549 - val_accuracy: 0.5417\n","Epoch 69/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2598 - accuracy: 0.4915 - val_loss: 2.2259 - val_accuracy: 0.5227\n","Epoch 70/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2723 - accuracy: 0.4981 - val_loss: 2.2064 - val_accuracy: 0.5468\n","Epoch 71/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3097 - accuracy: 0.4986 - val_loss: 2.2163 - val_accuracy: 0.5424\n","Epoch 72/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2733 - accuracy: 0.5021 - val_loss: 2.1983 - val_accuracy: 0.5186\n","Epoch 73/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2900 - accuracy: 0.4990 - val_loss: 2.1870 - val_accuracy: 0.5442\n","Epoch 74/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2764 - accuracy: 0.4997 - val_loss: 2.2021 - val_accuracy: 0.5364\n","Epoch 75/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2862 - accuracy: 0.4981 - val_loss: 2.1141 - val_accuracy: 0.5650\n","Epoch 76/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2696 - accuracy: 0.5002 - val_loss: 2.1593 - val_accuracy: 0.5447\n","Epoch 77/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2305 - accuracy: 0.4985 - val_loss: 2.1395 - val_accuracy: 0.5489\n","Epoch 78/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2414 - accuracy: 0.4985 - val_loss: 2.1542 - val_accuracy: 0.5613\n","Epoch 79/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2749 - accuracy: 0.4943 - val_loss: 2.1753 - val_accuracy: 0.5440\n","Epoch 80/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2509 - accuracy: 0.4959 - val_loss: 2.1727 - val_accuracy: 0.5612\n","Epoch 81/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2585 - accuracy: 0.4969 - val_loss: 2.1541 - val_accuracy: 0.5566\n","Epoch 82/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.2600 - accuracy: 0.4954 - val_loss: 2.1843 - val_accuracy: 0.5375\n","Epoch 83/100\n","301/301 [==============================] - 2s 7ms/step - loss: 2.2502 - accuracy: 0.5007 - val_loss: 2.1530 - val_accuracy: 0.5476\n","Epoch 84/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2571 - accuracy: 0.4948 - val_loss: 2.1602 - val_accuracy: 0.5400\n","Epoch 85/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2468 - accuracy: 0.5020 - val_loss: 2.1526 - val_accuracy: 0.5456\n","Epoch 86/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2574 - accuracy: 0.4978 - val_loss: 2.1879 - val_accuracy: 0.5350\n","Epoch 87/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2607 - accuracy: 0.4971 - val_loss: 2.2002 - val_accuracy: 0.5411\n","Epoch 88/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2756 - accuracy: 0.4957 - val_loss: 2.1904 - val_accuracy: 0.5251\n","Epoch 89/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2664 - accuracy: 0.4971 - val_loss: 2.1249 - val_accuracy: 0.5751\n","Epoch 90/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2412 - accuracy: 0.4937 - val_loss: 2.1346 - val_accuracy: 0.5526\n","Epoch 91/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2480 - accuracy: 0.5010 - val_loss: 2.1459 - val_accuracy: 0.5395\n","Epoch 92/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2570 - accuracy: 0.4991 - val_loss: 2.1526 - val_accuracy: 0.5447\n","Epoch 93/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2555 - accuracy: 0.5001 - val_loss: 2.1284 - val_accuracy: 0.5514\n","Epoch 94/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2609 - accuracy: 0.4983 - val_loss: 2.1358 - val_accuracy: 0.5485\n","Epoch 95/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2176 - accuracy: 0.5040 - val_loss: 2.1203 - val_accuracy: 0.5486\n","Epoch 96/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2180 - accuracy: 0.4958 - val_loss: 2.1520 - val_accuracy: 0.5479\n","Epoch 97/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2263 - accuracy: 0.4973 - val_loss: 2.1149 - val_accuracy: 0.5558\n","Epoch 98/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2313 - accuracy: 0.4978 - val_loss: 2.1268 - val_accuracy: 0.5399\n","Epoch 99/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2271 - accuracy: 0.5055 - val_loss: 2.1828 - val_accuracy: 0.5296\n","Epoch 100/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2384 - accuracy: 0.5019 - val_loss: 2.1461 - val_accuracy: 0.5285\n"]}]},{"cell_type":"code","source":["# maybe dropout makes a difference\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1',bias_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(4,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVkeQuhq6Ish","executionInfo":{"status":"ok","timestamp":1652053379104,"user_tz":240,"elapsed":323220,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"1ee3fd95-bdc6-41ca-c24c-642f96801dee"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 4s 9ms/step - loss: 14.4064 - accuracy: 0.2155 - val_loss: 4.0435 - val_accuracy: 0.1929\n","Epoch 2/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.7195 - accuracy: 0.2753 - val_loss: 3.5633 - val_accuracy: 0.2903\n","Epoch 3/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.5364 - accuracy: 0.2878 - val_loss: 3.3386 - val_accuracy: 0.3239\n","Epoch 4/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.3623 - accuracy: 0.2979 - val_loss: 3.2101 - val_accuracy: 0.3127\n","Epoch 5/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.2366 - accuracy: 0.3106 - val_loss: 3.2990 - val_accuracy: 0.2196\n","Epoch 6/100\n","301/301 [==============================] - 2s 8ms/step - loss: 3.1231 - accuracy: 0.3393 - val_loss: 3.0110 - val_accuracy: 0.2951\n","Epoch 7/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.0601 - accuracy: 0.3497 - val_loss: 3.0039 - val_accuracy: 0.3588\n","Epoch 8/100\n","301/301 [==============================] - 3s 8ms/step - loss: 3.0110 - accuracy: 0.3604 - val_loss: 3.0695 - val_accuracy: 0.3528\n","Epoch 9/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.9602 - accuracy: 0.3810 - val_loss: 2.9026 - val_accuracy: 0.3514\n","Epoch 10/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8997 - accuracy: 0.3835 - val_loss: 2.9047 - val_accuracy: 0.3995\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8602 - accuracy: 0.3969 - val_loss: 2.7826 - val_accuracy: 0.4085\n","Epoch 12/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.8281 - accuracy: 0.4079 - val_loss: 2.7059 - val_accuracy: 0.4374\n","Epoch 13/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.8364 - accuracy: 0.4054 - val_loss: 2.7304 - val_accuracy: 0.4769\n","Epoch 14/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.7861 - accuracy: 0.4136 - val_loss: 2.6853 - val_accuracy: 0.4549\n","Epoch 15/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.7219 - accuracy: 0.4278 - val_loss: 2.6439 - val_accuracy: 0.4546\n","Epoch 16/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7499 - accuracy: 0.4240 - val_loss: 2.5318 - val_accuracy: 0.4944\n","Epoch 17/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6973 - accuracy: 0.4260 - val_loss: 2.6348 - val_accuracy: 0.4641\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7176 - accuracy: 0.4248 - val_loss: 2.5376 - val_accuracy: 0.4654\n","Epoch 19/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.6244 - accuracy: 0.4342 - val_loss: 2.5712 - val_accuracy: 0.4778\n","Epoch 20/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.5907 - accuracy: 0.4279 - val_loss: 2.4886 - val_accuracy: 0.4518\n","Epoch 21/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.6074 - accuracy: 0.4299 - val_loss: 2.4684 - val_accuracy: 0.4996\n","Epoch 22/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5685 - accuracy: 0.4367 - val_loss: 2.4792 - val_accuracy: 0.4670\n","Epoch 23/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5885 - accuracy: 0.4366 - val_loss: 2.4832 - val_accuracy: 0.4960\n","Epoch 24/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5407 - accuracy: 0.4381 - val_loss: 2.4087 - val_accuracy: 0.5083\n","Epoch 25/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5306 - accuracy: 0.4417 - val_loss: 2.4097 - val_accuracy: 0.4825\n","Epoch 26/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4883 - accuracy: 0.4410 - val_loss: 2.3696 - val_accuracy: 0.5088\n","Epoch 27/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5214 - accuracy: 0.4472 - val_loss: 2.4083 - val_accuracy: 0.4716\n","Epoch 28/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4916 - accuracy: 0.4400 - val_loss: 2.3379 - val_accuracy: 0.5157\n","Epoch 29/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4644 - accuracy: 0.4479 - val_loss: 2.4085 - val_accuracy: 0.4688\n","Epoch 30/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4471 - accuracy: 0.4453 - val_loss: 2.3513 - val_accuracy: 0.5201\n","Epoch 31/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4567 - accuracy: 0.4480 - val_loss: 2.3439 - val_accuracy: 0.4803\n","Epoch 32/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4503 - accuracy: 0.4502 - val_loss: 2.3442 - val_accuracy: 0.4950\n","Epoch 33/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4233 - accuracy: 0.4529 - val_loss: 2.3356 - val_accuracy: 0.4969\n","Epoch 34/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4320 - accuracy: 0.4498 - val_loss: 2.3059 - val_accuracy: 0.5074\n","Epoch 35/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3922 - accuracy: 0.4490 - val_loss: 2.2691 - val_accuracy: 0.4912\n","Epoch 36/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4078 - accuracy: 0.4511 - val_loss: 2.3255 - val_accuracy: 0.5035\n","Epoch 37/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4236 - accuracy: 0.4531 - val_loss: 2.3752 - val_accuracy: 0.4771\n","Epoch 38/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3898 - accuracy: 0.4604 - val_loss: 2.2645 - val_accuracy: 0.5081\n","Epoch 39/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3875 - accuracy: 0.4538 - val_loss: 2.3197 - val_accuracy: 0.4959\n","Epoch 40/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.4101 - accuracy: 0.4509 - val_loss: 2.2749 - val_accuracy: 0.5104\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3696 - accuracy: 0.4535 - val_loss: 2.3349 - val_accuracy: 0.4737\n","Epoch 42/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3921 - accuracy: 0.4502 - val_loss: 2.2973 - val_accuracy: 0.5139\n","Epoch 43/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3805 - accuracy: 0.4599 - val_loss: 2.2408 - val_accuracy: 0.5079\n","Epoch 44/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3669 - accuracy: 0.4582 - val_loss: 2.2232 - val_accuracy: 0.5295\n","Epoch 45/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3659 - accuracy: 0.4530 - val_loss: 2.2768 - val_accuracy: 0.5085\n","Epoch 46/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3627 - accuracy: 0.4518 - val_loss: 2.2752 - val_accuracy: 0.4718\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3648 - accuracy: 0.4611 - val_loss: 2.2514 - val_accuracy: 0.5176\n","Epoch 48/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3717 - accuracy: 0.4584 - val_loss: 2.2538 - val_accuracy: 0.5011\n","Epoch 49/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3691 - accuracy: 0.4621 - val_loss: 2.2602 - val_accuracy: 0.4985\n","Epoch 50/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3549 - accuracy: 0.4517 - val_loss: 2.2697 - val_accuracy: 0.4932\n","Epoch 51/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3404 - accuracy: 0.4612 - val_loss: 2.2440 - val_accuracy: 0.5073\n","Epoch 52/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3410 - accuracy: 0.4647 - val_loss: 2.1773 - val_accuracy: 0.5150\n","Epoch 53/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3075 - accuracy: 0.4603 - val_loss: 2.1894 - val_accuracy: 0.5069\n","Epoch 54/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3192 - accuracy: 0.4520 - val_loss: 2.2006 - val_accuracy: 0.5133\n","Epoch 55/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3282 - accuracy: 0.4579 - val_loss: 2.1936 - val_accuracy: 0.5063\n","Epoch 56/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3016 - accuracy: 0.4630 - val_loss: 2.2379 - val_accuracy: 0.5059\n","Epoch 57/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3159 - accuracy: 0.4624 - val_loss: 2.1907 - val_accuracy: 0.5048\n","Epoch 58/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3033 - accuracy: 0.4607 - val_loss: 2.1903 - val_accuracy: 0.5149\n","Epoch 59/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3093 - accuracy: 0.4588 - val_loss: 2.1717 - val_accuracy: 0.5197\n","Epoch 60/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2873 - accuracy: 0.4657 - val_loss: 2.1747 - val_accuracy: 0.5187\n","Epoch 61/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.3099 - accuracy: 0.4523 - val_loss: 2.1965 - val_accuracy: 0.5057\n","Epoch 62/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3009 - accuracy: 0.4628 - val_loss: 2.1561 - val_accuracy: 0.5045\n","Epoch 63/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3013 - accuracy: 0.4636 - val_loss: 2.2127 - val_accuracy: 0.4992\n","Epoch 64/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2791 - accuracy: 0.4615 - val_loss: 2.1660 - val_accuracy: 0.5129\n","Epoch 65/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2801 - accuracy: 0.4629 - val_loss: 2.1843 - val_accuracy: 0.4932\n","Epoch 66/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3098 - accuracy: 0.4642 - val_loss: 2.1907 - val_accuracy: 0.5036\n","Epoch 67/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3208 - accuracy: 0.4686 - val_loss: 2.1760 - val_accuracy: 0.5186\n","Epoch 68/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2708 - accuracy: 0.4657 - val_loss: 2.1412 - val_accuracy: 0.5232\n","Epoch 69/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2803 - accuracy: 0.4646 - val_loss: 2.1952 - val_accuracy: 0.5022\n","Epoch 70/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3062 - accuracy: 0.4594 - val_loss: 2.1648 - val_accuracy: 0.5000\n","Epoch 71/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3101 - accuracy: 0.4619 - val_loss: 2.1603 - val_accuracy: 0.5259\n","Epoch 72/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2709 - accuracy: 0.4689 - val_loss: 2.1419 - val_accuracy: 0.5125\n","Epoch 73/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2733 - accuracy: 0.4681 - val_loss: 2.1711 - val_accuracy: 0.5120\n","Epoch 74/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2771 - accuracy: 0.4647 - val_loss: 2.2188 - val_accuracy: 0.4924\n","Epoch 75/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.3008 - accuracy: 0.4676 - val_loss: 2.1757 - val_accuracy: 0.5022\n","Epoch 76/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2677 - accuracy: 0.4638 - val_loss: 2.1688 - val_accuracy: 0.5141\n","Epoch 77/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2967 - accuracy: 0.4636 - val_loss: 2.1750 - val_accuracy: 0.5051\n","Epoch 78/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2635 - accuracy: 0.4676 - val_loss: 2.1713 - val_accuracy: 0.5069\n","Epoch 79/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.2666 - accuracy: 0.4659 - val_loss: 2.1967 - val_accuracy: 0.5026\n","Epoch 80/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2888 - accuracy: 0.4654 - val_loss: 2.1643 - val_accuracy: 0.4945\n","Epoch 81/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2691 - accuracy: 0.4695 - val_loss: 2.1769 - val_accuracy: 0.4993\n","Epoch 82/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2636 - accuracy: 0.4650 - val_loss: 2.1202 - val_accuracy: 0.5231\n","Epoch 83/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2389 - accuracy: 0.4656 - val_loss: 2.1313 - val_accuracy: 0.5179\n","Epoch 84/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2806 - accuracy: 0.4637 - val_loss: 2.2086 - val_accuracy: 0.5007\n","Epoch 85/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2676 - accuracy: 0.4616 - val_loss: 2.1148 - val_accuracy: 0.5203\n","Epoch 86/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2749 - accuracy: 0.4634 - val_loss: 2.1923 - val_accuracy: 0.5141\n","Epoch 87/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2494 - accuracy: 0.4667 - val_loss: 2.1767 - val_accuracy: 0.4914\n","Epoch 88/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2713 - accuracy: 0.4633 - val_loss: 2.1458 - val_accuracy: 0.5095\n","Epoch 89/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2649 - accuracy: 0.4640 - val_loss: 2.1148 - val_accuracy: 0.5171\n","Epoch 90/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2393 - accuracy: 0.4714 - val_loss: 2.1021 - val_accuracy: 0.5103\n","Epoch 91/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2432 - accuracy: 0.4720 - val_loss: 2.1376 - val_accuracy: 0.5078\n","Epoch 92/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2354 - accuracy: 0.4640 - val_loss: 2.0865 - val_accuracy: 0.5227\n","Epoch 93/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2485 - accuracy: 0.4642 - val_loss: 2.1868 - val_accuracy: 0.5171\n","Epoch 94/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.2693 - accuracy: 0.4634 - val_loss: 2.1737 - val_accuracy: 0.5078\n","Epoch 95/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2552 - accuracy: 0.4692 - val_loss: 2.1248 - val_accuracy: 0.5205\n","Epoch 96/100\n","301/301 [==============================] - 2s 8ms/step - loss: 2.2338 - accuracy: 0.4686 - val_loss: 2.1149 - val_accuracy: 0.5163\n","Epoch 97/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2377 - accuracy: 0.4664 - val_loss: 2.1346 - val_accuracy: 0.5116\n","Epoch 98/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.2340 - accuracy: 0.4707 - val_loss: 2.1384 - val_accuracy: 0.5100\n","Epoch 99/100\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2328 - accuracy: 0.4620 - val_loss: 2.1076 - val_accuracy: 0.5117\n","Epoch 100/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2375 - accuracy: 0.4642 - val_loss: 2.1301 - val_accuracy: 0.5230\n"]}]},{"cell_type":"code","source":["# val loss and train loss were pretty similar, can we add just a little more regularized complexity?\n","# maybe dropout makes a difference\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1',bias_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(4,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dropout(0.25),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6oYV31x8LG-","executionInfo":{"status":"ok","timestamp":1652053762249,"user_tz":240,"elapsed":383201,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"94c7a6b0-1bf7-464c-a2d9-6c8b93c8229a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 5s 14ms/step - loss: 14.7420 - accuracy: 0.2396 - val_loss: 4.1664 - val_accuracy: 0.0870\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.8013 - accuracy: 0.2737 - val_loss: 3.4865 - val_accuracy: 0.3512\n","Epoch 3/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.6122 - accuracy: 0.2844 - val_loss: 3.3684 - val_accuracy: 0.3631\n","Epoch 4/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.4225 - accuracy: 0.3006 - val_loss: 3.2223 - val_accuracy: 0.3839\n","Epoch 5/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.2787 - accuracy: 0.3096 - val_loss: 2.9786 - val_accuracy: 0.4142\n","Epoch 6/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.1969 - accuracy: 0.3202 - val_loss: 2.8917 - val_accuracy: 0.4070\n","Epoch 7/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.0932 - accuracy: 0.3203 - val_loss: 2.8793 - val_accuracy: 0.4492\n","Epoch 8/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.0810 - accuracy: 0.3251 - val_loss: 2.9413 - val_accuracy: 0.3897\n","Epoch 9/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.0390 - accuracy: 0.3293 - val_loss: 2.7306 - val_accuracy: 0.4486\n","Epoch 10/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.9700 - accuracy: 0.3384 - val_loss: 2.7717 - val_accuracy: 0.3963\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9351 - accuracy: 0.3362 - val_loss: 2.6643 - val_accuracy: 0.4475\n","Epoch 12/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.9121 - accuracy: 0.3412 - val_loss: 2.6471 - val_accuracy: 0.4711\n","Epoch 13/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8780 - accuracy: 0.3405 - val_loss: 2.6787 - val_accuracy: 0.4303\n","Epoch 14/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.8520 - accuracy: 0.3482 - val_loss: 2.5778 - val_accuracy: 0.4306\n","Epoch 15/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.8337 - accuracy: 0.3615 - val_loss: 2.6034 - val_accuracy: 0.4460\n","Epoch 16/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7958 - accuracy: 0.3520 - val_loss: 2.4643 - val_accuracy: 0.4380\n","Epoch 17/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7680 - accuracy: 0.3549 - val_loss: 2.5343 - val_accuracy: 0.4561\n","Epoch 18/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7137 - accuracy: 0.3657 - val_loss: 2.5821 - val_accuracy: 0.4843\n","Epoch 19/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.7261 - accuracy: 0.3695 - val_loss: 2.5269 - val_accuracy: 0.4836\n","Epoch 20/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7134 - accuracy: 0.3739 - val_loss: 2.4765 - val_accuracy: 0.4616\n","Epoch 21/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6983 - accuracy: 0.3583 - val_loss: 2.4212 - val_accuracy: 0.4482\n","Epoch 22/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6482 - accuracy: 0.3672 - val_loss: 2.4552 - val_accuracy: 0.4664\n","Epoch 23/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7050 - accuracy: 0.3675 - val_loss: 2.5468 - val_accuracy: 0.4247\n","Epoch 24/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6344 - accuracy: 0.3743 - val_loss: 2.3710 - val_accuracy: 0.4833\n","Epoch 25/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.6343 - accuracy: 0.3705 - val_loss: 2.3990 - val_accuracy: 0.4642\n","Epoch 26/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6216 - accuracy: 0.3740 - val_loss: 2.3427 - val_accuracy: 0.4799\n","Epoch 27/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6616 - accuracy: 0.3676 - val_loss: 2.5034 - val_accuracy: 0.4643\n","Epoch 28/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6025 - accuracy: 0.3678 - val_loss: 2.3276 - val_accuracy: 0.4769\n","Epoch 29/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.5868 - accuracy: 0.3720 - val_loss: 2.3357 - val_accuracy: 0.4588\n","Epoch 30/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5839 - accuracy: 0.3767 - val_loss: 2.3892 - val_accuracy: 0.4668\n","Epoch 31/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5812 - accuracy: 0.3731 - val_loss: 2.3798 - val_accuracy: 0.4348\n","Epoch 32/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5732 - accuracy: 0.3792 - val_loss: 2.3616 - val_accuracy: 0.4489\n","Epoch 33/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5540 - accuracy: 0.3726 - val_loss: 2.3741 - val_accuracy: 0.4616\n","Epoch 34/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5807 - accuracy: 0.3691 - val_loss: 2.3401 - val_accuracy: 0.4560\n","Epoch 35/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5635 - accuracy: 0.3769 - val_loss: 2.3570 - val_accuracy: 0.4609\n","Epoch 36/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5563 - accuracy: 0.3723 - val_loss: 2.3567 - val_accuracy: 0.4666\n","Epoch 37/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.5531 - accuracy: 0.3722 - val_loss: 2.3793 - val_accuracy: 0.4517\n","Epoch 38/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.5270 - accuracy: 0.3752 - val_loss: 2.3188 - val_accuracy: 0.4529\n","Epoch 39/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5084 - accuracy: 0.3713 - val_loss: 2.2993 - val_accuracy: 0.4721\n","Epoch 40/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5465 - accuracy: 0.3827 - val_loss: 2.3650 - val_accuracy: 0.4539\n","Epoch 41/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5176 - accuracy: 0.3775 - val_loss: 2.2894 - val_accuracy: 0.4715\n","Epoch 42/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4814 - accuracy: 0.3830 - val_loss: 2.2515 - val_accuracy: 0.4673\n","Epoch 43/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4759 - accuracy: 0.3774 - val_loss: 2.2566 - val_accuracy: 0.4703\n","Epoch 44/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4996 - accuracy: 0.3702 - val_loss: 2.2773 - val_accuracy: 0.4701\n","Epoch 45/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4812 - accuracy: 0.3742 - val_loss: 2.2477 - val_accuracy: 0.4708\n","Epoch 46/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4653 - accuracy: 0.3857 - val_loss: 2.2076 - val_accuracy: 0.4761\n","Epoch 47/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5063 - accuracy: 0.3752 - val_loss: 2.2902 - val_accuracy: 0.4795\n","Epoch 48/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.4935 - accuracy: 0.3796 - val_loss: 2.2912 - val_accuracy: 0.4706\n","Epoch 49/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5048 - accuracy: 0.3853 - val_loss: 2.2886 - val_accuracy: 0.4548\n","Epoch 50/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4897 - accuracy: 0.3822 - val_loss: 2.2900 - val_accuracy: 0.4733\n","Epoch 51/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4807 - accuracy: 0.3860 - val_loss: 2.2635 - val_accuracy: 0.4600\n","Epoch 52/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4866 - accuracy: 0.3851 - val_loss: 2.3154 - val_accuracy: 0.4425\n","Epoch 53/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4866 - accuracy: 0.3774 - val_loss: 2.2892 - val_accuracy: 0.4568\n","Epoch 54/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4456 - accuracy: 0.3923 - val_loss: 2.2704 - val_accuracy: 0.4583\n","Epoch 55/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4632 - accuracy: 0.3880 - val_loss: 2.2757 - val_accuracy: 0.4665\n","Epoch 56/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4578 - accuracy: 0.3805 - val_loss: 2.2229 - val_accuracy: 0.4591\n","Epoch 57/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4154 - accuracy: 0.3723 - val_loss: 2.2327 - val_accuracy: 0.4602\n","Epoch 58/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4138 - accuracy: 0.3850 - val_loss: 2.2604 - val_accuracy: 0.4548\n","Epoch 59/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4272 - accuracy: 0.3905 - val_loss: 2.2359 - val_accuracy: 0.4590\n","Epoch 60/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4359 - accuracy: 0.3874 - val_loss: 2.2232 - val_accuracy: 0.4640\n","Epoch 61/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4286 - accuracy: 0.3825 - val_loss: 2.2712 - val_accuracy: 0.4406\n","Epoch 62/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4336 - accuracy: 0.3856 - val_loss: 2.1879 - val_accuracy: 0.4712\n","Epoch 63/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4367 - accuracy: 0.3812 - val_loss: 2.2390 - val_accuracy: 0.4545\n","Epoch 64/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4228 - accuracy: 0.3852 - val_loss: 2.2692 - val_accuracy: 0.4601\n","Epoch 65/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4165 - accuracy: 0.3879 - val_loss: 2.1741 - val_accuracy: 0.4973\n","Epoch 66/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.4060 - accuracy: 0.3913 - val_loss: 2.2896 - val_accuracy: 0.4432\n","Epoch 67/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4268 - accuracy: 0.3888 - val_loss: 2.1914 - val_accuracy: 0.4640\n","Epoch 68/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4066 - accuracy: 0.3851 - val_loss: 2.2068 - val_accuracy: 0.4752\n","Epoch 69/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4064 - accuracy: 0.3950 - val_loss: 2.2476 - val_accuracy: 0.4799\n","Epoch 70/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4330 - accuracy: 0.3893 - val_loss: 2.2657 - val_accuracy: 0.4476\n","Epoch 71/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4410 - accuracy: 0.3864 - val_loss: 2.2202 - val_accuracy: 0.4751\n","Epoch 72/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.4147 - accuracy: 0.3915 - val_loss: 2.1978 - val_accuracy: 0.4787\n","Epoch 73/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4099 - accuracy: 0.3853 - val_loss: 2.2066 - val_accuracy: 0.4523\n","Epoch 74/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3799 - accuracy: 0.3868 - val_loss: 2.1554 - val_accuracy: 0.4729\n","Epoch 75/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3859 - accuracy: 0.3898 - val_loss: 2.2046 - val_accuracy: 0.4488\n","Epoch 76/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3954 - accuracy: 0.3769 - val_loss: 2.2530 - val_accuracy: 0.4373\n","Epoch 77/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3947 - accuracy: 0.3921 - val_loss: 2.1541 - val_accuracy: 0.4860\n","Epoch 78/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3888 - accuracy: 0.3869 - val_loss: 2.1539 - val_accuracy: 0.4852\n","Epoch 79/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4171 - accuracy: 0.3883 - val_loss: 2.1779 - val_accuracy: 0.4946\n","Epoch 80/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3797 - accuracy: 0.3897 - val_loss: 2.2115 - val_accuracy: 0.4520\n","Epoch 81/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3680 - accuracy: 0.3968 - val_loss: 2.2144 - val_accuracy: 0.4589\n","Epoch 82/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3889 - accuracy: 0.3928 - val_loss: 2.2017 - val_accuracy: 0.4622\n","Epoch 83/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.3741 - accuracy: 0.3951 - val_loss: 2.2187 - val_accuracy: 0.4382\n","Epoch 84/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.3566 - accuracy: 0.3989 - val_loss: 2.1950 - val_accuracy: 0.4527\n","Epoch 85/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.3975 - accuracy: 0.3872 - val_loss: 2.1691 - val_accuracy: 0.4672\n","Epoch 86/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3679 - accuracy: 0.3900 - val_loss: 2.1372 - val_accuracy: 0.4664\n","Epoch 87/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3703 - accuracy: 0.3929 - val_loss: 2.1811 - val_accuracy: 0.4691\n","Epoch 88/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3836 - accuracy: 0.3955 - val_loss: 2.0942 - val_accuracy: 0.4920\n","Epoch 89/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3831 - accuracy: 0.3904 - val_loss: 2.1456 - val_accuracy: 0.4592\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3831 - accuracy: 0.3899 - val_loss: 2.1839 - val_accuracy: 0.4611\n","Epoch 91/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3803 - accuracy: 0.3953 - val_loss: 2.1337 - val_accuracy: 0.4885\n","Epoch 92/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3699 - accuracy: 0.3974 - val_loss: 2.1413 - val_accuracy: 0.5058\n","Epoch 93/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3510 - accuracy: 0.3936 - val_loss: 2.1521 - val_accuracy: 0.4727\n","Epoch 94/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3389 - accuracy: 0.3964 - val_loss: 2.1713 - val_accuracy: 0.4594\n","Epoch 95/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3651 - accuracy: 0.3881 - val_loss: 2.1197 - val_accuracy: 0.4933\n","Epoch 96/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3581 - accuracy: 0.3856 - val_loss: 2.1606 - val_accuracy: 0.4680\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3527 - accuracy: 0.3958 - val_loss: 2.1825 - val_accuracy: 0.4568\n","Epoch 98/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3709 - accuracy: 0.3952 - val_loss: 2.1618 - val_accuracy: 0.4619\n","Epoch 99/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3722 - accuracy: 0.3962 - val_loss: 2.1546 - val_accuracy: 0.4603\n","Epoch 100/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3401 - accuracy: 0.4021 - val_loss: 2.1586 - val_accuracy: 0.4588\n"]}]},{"cell_type":"code","source":["# val loss and train loss were pretty similar, can we add just a little more regularized complexity?\n","# maybe dropout makes a difference\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1',bias_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(8,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dropout(0.25),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqjzYVbC95HO","executionInfo":{"status":"ok","timestamp":1652054145247,"user_tz":240,"elapsed":383040,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"c464c077-2f07-4325-c2d6-4531e6fe0fe3"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 5s 14ms/step - loss: 15.4253 - accuracy: 0.2785 - val_loss: 4.6458 - val_accuracy: 0.2282\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 4.1498 - accuracy: 0.3268 - val_loss: 4.0213 - val_accuracy: 0.3404\n","Epoch 3/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.9044 - accuracy: 0.3330 - val_loss: 3.6179 - val_accuracy: 0.4080\n","Epoch 4/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.7115 - accuracy: 0.3406 - val_loss: 3.4294 - val_accuracy: 0.4662\n","Epoch 5/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.5490 - accuracy: 0.3432 - val_loss: 3.2862 - val_accuracy: 0.4359\n","Epoch 6/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.3768 - accuracy: 0.3576 - val_loss: 3.2538 - val_accuracy: 0.4473\n","Epoch 7/100\n","301/301 [==============================] - 4s 14ms/step - loss: 3.2826 - accuracy: 0.3641 - val_loss: 3.1454 - val_accuracy: 0.4157\n","Epoch 8/100\n","301/301 [==============================] - 3s 10ms/step - loss: 3.1675 - accuracy: 0.3766 - val_loss: 2.9258 - val_accuracy: 0.4776\n","Epoch 9/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.1039 - accuracy: 0.3733 - val_loss: 2.8535 - val_accuracy: 0.4503\n","Epoch 10/100\n","301/301 [==============================] - 4s 14ms/step - loss: 3.0422 - accuracy: 0.3858 - val_loss: 2.8280 - val_accuracy: 0.4889\n","Epoch 11/100\n","301/301 [==============================] - 4s 14ms/step - loss: 3.0454 - accuracy: 0.3906 - val_loss: 2.8298 - val_accuracy: 0.5099\n","Epoch 12/100\n","301/301 [==============================] - 3s 9ms/step - loss: 3.0085 - accuracy: 0.3954 - val_loss: 2.7602 - val_accuracy: 0.4804\n","Epoch 13/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9540 - accuracy: 0.3930 - val_loss: 2.7844 - val_accuracy: 0.4707\n","Epoch 14/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.9562 - accuracy: 0.4068 - val_loss: 2.7453 - val_accuracy: 0.4759\n","Epoch 15/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.9022 - accuracy: 0.4030 - val_loss: 2.6664 - val_accuracy: 0.4822\n","Epoch 16/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9072 - accuracy: 0.4050 - val_loss: 2.7176 - val_accuracy: 0.4851\n","Epoch 17/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.8868 - accuracy: 0.4097 - val_loss: 2.5395 - val_accuracy: 0.5119\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8337 - accuracy: 0.4100 - val_loss: 2.7159 - val_accuracy: 0.4937\n","Epoch 19/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.8290 - accuracy: 0.4228 - val_loss: 2.5533 - val_accuracy: 0.5170\n","Epoch 20/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7505 - accuracy: 0.4164 - val_loss: 2.5902 - val_accuracy: 0.5096\n","Epoch 21/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7477 - accuracy: 0.4176 - val_loss: 2.4543 - val_accuracy: 0.5499\n","Epoch 22/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7424 - accuracy: 0.4183 - val_loss: 2.5653 - val_accuracy: 0.4952\n","Epoch 23/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.7083 - accuracy: 0.4235 - val_loss: 2.5233 - val_accuracy: 0.5079\n","Epoch 24/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7041 - accuracy: 0.4163 - val_loss: 2.4557 - val_accuracy: 0.5326\n","Epoch 25/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.6754 - accuracy: 0.4268 - val_loss: 2.5153 - val_accuracy: 0.5080\n","Epoch 26/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6653 - accuracy: 0.4230 - val_loss: 2.4808 - val_accuracy: 0.5141\n","Epoch 27/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6626 - accuracy: 0.4279 - val_loss: 2.4055 - val_accuracy: 0.5232\n","Epoch 28/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6646 - accuracy: 0.4263 - val_loss: 2.3881 - val_accuracy: 0.5360\n","Epoch 29/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5796 - accuracy: 0.4338 - val_loss: 2.3882 - val_accuracy: 0.5160\n","Epoch 30/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.6026 - accuracy: 0.4258 - val_loss: 2.3446 - val_accuracy: 0.5237\n","Epoch 31/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5757 - accuracy: 0.4277 - val_loss: 2.3841 - val_accuracy: 0.5326\n","Epoch 32/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5945 - accuracy: 0.4312 - val_loss: 2.3230 - val_accuracy: 0.5307\n","Epoch 33/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.5479 - accuracy: 0.4365 - val_loss: 2.3527 - val_accuracy: 0.5183\n","Epoch 34/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.5586 - accuracy: 0.4304 - val_loss: 2.3369 - val_accuracy: 0.5271\n","Epoch 35/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5466 - accuracy: 0.4323 - val_loss: 2.3651 - val_accuracy: 0.5300\n","Epoch 36/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5848 - accuracy: 0.4332 - val_loss: 2.3796 - val_accuracy: 0.5204\n","Epoch 37/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5111 - accuracy: 0.4373 - val_loss: 2.3750 - val_accuracy: 0.4848\n","Epoch 38/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5137 - accuracy: 0.4390 - val_loss: 2.3098 - val_accuracy: 0.5378\n","Epoch 39/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5303 - accuracy: 0.4337 - val_loss: 2.2973 - val_accuracy: 0.5443\n","Epoch 40/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5107 - accuracy: 0.4435 - val_loss: 2.2586 - val_accuracy: 0.5411\n","Epoch 41/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4764 - accuracy: 0.4331 - val_loss: 2.2927 - val_accuracy: 0.5288\n","Epoch 42/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4940 - accuracy: 0.4371 - val_loss: 2.2943 - val_accuracy: 0.5385\n","Epoch 43/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4865 - accuracy: 0.4413 - val_loss: 2.2854 - val_accuracy: 0.5319\n","Epoch 44/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5099 - accuracy: 0.4372 - val_loss: 2.2641 - val_accuracy: 0.5372\n","Epoch 45/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5068 - accuracy: 0.4425 - val_loss: 2.2882 - val_accuracy: 0.5411\n","Epoch 46/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.5039 - accuracy: 0.4375 - val_loss: 2.2669 - val_accuracy: 0.5335\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4596 - accuracy: 0.4385 - val_loss: 2.2558 - val_accuracy: 0.5208\n","Epoch 48/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4478 - accuracy: 0.4443 - val_loss: 2.2880 - val_accuracy: 0.5267\n","Epoch 49/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4276 - accuracy: 0.4458 - val_loss: 2.1813 - val_accuracy: 0.5494\n","Epoch 50/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4311 - accuracy: 0.4375 - val_loss: 2.2331 - val_accuracy: 0.5308\n","Epoch 51/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3952 - accuracy: 0.4498 - val_loss: 2.2052 - val_accuracy: 0.5205\n","Epoch 52/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4118 - accuracy: 0.4426 - val_loss: 2.1885 - val_accuracy: 0.5491\n","Epoch 53/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4274 - accuracy: 0.4449 - val_loss: 2.2231 - val_accuracy: 0.5339\n","Epoch 54/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4354 - accuracy: 0.4410 - val_loss: 2.1747 - val_accuracy: 0.5117\n","Epoch 55/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4176 - accuracy: 0.4468 - val_loss: 2.1610 - val_accuracy: 0.5518\n","Epoch 56/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4025 - accuracy: 0.4458 - val_loss: 2.2151 - val_accuracy: 0.5351\n","Epoch 57/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3955 - accuracy: 0.4476 - val_loss: 2.2283 - val_accuracy: 0.5223\n","Epoch 58/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3835 - accuracy: 0.4407 - val_loss: 2.1623 - val_accuracy: 0.5446\n","Epoch 59/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3732 - accuracy: 0.4435 - val_loss: 2.1905 - val_accuracy: 0.5259\n","Epoch 60/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4005 - accuracy: 0.4366 - val_loss: 2.1624 - val_accuracy: 0.5427\n","Epoch 61/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.4031 - accuracy: 0.4428 - val_loss: 2.1920 - val_accuracy: 0.5381\n","Epoch 62/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3829 - accuracy: 0.4423 - val_loss: 2.1545 - val_accuracy: 0.5447\n","Epoch 63/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3748 - accuracy: 0.4475 - val_loss: 2.2024 - val_accuracy: 0.5171\n","Epoch 64/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3748 - accuracy: 0.4514 - val_loss: 2.2011 - val_accuracy: 0.5158\n","Epoch 65/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4138 - accuracy: 0.4469 - val_loss: 2.1926 - val_accuracy: 0.5255\n","Epoch 66/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3907 - accuracy: 0.4501 - val_loss: 2.1729 - val_accuracy: 0.5491\n","Epoch 67/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4067 - accuracy: 0.4424 - val_loss: 2.1608 - val_accuracy: 0.5399\n","Epoch 68/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3727 - accuracy: 0.4477 - val_loss: 2.1646 - val_accuracy: 0.5351\n","Epoch 69/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3769 - accuracy: 0.4462 - val_loss: 2.1201 - val_accuracy: 0.5550\n","Epoch 70/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.4054 - accuracy: 0.4438 - val_loss: 2.2192 - val_accuracy: 0.5230\n","Epoch 71/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3798 - accuracy: 0.4447 - val_loss: 2.1664 - val_accuracy: 0.5187\n","Epoch 72/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3966 - accuracy: 0.4490 - val_loss: 2.2217 - val_accuracy: 0.5151\n","Epoch 73/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3753 - accuracy: 0.4482 - val_loss: 2.1416 - val_accuracy: 0.5520\n","Epoch 74/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3952 - accuracy: 0.4496 - val_loss: 2.1730 - val_accuracy: 0.5282\n","Epoch 75/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3661 - accuracy: 0.4435 - val_loss: 2.1373 - val_accuracy: 0.5440\n","Epoch 76/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3470 - accuracy: 0.4482 - val_loss: 2.1200 - val_accuracy: 0.5386\n","Epoch 77/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3584 - accuracy: 0.4464 - val_loss: 2.1380 - val_accuracy: 0.5291\n","Epoch 78/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3380 - accuracy: 0.4426 - val_loss: 2.0958 - val_accuracy: 0.5474\n","Epoch 79/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3177 - accuracy: 0.4423 - val_loss: 2.1050 - val_accuracy: 0.5349\n","Epoch 80/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.3429 - accuracy: 0.4454 - val_loss: 2.1188 - val_accuracy: 0.5659\n","Epoch 81/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3411 - accuracy: 0.4457 - val_loss: 2.1246 - val_accuracy: 0.5454\n","Epoch 82/100\n","301/301 [==============================] - 3s 10ms/step - loss: 2.3438 - accuracy: 0.4491 - val_loss: 2.1630 - val_accuracy: 0.5306\n","Epoch 83/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3225 - accuracy: 0.4459 - val_loss: 2.1290 - val_accuracy: 0.5474\n","Epoch 84/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3427 - accuracy: 0.4453 - val_loss: 2.1396 - val_accuracy: 0.5534\n","Epoch 85/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3665 - accuracy: 0.4438 - val_loss: 2.1598 - val_accuracy: 0.5322\n","Epoch 86/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3526 - accuracy: 0.4507 - val_loss: 2.1267 - val_accuracy: 0.5284\n","Epoch 87/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3370 - accuracy: 0.4475 - val_loss: 2.1669 - val_accuracy: 0.5287\n","Epoch 88/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3300 - accuracy: 0.4506 - val_loss: 2.1174 - val_accuracy: 0.5363\n","Epoch 89/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3215 - accuracy: 0.4407 - val_loss: 2.0928 - val_accuracy: 0.5587\n","Epoch 90/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3164 - accuracy: 0.4496 - val_loss: 2.0630 - val_accuracy: 0.5477\n","Epoch 91/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3133 - accuracy: 0.4555 - val_loss: 2.1476 - val_accuracy: 0.5295\n","Epoch 92/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3541 - accuracy: 0.4508 - val_loss: 2.0718 - val_accuracy: 0.5613\n","Epoch 93/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3291 - accuracy: 0.4483 - val_loss: 2.1432 - val_accuracy: 0.5271\n","Epoch 94/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3124 - accuracy: 0.4542 - val_loss: 2.2028 - val_accuracy: 0.5157\n","Epoch 95/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3544 - accuracy: 0.4480 - val_loss: 2.1272 - val_accuracy: 0.5503\n","Epoch 96/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3378 - accuracy: 0.4519 - val_loss: 2.1072 - val_accuracy: 0.5465\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3385 - accuracy: 0.4520 - val_loss: 2.0955 - val_accuracy: 0.5348\n","Epoch 98/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3547 - accuracy: 0.4463 - val_loss: 2.1153 - val_accuracy: 0.5329\n","Epoch 99/100\n","301/301 [==============================] - 3s 9ms/step - loss: 2.3346 - accuracy: 0.4509 - val_loss: 2.1008 - val_accuracy: 0.5541\n","Epoch 100/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.3458 - accuracy: 0.4454 - val_loss: 2.1530 - val_accuracy: 0.5448\n"]}]},{"cell_type":"code","source":["'''\n","# val loss and train loss were pretty similar, can we add just a little more regularized complexity?\n","# maybe dropout makes a difference\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1',bias_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(4,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))\n","'''"],"metadata":{"id":"TyA80-Y-9L-o","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1652054145259,"user_tz":240,"elapsed":43,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"a1331cef-dc3b-41b9-81d2-9f07e80a68a0"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# val loss and train loss were pretty similar, can we add just a little more regularized complexity?\\n# maybe dropout makes a difference\\nlayer_list = [ #520 parameters in input\\n          layers.BatchNormalization(),\\n          layers.Dropout(0.4),\\n          layers.Dense(520,kernel_initializer=\\'lecun_normal\\',kernel_regularizer=\\'l1\\',bias_regularizer=\\'l1\\'),\\n          layers.BatchNormalization(),\\n          layers.Activation(\\'selu\\'),\\n          layers.Dense(4,kernel_initializer=\\'lecun_normal\\'), #let\\'s try dimensionality reduction\\n          layers.BatchNormalization(),\\n          layers.Activation(\\'selu\\'),\\n          layers.Dense(16,kernel_initializer=\\'lecun_normal\\'), \\n          layers.BatchNormalization(),\\n          layers.Activation(\\'selu\\'),\\n          layers.Dense(16,activation=\\'softmax\\',kernel_initializer=\\'lecun_normal\\'),\\n          ]\\nmodel = Sequential(layer_list)\\nmodel.compile(optimizer=keras.optimizers.Adam(\\n    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\\n    loss=\"categorical_crossentropy\", metrics=[\\'accuracy\\'],)\\nhistory = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# further modifying the previous one that had the lowest validation loss\n","# clearly we need some strong regularization with this small training set\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S7Q1HLt23-YY","executionInfo":{"status":"ok","timestamp":1652055033565,"user_tz":240,"elapsed":888338,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"08894845-83a8-412f-8ff3-1492545ca1fa"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/400\n","301/301 [==============================] - 3s 8ms/step - loss: 2.2214 - accuracy: 0.3266 - val_loss: 1.8802 - val_accuracy: 0.3886\n","Epoch 2/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8050 - accuracy: 0.4469 - val_loss: 1.6293 - val_accuracy: 0.5144\n","Epoch 3/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.6661 - accuracy: 0.4802 - val_loss: 1.5500 - val_accuracy: 0.5348\n","Epoch 4/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.6114 - accuracy: 0.4980 - val_loss: 1.4935 - val_accuracy: 0.5435\n","Epoch 5/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5682 - accuracy: 0.5049 - val_loss: 1.4675 - val_accuracy: 0.5518\n","Epoch 6/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5322 - accuracy: 0.5125 - val_loss: 1.4595 - val_accuracy: 0.5566\n","Epoch 7/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5244 - accuracy: 0.5107 - val_loss: 1.4397 - val_accuracy: 0.5633\n","Epoch 8/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5085 - accuracy: 0.5147 - val_loss: 1.4442 - val_accuracy: 0.5566\n","Epoch 9/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4994 - accuracy: 0.5226 - val_loss: 1.4241 - val_accuracy: 0.5603\n","Epoch 10/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.5017 - accuracy: 0.5225 - val_loss: 1.3969 - val_accuracy: 0.5756\n","Epoch 11/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4818 - accuracy: 0.5229 - val_loss: 1.4114 - val_accuracy: 0.5657\n","Epoch 12/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4818 - accuracy: 0.5262 - val_loss: 1.4088 - val_accuracy: 0.5647\n","Epoch 13/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4775 - accuracy: 0.5243 - val_loss: 1.3960 - val_accuracy: 0.5751\n","Epoch 14/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4667 - accuracy: 0.5243 - val_loss: 1.3968 - val_accuracy: 0.5708\n","Epoch 15/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4651 - accuracy: 0.5279 - val_loss: 1.4060 - val_accuracy: 0.5688\n","Epoch 16/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4650 - accuracy: 0.5256 - val_loss: 1.3951 - val_accuracy: 0.5724\n","Epoch 17/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4532 - accuracy: 0.5332 - val_loss: 1.3903 - val_accuracy: 0.5732\n","Epoch 18/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4602 - accuracy: 0.5295 - val_loss: 1.4113 - val_accuracy: 0.5635\n","Epoch 19/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4489 - accuracy: 0.5311 - val_loss: 1.4142 - val_accuracy: 0.5652\n","Epoch 20/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4542 - accuracy: 0.5319 - val_loss: 1.3910 - val_accuracy: 0.5710\n","Epoch 21/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4564 - accuracy: 0.5298 - val_loss: 1.3874 - val_accuracy: 0.5726\n","Epoch 22/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4478 - accuracy: 0.5300 - val_loss: 1.3830 - val_accuracy: 0.5727\n","Epoch 23/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4421 - accuracy: 0.5345 - val_loss: 1.3849 - val_accuracy: 0.5772\n","Epoch 24/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4471 - accuracy: 0.5302 - val_loss: 1.3823 - val_accuracy: 0.5703\n","Epoch 25/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4493 - accuracy: 0.5363 - val_loss: 1.3944 - val_accuracy: 0.5728\n","Epoch 26/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4526 - accuracy: 0.5310 - val_loss: 1.4053 - val_accuracy: 0.5697\n","Epoch 27/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4427 - accuracy: 0.5361 - val_loss: 1.3935 - val_accuracy: 0.5681\n","Epoch 28/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4405 - accuracy: 0.5316 - val_loss: 1.4003 - val_accuracy: 0.5673\n","Epoch 29/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4495 - accuracy: 0.5339 - val_loss: 1.3809 - val_accuracy: 0.5742\n","Epoch 30/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4492 - accuracy: 0.5296 - val_loss: 1.3999 - val_accuracy: 0.5700\n","Epoch 31/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4483 - accuracy: 0.5349 - val_loss: 1.4094 - val_accuracy: 0.5636\n","Epoch 32/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4455 - accuracy: 0.5311 - val_loss: 1.3977 - val_accuracy: 0.5654\n","Epoch 33/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4441 - accuracy: 0.5342 - val_loss: 1.3906 - val_accuracy: 0.5687\n","Epoch 34/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4370 - accuracy: 0.5369 - val_loss: 1.3842 - val_accuracy: 0.5780\n","Epoch 35/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4415 - accuracy: 0.5311 - val_loss: 1.3851 - val_accuracy: 0.5749\n","Epoch 36/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4384 - accuracy: 0.5315 - val_loss: 1.3847 - val_accuracy: 0.5744\n","Epoch 37/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4305 - accuracy: 0.5397 - val_loss: 1.3824 - val_accuracy: 0.5702\n","Epoch 38/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4281 - accuracy: 0.5320 - val_loss: 1.3948 - val_accuracy: 0.5700\n","Epoch 39/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4394 - accuracy: 0.5340 - val_loss: 1.3916 - val_accuracy: 0.5716\n","Epoch 40/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4415 - accuracy: 0.5334 - val_loss: 1.3917 - val_accuracy: 0.5712\n","Epoch 41/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4299 - accuracy: 0.5373 - val_loss: 1.3853 - val_accuracy: 0.5724\n","Epoch 42/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4373 - accuracy: 0.5336 - val_loss: 1.3759 - val_accuracy: 0.5777\n","Epoch 43/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4372 - accuracy: 0.5296 - val_loss: 1.3807 - val_accuracy: 0.5726\n","Epoch 44/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4327 - accuracy: 0.5410 - val_loss: 1.3863 - val_accuracy: 0.5700\n","Epoch 45/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4383 - accuracy: 0.5301 - val_loss: 1.3842 - val_accuracy: 0.5729\n","Epoch 46/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4376 - accuracy: 0.5351 - val_loss: 1.3792 - val_accuracy: 0.5750\n","Epoch 47/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4218 - accuracy: 0.5396 - val_loss: 1.3809 - val_accuracy: 0.5737\n","Epoch 48/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4319 - accuracy: 0.5412 - val_loss: 1.3944 - val_accuracy: 0.5652\n","Epoch 49/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4392 - accuracy: 0.5333 - val_loss: 1.3736 - val_accuracy: 0.5799\n","Epoch 50/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4242 - accuracy: 0.5366 - val_loss: 1.3743 - val_accuracy: 0.5754\n","Epoch 51/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4313 - accuracy: 0.5315 - val_loss: 1.4009 - val_accuracy: 0.5648\n","Epoch 52/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4211 - accuracy: 0.5393 - val_loss: 1.3774 - val_accuracy: 0.5733\n","Epoch 53/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4314 - accuracy: 0.5368 - val_loss: 1.3781 - val_accuracy: 0.5726\n","Epoch 54/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4309 - accuracy: 0.5386 - val_loss: 1.4021 - val_accuracy: 0.5686\n","Epoch 55/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4313 - accuracy: 0.5349 - val_loss: 1.3884 - val_accuracy: 0.5713\n","Epoch 56/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4306 - accuracy: 0.5376 - val_loss: 1.3808 - val_accuracy: 0.5717\n","Epoch 57/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4401 - accuracy: 0.5319 - val_loss: 1.3887 - val_accuracy: 0.5694\n","Epoch 58/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4312 - accuracy: 0.5328 - val_loss: 1.3835 - val_accuracy: 0.5747\n","Epoch 59/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4137 - accuracy: 0.5440 - val_loss: 1.3942 - val_accuracy: 0.5689\n","Epoch 60/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4256 - accuracy: 0.5399 - val_loss: 1.3764 - val_accuracy: 0.5768\n","Epoch 61/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4227 - accuracy: 0.5371 - val_loss: 1.3852 - val_accuracy: 0.5745\n","Epoch 62/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4217 - accuracy: 0.5409 - val_loss: 1.3897 - val_accuracy: 0.5701\n","Epoch 63/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4331 - accuracy: 0.5323 - val_loss: 1.3683 - val_accuracy: 0.5787\n","Epoch 64/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4270 - accuracy: 0.5362 - val_loss: 1.3814 - val_accuracy: 0.5756\n","Epoch 65/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4251 - accuracy: 0.5403 - val_loss: 1.3784 - val_accuracy: 0.5748\n","Epoch 66/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4256 - accuracy: 0.5401 - val_loss: 1.3824 - val_accuracy: 0.5760\n","Epoch 67/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4300 - accuracy: 0.5307 - val_loss: 1.3696 - val_accuracy: 0.5799\n","Epoch 68/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4259 - accuracy: 0.5427 - val_loss: 1.3838 - val_accuracy: 0.5730\n","Epoch 69/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4348 - accuracy: 0.5362 - val_loss: 1.3724 - val_accuracy: 0.5784\n","Epoch 70/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4201 - accuracy: 0.5393 - val_loss: 1.3704 - val_accuracy: 0.5772\n","Epoch 71/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4255 - accuracy: 0.5378 - val_loss: 1.3875 - val_accuracy: 0.5737\n","Epoch 72/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4160 - accuracy: 0.5367 - val_loss: 1.3922 - val_accuracy: 0.5697\n","Epoch 73/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4252 - accuracy: 0.5375 - val_loss: 1.3747 - val_accuracy: 0.5760\n","Epoch 74/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4196 - accuracy: 0.5406 - val_loss: 1.3706 - val_accuracy: 0.5792\n","Epoch 75/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4199 - accuracy: 0.5412 - val_loss: 1.3744 - val_accuracy: 0.5770\n","Epoch 76/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4261 - accuracy: 0.5376 - val_loss: 1.3874 - val_accuracy: 0.5714\n","Epoch 77/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4249 - accuracy: 0.5349 - val_loss: 1.3848 - val_accuracy: 0.5751\n","Epoch 78/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4214 - accuracy: 0.5366 - val_loss: 1.3775 - val_accuracy: 0.5748\n","Epoch 79/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4137 - accuracy: 0.5379 - val_loss: 1.3677 - val_accuracy: 0.5774\n","Epoch 80/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4278 - accuracy: 0.5422 - val_loss: 1.3654 - val_accuracy: 0.5795\n","Epoch 81/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4231 - accuracy: 0.5384 - val_loss: 1.3779 - val_accuracy: 0.5770\n","Epoch 82/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4253 - accuracy: 0.5358 - val_loss: 1.3765 - val_accuracy: 0.5770\n","Epoch 83/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4192 - accuracy: 0.5400 - val_loss: 1.3872 - val_accuracy: 0.5731\n","Epoch 84/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4206 - accuracy: 0.5397 - val_loss: 1.3875 - val_accuracy: 0.5710\n","Epoch 85/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4191 - accuracy: 0.5376 - val_loss: 1.3744 - val_accuracy: 0.5792\n","Epoch 86/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4197 - accuracy: 0.5383 - val_loss: 1.3665 - val_accuracy: 0.5783\n","Epoch 87/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4245 - accuracy: 0.5370 - val_loss: 1.3738 - val_accuracy: 0.5771\n","Epoch 88/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4263 - accuracy: 0.5314 - val_loss: 1.3797 - val_accuracy: 0.5771\n","Epoch 89/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4250 - accuracy: 0.5397 - val_loss: 1.3843 - val_accuracy: 0.5741\n","Epoch 90/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4231 - accuracy: 0.5380 - val_loss: 1.3866 - val_accuracy: 0.5728\n","Epoch 91/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4286 - accuracy: 0.5379 - val_loss: 1.3837 - val_accuracy: 0.5750\n","Epoch 92/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4159 - accuracy: 0.5447 - val_loss: 1.3903 - val_accuracy: 0.5733\n","Epoch 93/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4217 - accuracy: 0.5373 - val_loss: 1.3707 - val_accuracy: 0.5793\n","Epoch 94/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4217 - accuracy: 0.5369 - val_loss: 1.3786 - val_accuracy: 0.5752\n","Epoch 95/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4200 - accuracy: 0.5402 - val_loss: 1.3761 - val_accuracy: 0.5764\n","Epoch 96/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4303 - accuracy: 0.5335 - val_loss: 1.3622 - val_accuracy: 0.5803\n","Epoch 97/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4228 - accuracy: 0.5387 - val_loss: 1.3845 - val_accuracy: 0.5725\n","Epoch 98/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4168 - accuracy: 0.5415 - val_loss: 1.3736 - val_accuracy: 0.5776\n","Epoch 99/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4218 - accuracy: 0.5377 - val_loss: 1.3898 - val_accuracy: 0.5714\n","Epoch 100/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4169 - accuracy: 0.5433 - val_loss: 1.3891 - val_accuracy: 0.5724\n","Epoch 101/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4137 - accuracy: 0.5439 - val_loss: 1.3842 - val_accuracy: 0.5724\n","Epoch 102/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4126 - accuracy: 0.5386 - val_loss: 1.3793 - val_accuracy: 0.5736\n","Epoch 103/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4227 - accuracy: 0.5414 - val_loss: 1.3744 - val_accuracy: 0.5751\n","Epoch 104/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4241 - accuracy: 0.5364 - val_loss: 1.3752 - val_accuracy: 0.5774\n","Epoch 105/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4146 - accuracy: 0.5416 - val_loss: 1.3900 - val_accuracy: 0.5689\n","Epoch 106/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4182 - accuracy: 0.5352 - val_loss: 1.3669 - val_accuracy: 0.5794\n","Epoch 107/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4247 - accuracy: 0.5384 - val_loss: 1.3666 - val_accuracy: 0.5779\n","Epoch 108/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4213 - accuracy: 0.5372 - val_loss: 1.3797 - val_accuracy: 0.5737\n","Epoch 109/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4232 - accuracy: 0.5376 - val_loss: 1.3848 - val_accuracy: 0.5732\n","Epoch 110/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4131 - accuracy: 0.5426 - val_loss: 1.3835 - val_accuracy: 0.5723\n","Epoch 111/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4287 - accuracy: 0.5353 - val_loss: 1.3691 - val_accuracy: 0.5783\n","Epoch 112/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4173 - accuracy: 0.5431 - val_loss: 1.3649 - val_accuracy: 0.5788\n","Epoch 113/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4161 - accuracy: 0.5364 - val_loss: 1.3722 - val_accuracy: 0.5778\n","Epoch 114/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4134 - accuracy: 0.5428 - val_loss: 1.3719 - val_accuracy: 0.5774\n","Epoch 115/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4089 - accuracy: 0.5423 - val_loss: 1.3790 - val_accuracy: 0.5761\n","Epoch 116/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4190 - accuracy: 0.5364 - val_loss: 1.3782 - val_accuracy: 0.5786\n","Epoch 117/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4187 - accuracy: 0.5406 - val_loss: 1.3744 - val_accuracy: 0.5771\n","Epoch 118/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4083 - accuracy: 0.5447 - val_loss: 1.3795 - val_accuracy: 0.5761\n","Epoch 119/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4157 - accuracy: 0.5371 - val_loss: 1.3811 - val_accuracy: 0.5778\n","Epoch 120/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4205 - accuracy: 0.5341 - val_loss: 1.3719 - val_accuracy: 0.5764\n","Epoch 121/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4138 - accuracy: 0.5435 - val_loss: 1.3755 - val_accuracy: 0.5757\n","Epoch 122/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4129 - accuracy: 0.5409 - val_loss: 1.3857 - val_accuracy: 0.5725\n","Epoch 123/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4108 - accuracy: 0.5388 - val_loss: 1.3734 - val_accuracy: 0.5771\n","Epoch 124/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4111 - accuracy: 0.5447 - val_loss: 1.3673 - val_accuracy: 0.5758\n","Epoch 125/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4094 - accuracy: 0.5393 - val_loss: 1.3616 - val_accuracy: 0.5802\n","Epoch 126/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4114 - accuracy: 0.5444 - val_loss: 1.3799 - val_accuracy: 0.5722\n","Epoch 127/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4170 - accuracy: 0.5385 - val_loss: 1.3668 - val_accuracy: 0.5794\n","Epoch 128/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4163 - accuracy: 0.5421 - val_loss: 1.3564 - val_accuracy: 0.5800\n","Epoch 129/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4189 - accuracy: 0.5430 - val_loss: 1.3813 - val_accuracy: 0.5732\n","Epoch 130/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4061 - accuracy: 0.5440 - val_loss: 1.3859 - val_accuracy: 0.5713\n","Epoch 131/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4126 - accuracy: 0.5418 - val_loss: 1.3765 - val_accuracy: 0.5755\n","Epoch 132/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4071 - accuracy: 0.5428 - val_loss: 1.3845 - val_accuracy: 0.5686\n","Epoch 133/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4085 - accuracy: 0.5414 - val_loss: 1.3896 - val_accuracy: 0.5693\n","Epoch 134/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4055 - accuracy: 0.5469 - val_loss: 1.3638 - val_accuracy: 0.5804\n","Epoch 135/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4132 - accuracy: 0.5424 - val_loss: 1.3666 - val_accuracy: 0.5808\n","Epoch 136/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4147 - accuracy: 0.5400 - val_loss: 1.3763 - val_accuracy: 0.5802\n","Epoch 137/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4132 - accuracy: 0.5388 - val_loss: 1.3687 - val_accuracy: 0.5823\n","Epoch 138/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4056 - accuracy: 0.5429 - val_loss: 1.3923 - val_accuracy: 0.5720\n","Epoch 139/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4053 - accuracy: 0.5427 - val_loss: 1.3726 - val_accuracy: 0.5800\n","Epoch 140/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4213 - accuracy: 0.5378 - val_loss: 1.3820 - val_accuracy: 0.5724\n","Epoch 141/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4101 - accuracy: 0.5496 - val_loss: 1.3710 - val_accuracy: 0.5789\n","Epoch 142/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4122 - accuracy: 0.5417 - val_loss: 1.3837 - val_accuracy: 0.5723\n","Epoch 143/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4148 - accuracy: 0.5426 - val_loss: 1.3689 - val_accuracy: 0.5810\n","Epoch 144/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4054 - accuracy: 0.5406 - val_loss: 1.3752 - val_accuracy: 0.5788\n","Epoch 145/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4143 - accuracy: 0.5425 - val_loss: 1.3760 - val_accuracy: 0.5773\n","Epoch 146/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4182 - accuracy: 0.5376 - val_loss: 1.3684 - val_accuracy: 0.5777\n","Epoch 147/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4209 - accuracy: 0.5347 - val_loss: 1.3758 - val_accuracy: 0.5757\n","Epoch 148/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4276 - accuracy: 0.5379 - val_loss: 1.3699 - val_accuracy: 0.5828\n","Epoch 149/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4025 - accuracy: 0.5452 - val_loss: 1.3767 - val_accuracy: 0.5793\n","Epoch 150/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4086 - accuracy: 0.5435 - val_loss: 1.3776 - val_accuracy: 0.5765\n","Epoch 151/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4072 - accuracy: 0.5399 - val_loss: 1.3854 - val_accuracy: 0.5754\n","Epoch 152/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4081 - accuracy: 0.5393 - val_loss: 1.3774 - val_accuracy: 0.5774\n","Epoch 153/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4101 - accuracy: 0.5406 - val_loss: 1.3682 - val_accuracy: 0.5806\n","Epoch 154/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3961 - accuracy: 0.5485 - val_loss: 1.3719 - val_accuracy: 0.5764\n","Epoch 155/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4082 - accuracy: 0.5412 - val_loss: 1.3683 - val_accuracy: 0.5811\n","Epoch 156/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4067 - accuracy: 0.5399 - val_loss: 1.3744 - val_accuracy: 0.5776\n","Epoch 157/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4125 - accuracy: 0.5376 - val_loss: 1.3583 - val_accuracy: 0.5816\n","Epoch 158/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4206 - accuracy: 0.5335 - val_loss: 1.3732 - val_accuracy: 0.5776\n","Epoch 159/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4036 - accuracy: 0.5398 - val_loss: 1.3772 - val_accuracy: 0.5743\n","Epoch 160/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4053 - accuracy: 0.5453 - val_loss: 1.3771 - val_accuracy: 0.5740\n","Epoch 161/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4104 - accuracy: 0.5407 - val_loss: 1.3684 - val_accuracy: 0.5790\n","Epoch 162/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4133 - accuracy: 0.5413 - val_loss: 1.3633 - val_accuracy: 0.5794\n","Epoch 163/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4142 - accuracy: 0.5417 - val_loss: 1.3787 - val_accuracy: 0.5751\n","Epoch 164/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4083 - accuracy: 0.5428 - val_loss: 1.3719 - val_accuracy: 0.5794\n","Epoch 165/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4141 - accuracy: 0.5402 - val_loss: 1.3871 - val_accuracy: 0.5730\n","Epoch 166/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4055 - accuracy: 0.5436 - val_loss: 1.3755 - val_accuracy: 0.5741\n","Epoch 167/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4102 - accuracy: 0.5430 - val_loss: 1.3801 - val_accuracy: 0.5744\n","Epoch 168/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4126 - accuracy: 0.5388 - val_loss: 1.3659 - val_accuracy: 0.5817\n","Epoch 169/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4122 - accuracy: 0.5410 - val_loss: 1.3642 - val_accuracy: 0.5799\n","Epoch 170/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4069 - accuracy: 0.5394 - val_loss: 1.3820 - val_accuracy: 0.5725\n","Epoch 171/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4235 - accuracy: 0.5345 - val_loss: 1.3830 - val_accuracy: 0.5727\n","Epoch 172/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4038 - accuracy: 0.5463 - val_loss: 1.3936 - val_accuracy: 0.5711\n","Epoch 173/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3933 - accuracy: 0.5390 - val_loss: 1.3660 - val_accuracy: 0.5790\n","Epoch 174/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4156 - accuracy: 0.5422 - val_loss: 1.3745 - val_accuracy: 0.5787\n","Epoch 175/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4078 - accuracy: 0.5384 - val_loss: 1.3762 - val_accuracy: 0.5773\n","Epoch 176/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4131 - accuracy: 0.5393 - val_loss: 1.3639 - val_accuracy: 0.5832\n","Epoch 177/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4097 - accuracy: 0.5438 - val_loss: 1.3840 - val_accuracy: 0.5742\n","Epoch 178/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4068 - accuracy: 0.5479 - val_loss: 1.3787 - val_accuracy: 0.5762\n","Epoch 179/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4143 - accuracy: 0.5462 - val_loss: 1.3685 - val_accuracy: 0.5784\n","Epoch 180/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4041 - accuracy: 0.5453 - val_loss: 1.3751 - val_accuracy: 0.5754\n","Epoch 181/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4174 - accuracy: 0.5403 - val_loss: 1.3795 - val_accuracy: 0.5747\n","Epoch 182/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4030 - accuracy: 0.5454 - val_loss: 1.3779 - val_accuracy: 0.5749\n","Epoch 183/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4124 - accuracy: 0.5443 - val_loss: 1.3841 - val_accuracy: 0.5745\n","Epoch 184/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4074 - accuracy: 0.5436 - val_loss: 1.3770 - val_accuracy: 0.5738\n","Epoch 185/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4126 - accuracy: 0.5401 - val_loss: 1.3641 - val_accuracy: 0.5798\n","Epoch 186/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4040 - accuracy: 0.5452 - val_loss: 1.3859 - val_accuracy: 0.5692\n","Epoch 187/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4126 - accuracy: 0.5386 - val_loss: 1.3740 - val_accuracy: 0.5788\n","Epoch 188/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3904 - accuracy: 0.5482 - val_loss: 1.3661 - val_accuracy: 0.5809\n","Epoch 189/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4050 - accuracy: 0.5410 - val_loss: 1.3768 - val_accuracy: 0.5730\n","Epoch 190/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4051 - accuracy: 0.5437 - val_loss: 1.3709 - val_accuracy: 0.5795\n","Epoch 191/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3965 - accuracy: 0.5446 - val_loss: 1.3740 - val_accuracy: 0.5783\n","Epoch 192/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4050 - accuracy: 0.5452 - val_loss: 1.3654 - val_accuracy: 0.5778\n","Epoch 193/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4118 - accuracy: 0.5385 - val_loss: 1.3817 - val_accuracy: 0.5745\n","Epoch 194/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4193 - accuracy: 0.5398 - val_loss: 1.3811 - val_accuracy: 0.5731\n","Epoch 195/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4112 - accuracy: 0.5433 - val_loss: 1.3870 - val_accuracy: 0.5734\n","Epoch 196/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4120 - accuracy: 0.5364 - val_loss: 1.3818 - val_accuracy: 0.5743\n","Epoch 197/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4059 - accuracy: 0.5383 - val_loss: 1.3850 - val_accuracy: 0.5763\n","Epoch 198/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4070 - accuracy: 0.5397 - val_loss: 1.3677 - val_accuracy: 0.5812\n","Epoch 199/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4080 - accuracy: 0.5376 - val_loss: 1.3735 - val_accuracy: 0.5792\n","Epoch 200/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4150 - accuracy: 0.5396 - val_loss: 1.3682 - val_accuracy: 0.5802\n","Epoch 201/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4035 - accuracy: 0.5429 - val_loss: 1.3673 - val_accuracy: 0.5777\n","Epoch 202/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4024 - accuracy: 0.5430 - val_loss: 1.3710 - val_accuracy: 0.5792\n","Epoch 203/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4040 - accuracy: 0.5435 - val_loss: 1.3711 - val_accuracy: 0.5764\n","Epoch 204/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4203 - accuracy: 0.5387 - val_loss: 1.3629 - val_accuracy: 0.5796\n","Epoch 205/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4091 - accuracy: 0.5455 - val_loss: 1.3647 - val_accuracy: 0.5795\n","Epoch 206/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3973 - accuracy: 0.5479 - val_loss: 1.3688 - val_accuracy: 0.5777\n","Epoch 207/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4029 - accuracy: 0.5439 - val_loss: 1.3633 - val_accuracy: 0.5810\n","Epoch 208/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4057 - accuracy: 0.5441 - val_loss: 1.3650 - val_accuracy: 0.5773\n","Epoch 209/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4157 - accuracy: 0.5406 - val_loss: 1.3604 - val_accuracy: 0.5789\n","Epoch 210/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4089 - accuracy: 0.5419 - val_loss: 1.3542 - val_accuracy: 0.5809\n","Epoch 211/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4056 - accuracy: 0.5425 - val_loss: 1.3697 - val_accuracy: 0.5776\n","Epoch 212/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4083 - accuracy: 0.5377 - val_loss: 1.3689 - val_accuracy: 0.5786\n","Epoch 213/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4045 - accuracy: 0.5436 - val_loss: 1.3613 - val_accuracy: 0.5810\n","Epoch 214/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4058 - accuracy: 0.5394 - val_loss: 1.3707 - val_accuracy: 0.5774\n","Epoch 215/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4069 - accuracy: 0.5450 - val_loss: 1.3796 - val_accuracy: 0.5741\n","Epoch 216/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4123 - accuracy: 0.5427 - val_loss: 1.3783 - val_accuracy: 0.5742\n","Epoch 217/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4081 - accuracy: 0.5385 - val_loss: 1.3737 - val_accuracy: 0.5757\n","Epoch 218/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4100 - accuracy: 0.5447 - val_loss: 1.3630 - val_accuracy: 0.5787\n","Epoch 219/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4009 - accuracy: 0.5463 - val_loss: 1.3617 - val_accuracy: 0.5808\n","Epoch 220/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3992 - accuracy: 0.5414 - val_loss: 1.3810 - val_accuracy: 0.5717\n","Epoch 221/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4044 - accuracy: 0.5436 - val_loss: 1.3752 - val_accuracy: 0.5744\n","Epoch 222/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4003 - accuracy: 0.5442 - val_loss: 1.3626 - val_accuracy: 0.5790\n","Epoch 223/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4149 - accuracy: 0.5367 - val_loss: 1.3763 - val_accuracy: 0.5761\n","Epoch 224/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4052 - accuracy: 0.5504 - val_loss: 1.3797 - val_accuracy: 0.5774\n","Epoch 225/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3994 - accuracy: 0.5453 - val_loss: 1.3752 - val_accuracy: 0.5770\n","Epoch 226/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4196 - accuracy: 0.5371 - val_loss: 1.3638 - val_accuracy: 0.5761\n","Epoch 227/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4067 - accuracy: 0.5395 - val_loss: 1.3624 - val_accuracy: 0.5800\n","Epoch 228/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4102 - accuracy: 0.5448 - val_loss: 1.3565 - val_accuracy: 0.5819\n","Epoch 229/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4040 - accuracy: 0.5449 - val_loss: 1.3649 - val_accuracy: 0.5775\n","Epoch 230/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4075 - accuracy: 0.5437 - val_loss: 1.3724 - val_accuracy: 0.5740\n","Epoch 231/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4066 - accuracy: 0.5444 - val_loss: 1.3602 - val_accuracy: 0.5814\n","Epoch 232/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3973 - accuracy: 0.5465 - val_loss: 1.3659 - val_accuracy: 0.5776\n","Epoch 233/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4035 - accuracy: 0.5369 - val_loss: 1.3662 - val_accuracy: 0.5799\n","Epoch 234/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3941 - accuracy: 0.5496 - val_loss: 1.3791 - val_accuracy: 0.5780\n","Epoch 235/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4025 - accuracy: 0.5435 - val_loss: 1.3666 - val_accuracy: 0.5787\n","Epoch 236/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4039 - accuracy: 0.5398 - val_loss: 1.3645 - val_accuracy: 0.5805\n","Epoch 237/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3956 - accuracy: 0.5457 - val_loss: 1.3654 - val_accuracy: 0.5808\n","Epoch 238/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4019 - accuracy: 0.5454 - val_loss: 1.3627 - val_accuracy: 0.5810\n","Epoch 239/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4036 - accuracy: 0.5411 - val_loss: 1.3778 - val_accuracy: 0.5737\n","Epoch 240/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4097 - accuracy: 0.5405 - val_loss: 1.3644 - val_accuracy: 0.5787\n","Epoch 241/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4052 - accuracy: 0.5471 - val_loss: 1.3725 - val_accuracy: 0.5755\n","Epoch 242/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4070 - accuracy: 0.5442 - val_loss: 1.3609 - val_accuracy: 0.5817\n","Epoch 243/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4026 - accuracy: 0.5460 - val_loss: 1.3710 - val_accuracy: 0.5774\n","Epoch 244/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4029 - accuracy: 0.5411 - val_loss: 1.3708 - val_accuracy: 0.5757\n","Epoch 245/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4046 - accuracy: 0.5438 - val_loss: 1.3743 - val_accuracy: 0.5729\n","Epoch 246/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4104 - accuracy: 0.5476 - val_loss: 1.3656 - val_accuracy: 0.5747\n","Epoch 247/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3991 - accuracy: 0.5406 - val_loss: 1.3635 - val_accuracy: 0.5785\n","Epoch 248/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3968 - accuracy: 0.5477 - val_loss: 1.3683 - val_accuracy: 0.5780\n","Epoch 249/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3997 - accuracy: 0.5455 - val_loss: 1.3712 - val_accuracy: 0.5764\n","Epoch 250/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4127 - accuracy: 0.5402 - val_loss: 1.3666 - val_accuracy: 0.5788\n","Epoch 251/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4046 - accuracy: 0.5474 - val_loss: 1.3800 - val_accuracy: 0.5714\n","Epoch 252/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4006 - accuracy: 0.5459 - val_loss: 1.3643 - val_accuracy: 0.5760\n","Epoch 253/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3977 - accuracy: 0.5486 - val_loss: 1.3744 - val_accuracy: 0.5744\n","Epoch 254/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4091 - accuracy: 0.5427 - val_loss: 1.3859 - val_accuracy: 0.5741\n","Epoch 255/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4058 - accuracy: 0.5451 - val_loss: 1.3709 - val_accuracy: 0.5793\n","Epoch 256/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4066 - accuracy: 0.5407 - val_loss: 1.3781 - val_accuracy: 0.5755\n","Epoch 257/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4060 - accuracy: 0.5431 - val_loss: 1.3811 - val_accuracy: 0.5749\n","Epoch 258/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3978 - accuracy: 0.5455 - val_loss: 1.3741 - val_accuracy: 0.5744\n","Epoch 259/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4075 - accuracy: 0.5446 - val_loss: 1.3705 - val_accuracy: 0.5763\n","Epoch 260/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3982 - accuracy: 0.5444 - val_loss: 1.3631 - val_accuracy: 0.5813\n","Epoch 261/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4058 - accuracy: 0.5425 - val_loss: 1.3667 - val_accuracy: 0.5804\n","Epoch 262/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4032 - accuracy: 0.5400 - val_loss: 1.3721 - val_accuracy: 0.5771\n","Epoch 263/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4049 - accuracy: 0.5335 - val_loss: 1.3770 - val_accuracy: 0.5778\n","Epoch 264/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4126 - accuracy: 0.5429 - val_loss: 1.3736 - val_accuracy: 0.5768\n","Epoch 265/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4050 - accuracy: 0.5445 - val_loss: 1.3679 - val_accuracy: 0.5788\n","Epoch 266/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3998 - accuracy: 0.5457 - val_loss: 1.3759 - val_accuracy: 0.5770\n","Epoch 267/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3972 - accuracy: 0.5446 - val_loss: 1.3667 - val_accuracy: 0.5787\n","Epoch 268/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4010 - accuracy: 0.5442 - val_loss: 1.3702 - val_accuracy: 0.5776\n","Epoch 269/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4096 - accuracy: 0.5440 - val_loss: 1.3745 - val_accuracy: 0.5751\n","Epoch 270/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3991 - accuracy: 0.5437 - val_loss: 1.3795 - val_accuracy: 0.5744\n","Epoch 271/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4060 - accuracy: 0.5469 - val_loss: 1.3732 - val_accuracy: 0.5793\n","Epoch 272/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4075 - accuracy: 0.5404 - val_loss: 1.3797 - val_accuracy: 0.5748\n","Epoch 273/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4093 - accuracy: 0.5405 - val_loss: 1.3685 - val_accuracy: 0.5806\n","Epoch 274/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4072 - accuracy: 0.5363 - val_loss: 1.3731 - val_accuracy: 0.5757\n","Epoch 275/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3913 - accuracy: 0.5469 - val_loss: 1.3750 - val_accuracy: 0.5748\n","Epoch 276/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4077 - accuracy: 0.5441 - val_loss: 1.3776 - val_accuracy: 0.5763\n","Epoch 277/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4095 - accuracy: 0.5384 - val_loss: 1.3671 - val_accuracy: 0.5816\n","Epoch 278/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3993 - accuracy: 0.5472 - val_loss: 1.3683 - val_accuracy: 0.5794\n","Epoch 279/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4082 - accuracy: 0.5455 - val_loss: 1.3730 - val_accuracy: 0.5789\n","Epoch 280/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4064 - accuracy: 0.5460 - val_loss: 1.3841 - val_accuracy: 0.5753\n","Epoch 281/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4083 - accuracy: 0.5460 - val_loss: 1.3807 - val_accuracy: 0.5757\n","Epoch 282/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4077 - accuracy: 0.5411 - val_loss: 1.3763 - val_accuracy: 0.5789\n","Epoch 283/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3996 - accuracy: 0.5471 - val_loss: 1.3680 - val_accuracy: 0.5806\n","Epoch 284/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4055 - accuracy: 0.5460 - val_loss: 1.3764 - val_accuracy: 0.5742\n","Epoch 285/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4002 - accuracy: 0.5429 - val_loss: 1.3688 - val_accuracy: 0.5785\n","Epoch 286/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4042 - accuracy: 0.5450 - val_loss: 1.3756 - val_accuracy: 0.5784\n","Epoch 287/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3990 - accuracy: 0.5463 - val_loss: 1.3753 - val_accuracy: 0.5785\n","Epoch 288/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3988 - accuracy: 0.5458 - val_loss: 1.3719 - val_accuracy: 0.5783\n","Epoch 289/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4107 - accuracy: 0.5420 - val_loss: 1.3841 - val_accuracy: 0.5738\n","Epoch 290/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3984 - accuracy: 0.5423 - val_loss: 1.3723 - val_accuracy: 0.5771\n","Epoch 291/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4016 - accuracy: 0.5444 - val_loss: 1.3737 - val_accuracy: 0.5766\n","Epoch 292/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4067 - accuracy: 0.5369 - val_loss: 1.3723 - val_accuracy: 0.5773\n","Epoch 293/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4052 - accuracy: 0.5430 - val_loss: 1.3781 - val_accuracy: 0.5744\n","Epoch 294/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4051 - accuracy: 0.5452 - val_loss: 1.3706 - val_accuracy: 0.5772\n","Epoch 295/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4085 - accuracy: 0.5447 - val_loss: 1.3644 - val_accuracy: 0.5789\n","Epoch 296/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4046 - accuracy: 0.5417 - val_loss: 1.3663 - val_accuracy: 0.5806\n","Epoch 297/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4019 - accuracy: 0.5456 - val_loss: 1.3805 - val_accuracy: 0.5757\n","Epoch 298/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4057 - accuracy: 0.5399 - val_loss: 1.3779 - val_accuracy: 0.5785\n","Epoch 299/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4144 - accuracy: 0.5389 - val_loss: 1.3792 - val_accuracy: 0.5754\n","Epoch 300/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3977 - accuracy: 0.5514 - val_loss: 1.3784 - val_accuracy: 0.5764\n","Epoch 301/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3947 - accuracy: 0.5450 - val_loss: 1.3659 - val_accuracy: 0.5796\n","Epoch 302/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4021 - accuracy: 0.5457 - val_loss: 1.3614 - val_accuracy: 0.5834\n","Epoch 303/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3986 - accuracy: 0.5427 - val_loss: 1.3760 - val_accuracy: 0.5777\n","Epoch 304/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3989 - accuracy: 0.5480 - val_loss: 1.3793 - val_accuracy: 0.5813\n","Epoch 305/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3971 - accuracy: 0.5459 - val_loss: 1.3689 - val_accuracy: 0.5800\n","Epoch 306/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4100 - accuracy: 0.5374 - val_loss: 1.3717 - val_accuracy: 0.5789\n","Epoch 307/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4043 - accuracy: 0.5447 - val_loss: 1.3735 - val_accuracy: 0.5785\n","Epoch 308/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3983 - accuracy: 0.5484 - val_loss: 1.3677 - val_accuracy: 0.5831\n","Epoch 309/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4051 - accuracy: 0.5455 - val_loss: 1.3865 - val_accuracy: 0.5741\n","Epoch 310/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4076 - accuracy: 0.5390 - val_loss: 1.3758 - val_accuracy: 0.5773\n","Epoch 311/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3939 - accuracy: 0.5458 - val_loss: 1.3760 - val_accuracy: 0.5796\n","Epoch 312/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4033 - accuracy: 0.5410 - val_loss: 1.3731 - val_accuracy: 0.5817\n","Epoch 313/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4035 - accuracy: 0.5459 - val_loss: 1.3706 - val_accuracy: 0.5792\n","Epoch 314/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3985 - accuracy: 0.5482 - val_loss: 1.3731 - val_accuracy: 0.5792\n","Epoch 315/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4035 - accuracy: 0.5407 - val_loss: 1.3751 - val_accuracy: 0.5805\n","Epoch 316/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4045 - accuracy: 0.5444 - val_loss: 1.3777 - val_accuracy: 0.5771\n","Epoch 317/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3998 - accuracy: 0.5426 - val_loss: 1.3762 - val_accuracy: 0.5764\n","Epoch 318/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3989 - accuracy: 0.5463 - val_loss: 1.3796 - val_accuracy: 0.5756\n","Epoch 319/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3991 - accuracy: 0.5463 - val_loss: 1.3705 - val_accuracy: 0.5771\n","Epoch 320/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3979 - accuracy: 0.5455 - val_loss: 1.3798 - val_accuracy: 0.5746\n","Epoch 321/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3939 - accuracy: 0.5467 - val_loss: 1.3820 - val_accuracy: 0.5737\n","Epoch 322/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3972 - accuracy: 0.5456 - val_loss: 1.3776 - val_accuracy: 0.5760\n","Epoch 323/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4011 - accuracy: 0.5449 - val_loss: 1.3786 - val_accuracy: 0.5749\n","Epoch 324/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4006 - accuracy: 0.5418 - val_loss: 1.3683 - val_accuracy: 0.5784\n","Epoch 325/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4010 - accuracy: 0.5469 - val_loss: 1.3656 - val_accuracy: 0.5813\n","Epoch 326/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4049 - accuracy: 0.5386 - val_loss: 1.3662 - val_accuracy: 0.5805\n","Epoch 327/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4028 - accuracy: 0.5437 - val_loss: 1.3662 - val_accuracy: 0.5818\n","Epoch 328/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4038 - accuracy: 0.5489 - val_loss: 1.3791 - val_accuracy: 0.5761\n","Epoch 329/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3978 - accuracy: 0.5411 - val_loss: 1.3696 - val_accuracy: 0.5781\n","Epoch 330/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3959 - accuracy: 0.5468 - val_loss: 1.3858 - val_accuracy: 0.5744\n","Epoch 331/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3946 - accuracy: 0.5485 - val_loss: 1.3664 - val_accuracy: 0.5799\n","Epoch 332/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3980 - accuracy: 0.5484 - val_loss: 1.3685 - val_accuracy: 0.5789\n","Epoch 333/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3975 - accuracy: 0.5494 - val_loss: 1.3781 - val_accuracy: 0.5798\n","Epoch 334/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3981 - accuracy: 0.5483 - val_loss: 1.3663 - val_accuracy: 0.5792\n","Epoch 335/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4060 - accuracy: 0.5442 - val_loss: 1.3769 - val_accuracy: 0.5772\n","Epoch 336/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3954 - accuracy: 0.5466 - val_loss: 1.3863 - val_accuracy: 0.5763\n","Epoch 337/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3887 - accuracy: 0.5453 - val_loss: 1.3713 - val_accuracy: 0.5800\n","Epoch 338/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3875 - accuracy: 0.5455 - val_loss: 1.3674 - val_accuracy: 0.5806\n","Epoch 339/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4036 - accuracy: 0.5467 - val_loss: 1.3672 - val_accuracy: 0.5788\n","Epoch 340/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4036 - accuracy: 0.5429 - val_loss: 1.3761 - val_accuracy: 0.5790\n","Epoch 341/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4038 - accuracy: 0.5399 - val_loss: 1.3746 - val_accuracy: 0.5782\n","Epoch 342/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3972 - accuracy: 0.5470 - val_loss: 1.3639 - val_accuracy: 0.5830\n","Epoch 343/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3949 - accuracy: 0.5454 - val_loss: 1.3802 - val_accuracy: 0.5741\n","Epoch 344/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3977 - accuracy: 0.5424 - val_loss: 1.3730 - val_accuracy: 0.5756\n","Epoch 345/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4118 - accuracy: 0.5400 - val_loss: 1.3707 - val_accuracy: 0.5796\n","Epoch 346/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3878 - accuracy: 0.5476 - val_loss: 1.3753 - val_accuracy: 0.5773\n","Epoch 347/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4014 - accuracy: 0.5482 - val_loss: 1.3795 - val_accuracy: 0.5770\n","Epoch 348/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3909 - accuracy: 0.5498 - val_loss: 1.3709 - val_accuracy: 0.5766\n","Epoch 349/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4015 - accuracy: 0.5412 - val_loss: 1.3884 - val_accuracy: 0.5740\n","Epoch 350/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3969 - accuracy: 0.5449 - val_loss: 1.3854 - val_accuracy: 0.5740\n","Epoch 351/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3923 - accuracy: 0.5500 - val_loss: 1.3728 - val_accuracy: 0.5788\n","Epoch 352/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3946 - accuracy: 0.5429 - val_loss: 1.3695 - val_accuracy: 0.5777\n","Epoch 353/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4082 - accuracy: 0.5379 - val_loss: 1.3693 - val_accuracy: 0.5796\n","Epoch 354/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3908 - accuracy: 0.5472 - val_loss: 1.3709 - val_accuracy: 0.5787\n","Epoch 355/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4037 - accuracy: 0.5373 - val_loss: 1.3777 - val_accuracy: 0.5760\n","Epoch 356/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4035 - accuracy: 0.5455 - val_loss: 1.3639 - val_accuracy: 0.5812\n","Epoch 357/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4014 - accuracy: 0.5483 - val_loss: 1.3703 - val_accuracy: 0.5785\n","Epoch 358/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4046 - accuracy: 0.5472 - val_loss: 1.3732 - val_accuracy: 0.5800\n","Epoch 359/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3978 - accuracy: 0.5456 - val_loss: 1.3680 - val_accuracy: 0.5795\n","Epoch 360/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4017 - accuracy: 0.5449 - val_loss: 1.3715 - val_accuracy: 0.5802\n","Epoch 361/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4063 - accuracy: 0.5412 - val_loss: 1.3880 - val_accuracy: 0.5759\n","Epoch 362/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4067 - accuracy: 0.5431 - val_loss: 1.3631 - val_accuracy: 0.5807\n","Epoch 363/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3983 - accuracy: 0.5488 - val_loss: 1.3800 - val_accuracy: 0.5784\n","Epoch 364/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3984 - accuracy: 0.5426 - val_loss: 1.3681 - val_accuracy: 0.5819\n","Epoch 365/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3984 - accuracy: 0.5389 - val_loss: 1.3722 - val_accuracy: 0.5778\n","Epoch 366/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4019 - accuracy: 0.5458 - val_loss: 1.3688 - val_accuracy: 0.5804\n","Epoch 367/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3861 - accuracy: 0.5476 - val_loss: 1.3794 - val_accuracy: 0.5784\n","Epoch 368/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4027 - accuracy: 0.5483 - val_loss: 1.3710 - val_accuracy: 0.5806\n","Epoch 369/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3942 - accuracy: 0.5451 - val_loss: 1.3719 - val_accuracy: 0.5767\n","Epoch 370/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3921 - accuracy: 0.5534 - val_loss: 1.3537 - val_accuracy: 0.5836\n","Epoch 371/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3990 - accuracy: 0.5444 - val_loss: 1.3762 - val_accuracy: 0.5768\n","Epoch 372/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4034 - accuracy: 0.5455 - val_loss: 1.3738 - val_accuracy: 0.5785\n","Epoch 373/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3939 - accuracy: 0.5520 - val_loss: 1.3781 - val_accuracy: 0.5742\n","Epoch 374/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4079 - accuracy: 0.5468 - val_loss: 1.3736 - val_accuracy: 0.5801\n","Epoch 375/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3882 - accuracy: 0.5464 - val_loss: 1.3846 - val_accuracy: 0.5760\n","Epoch 376/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3901 - accuracy: 0.5453 - val_loss: 1.3616 - val_accuracy: 0.5810\n","Epoch 377/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4002 - accuracy: 0.5452 - val_loss: 1.3664 - val_accuracy: 0.5796\n","Epoch 378/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3955 - accuracy: 0.5455 - val_loss: 1.3682 - val_accuracy: 0.5801\n","Epoch 379/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3963 - accuracy: 0.5438 - val_loss: 1.3742 - val_accuracy: 0.5783\n","Epoch 380/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3945 - accuracy: 0.5510 - val_loss: 1.3770 - val_accuracy: 0.5761\n","Epoch 381/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4031 - accuracy: 0.5465 - val_loss: 1.3803 - val_accuracy: 0.5758\n","Epoch 382/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3959 - accuracy: 0.5465 - val_loss: 1.3663 - val_accuracy: 0.5802\n","Epoch 383/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4007 - accuracy: 0.5442 - val_loss: 1.3726 - val_accuracy: 0.5803\n","Epoch 384/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3967 - accuracy: 0.5462 - val_loss: 1.3768 - val_accuracy: 0.5787\n","Epoch 385/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4039 - accuracy: 0.5414 - val_loss: 1.3782 - val_accuracy: 0.5767\n","Epoch 386/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4044 - accuracy: 0.5430 - val_loss: 1.3719 - val_accuracy: 0.5808\n","Epoch 387/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3989 - accuracy: 0.5358 - val_loss: 1.3741 - val_accuracy: 0.5807\n","Epoch 388/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3938 - accuracy: 0.5483 - val_loss: 1.3766 - val_accuracy: 0.5789\n","Epoch 389/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4042 - accuracy: 0.5463 - val_loss: 1.3672 - val_accuracy: 0.5808\n","Epoch 390/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3887 - accuracy: 0.5437 - val_loss: 1.3731 - val_accuracy: 0.5793\n","Epoch 391/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3952 - accuracy: 0.5447 - val_loss: 1.3693 - val_accuracy: 0.5793\n","Epoch 392/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3969 - accuracy: 0.5470 - val_loss: 1.3780 - val_accuracy: 0.5777\n","Epoch 393/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4061 - accuracy: 0.5414 - val_loss: 1.3725 - val_accuracy: 0.5796\n","Epoch 394/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.4004 - accuracy: 0.5441 - val_loss: 1.3684 - val_accuracy: 0.5806\n","Epoch 395/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3846 - accuracy: 0.5454 - val_loss: 1.3673 - val_accuracy: 0.5808\n","Epoch 396/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4004 - accuracy: 0.5435 - val_loss: 1.3623 - val_accuracy: 0.5829\n","Epoch 397/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.4014 - accuracy: 0.5445 - val_loss: 1.3728 - val_accuracy: 0.5789\n","Epoch 398/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.3944 - accuracy: 0.5384 - val_loss: 1.3768 - val_accuracy: 0.5783\n","Epoch 399/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3917 - accuracy: 0.5449 - val_loss: 1.3632 - val_accuracy: 0.5810\n","Epoch 400/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.3987 - accuracy: 0.5446 - val_loss: 1.3708 - val_accuracy: 0.5787\n"]}]},{"cell_type":"code","source":["# further modifying the previous one that had the lowest validation loss\n","# clearly we need some strong regularization with this small training set\n","layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwvTXjekJK21","outputId":"a50ca0c8-f48e-40f0-94f1-3dd8b9574f43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/400\n","301/301 [==============================] - 3s 8ms/step - loss: 2.5032 - accuracy: 0.2223 - val_loss: 2.1556 - val_accuracy: 0.3351\n","Epoch 2/400\n","301/301 [==============================] - 2s 7ms/step - loss: 2.0937 - accuracy: 0.3292 - val_loss: 1.9349 - val_accuracy: 0.4224\n","Epoch 3/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.9647 - accuracy: 0.3647 - val_loss: 1.8265 - val_accuracy: 0.4503\n","Epoch 4/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.9095 - accuracy: 0.3770 - val_loss: 1.7826 - val_accuracy: 0.4521\n","Epoch 5/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8729 - accuracy: 0.3860 - val_loss: 1.7195 - val_accuracy: 0.4920\n","Epoch 6/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8538 - accuracy: 0.3957 - val_loss: 1.6985 - val_accuracy: 0.4885\n","Epoch 7/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8328 - accuracy: 0.3953 - val_loss: 1.6858 - val_accuracy: 0.4850\n","Epoch 8/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8190 - accuracy: 0.4017 - val_loss: 1.6710 - val_accuracy: 0.4944\n","Epoch 9/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8262 - accuracy: 0.3978 - val_loss: 1.6789 - val_accuracy: 0.4931\n","Epoch 10/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8194 - accuracy: 0.4024 - val_loss: 1.6789 - val_accuracy: 0.4855\n","Epoch 11/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7975 - accuracy: 0.4084 - val_loss: 1.6482 - val_accuracy: 0.4988\n","Epoch 12/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8008 - accuracy: 0.4044 - val_loss: 1.6643 - val_accuracy: 0.4984\n","Epoch 13/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8018 - accuracy: 0.4068 - val_loss: 1.6355 - val_accuracy: 0.5036\n","Epoch 14/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.8007 - accuracy: 0.4117 - val_loss: 1.6503 - val_accuracy: 0.5007\n","Epoch 15/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7902 - accuracy: 0.4067 - val_loss: 1.6497 - val_accuracy: 0.5037\n","Epoch 16/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.8038 - accuracy: 0.4021 - val_loss: 1.6409 - val_accuracy: 0.5083\n","Epoch 17/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7944 - accuracy: 0.4082 - val_loss: 1.6581 - val_accuracy: 0.4962\n","Epoch 18/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7939 - accuracy: 0.4011 - val_loss: 1.6437 - val_accuracy: 0.5012\n","Epoch 19/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7859 - accuracy: 0.4155 - val_loss: 1.6499 - val_accuracy: 0.4945\n","Epoch 20/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7912 - accuracy: 0.4036 - val_loss: 1.6445 - val_accuracy: 0.4992\n","Epoch 21/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7890 - accuracy: 0.4073 - val_loss: 1.6374 - val_accuracy: 0.5046\n","Epoch 22/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7764 - accuracy: 0.4130 - val_loss: 1.6422 - val_accuracy: 0.5018\n","Epoch 23/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7794 - accuracy: 0.4156 - val_loss: 1.6351 - val_accuracy: 0.5024\n","Epoch 24/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7754 - accuracy: 0.4173 - val_loss: 1.6382 - val_accuracy: 0.4973\n","Epoch 25/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7862 - accuracy: 0.4129 - val_loss: 1.6508 - val_accuracy: 0.4953\n","Epoch 26/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.7648 - accuracy: 0.4182 - val_loss: 1.6484 - val_accuracy: 0.4898\n","Epoch 27/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7773 - accuracy: 0.4123 - val_loss: 1.6481 - val_accuracy: 0.4979\n","Epoch 28/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7573 - accuracy: 0.4188 - val_loss: 1.6325 - val_accuracy: 0.5041\n","Epoch 29/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7784 - accuracy: 0.4119 - val_loss: 1.6340 - val_accuracy: 0.4997\n","Epoch 30/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7715 - accuracy: 0.4155 - val_loss: 1.6306 - val_accuracy: 0.5075\n","Epoch 31/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7786 - accuracy: 0.4172 - val_loss: 1.6378 - val_accuracy: 0.5037\n","Epoch 32/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7775 - accuracy: 0.4100 - val_loss: 1.6427 - val_accuracy: 0.5023\n","Epoch 33/400\n","301/301 [==============================] - 2s 8ms/step - loss: 1.7669 - accuracy: 0.4138 - val_loss: 1.6523 - val_accuracy: 0.4930\n","Epoch 34/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7740 - accuracy: 0.4137 - val_loss: 1.6404 - val_accuracy: 0.5035\n","Epoch 35/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7700 - accuracy: 0.4174 - val_loss: 1.6497 - val_accuracy: 0.4939\n","Epoch 36/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7686 - accuracy: 0.4156 - val_loss: 1.6458 - val_accuracy: 0.4975\n","Epoch 37/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7625 - accuracy: 0.4181 - val_loss: 1.6392 - val_accuracy: 0.4992\n","Epoch 38/400\n","301/301 [==============================] - 2s 7ms/step - loss: 1.7668 - accuracy: 0.4166 - val_loss: 1.6280 - val_accuracy: 0.5032\n","Epoch 39/400\n","181/301 [=================>............] - ETA: 0s - loss: 1.7584 - accuracy: 0.4240"]}]},{"cell_type":"code","source":["layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(100,kernel_initializer='lecun_normal'),\n","          \n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"CHrDlGiELAfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(100,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"hE6fi0aEMEtQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dropout(0.6),\n","          layers.Dense(400,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"TnVj9sH8LxEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# so I created a model that isn't learning the training set! let's add another layer\n","layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(10,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dropout(0.6),\n","          layers.Dense(400,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(100,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"JEZbW4aWNYTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# that didn't help\n","# might need to lighten up on the dimensionality reduction\n","# so I created a model that isn't learning the training set! let's add another layer\n","layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(40,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dropout(0.6),\n","          layers.Dense(400,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"DrF4yDtAQAo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# that didn't help\n","# might need to lighten up on the dimensionality reduction\n","# so I created a model that isn't learning the training set! let's add another layer\n","layer_list = [ #520 parameters in input\n","          layers.Dropout(0.6),\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(40,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(40,kernel_initializer='lecun_normal',kernel_regularizer='l2'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dropout(0.6),\n","          layers.Dense(400,kernel_initializer='lecun_normal'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=400, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"cjK69D0SRDkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# it appears that this one was on the right track but needed more complexity\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(8,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"id":"rNAVSj8sU4vt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652124909459,"user_tz":240,"elapsed":446915,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"5bb20795-81f8-44bc-f3e9-c56be9e6866f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 8s 14ms/step - loss: 14.7015 - accuracy: 0.3223 - val_loss: 4.2405 - val_accuracy: 0.1912\n","Epoch 2/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.7446 - accuracy: 0.3839 - val_loss: 3.6120 - val_accuracy: 0.4471\n","Epoch 3/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.5008 - accuracy: 0.4030 - val_loss: 3.3449 - val_accuracy: 0.4438\n","Epoch 4/100\n","301/301 [==============================] - 4s 15ms/step - loss: 3.2867 - accuracy: 0.4247 - val_loss: 3.2592 - val_accuracy: 0.3690\n","Epoch 5/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.0945 - accuracy: 0.4402 - val_loss: 2.9947 - val_accuracy: 0.4523\n","Epoch 6/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9719 - accuracy: 0.4503 - val_loss: 2.8123 - val_accuracy: 0.5090\n","Epoch 7/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8741 - accuracy: 0.4670 - val_loss: 2.8489 - val_accuracy: 0.4579\n","Epoch 8/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8027 - accuracy: 0.4734 - val_loss: 2.6294 - val_accuracy: 0.5291\n","Epoch 9/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.7192 - accuracy: 0.4870 - val_loss: 2.6813 - val_accuracy: 0.5199\n","Epoch 10/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.6766 - accuracy: 0.4848 - val_loss: 2.7182 - val_accuracy: 0.4809\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6246 - accuracy: 0.4935 - val_loss: 2.5371 - val_accuracy: 0.5084\n","Epoch 12/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5841 - accuracy: 0.4976 - val_loss: 2.5466 - val_accuracy: 0.5112\n","Epoch 13/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5172 - accuracy: 0.5006 - val_loss: 2.5478 - val_accuracy: 0.5030\n","Epoch 14/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4976 - accuracy: 0.5042 - val_loss: 2.4964 - val_accuracy: 0.5212\n","Epoch 15/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4784 - accuracy: 0.5141 - val_loss: 2.4298 - val_accuracy: 0.5448\n","Epoch 16/100\n","301/301 [==============================] - 4s 12ms/step - loss: 2.3957 - accuracy: 0.5115 - val_loss: 2.3447 - val_accuracy: 0.5206\n","Epoch 17/100\n","301/301 [==============================] - 4s 12ms/step - loss: 2.4016 - accuracy: 0.5168 - val_loss: 2.3563 - val_accuracy: 0.5447\n","Epoch 18/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3465 - accuracy: 0.5182 - val_loss: 2.3120 - val_accuracy: 0.5452\n","Epoch 19/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3351 - accuracy: 0.5132 - val_loss: 2.3525 - val_accuracy: 0.5315\n","Epoch 20/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3206 - accuracy: 0.5242 - val_loss: 2.3138 - val_accuracy: 0.5286\n","Epoch 21/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2677 - accuracy: 0.5247 - val_loss: 2.2750 - val_accuracy: 0.5390\n","Epoch 22/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.2790 - accuracy: 0.5207 - val_loss: 2.2937 - val_accuracy: 0.5225\n","Epoch 23/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2309 - accuracy: 0.5295 - val_loss: 2.1922 - val_accuracy: 0.5370\n","Epoch 24/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2113 - accuracy: 0.5262 - val_loss: 2.2174 - val_accuracy: 0.5422\n","Epoch 25/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2233 - accuracy: 0.5268 - val_loss: 2.2286 - val_accuracy: 0.5386\n","Epoch 26/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1923 - accuracy: 0.5322 - val_loss: 2.1732 - val_accuracy: 0.5332\n","Epoch 27/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1526 - accuracy: 0.5317 - val_loss: 2.2184 - val_accuracy: 0.5239\n","Epoch 28/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1722 - accuracy: 0.5318 - val_loss: 2.1349 - val_accuracy: 0.5506\n","Epoch 29/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1375 - accuracy: 0.5359 - val_loss: 2.2737 - val_accuracy: 0.4946\n","Epoch 30/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1314 - accuracy: 0.5318 - val_loss: 2.1537 - val_accuracy: 0.5460\n","Epoch 31/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1194 - accuracy: 0.5356 - val_loss: 2.1129 - val_accuracy: 0.5489\n","Epoch 32/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0918 - accuracy: 0.5343 - val_loss: 2.0797 - val_accuracy: 0.5580\n","Epoch 33/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1004 - accuracy: 0.5381 - val_loss: 2.0876 - val_accuracy: 0.5432\n","Epoch 34/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0681 - accuracy: 0.5398 - val_loss: 2.1472 - val_accuracy: 0.5170\n","Epoch 35/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0734 - accuracy: 0.5352 - val_loss: 2.1043 - val_accuracy: 0.5354\n","Epoch 36/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.0573 - accuracy: 0.5416 - val_loss: 2.0808 - val_accuracy: 0.5383\n","Epoch 37/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0479 - accuracy: 0.5384 - val_loss: 2.0341 - val_accuracy: 0.5589\n","Epoch 38/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0613 - accuracy: 0.5354 - val_loss: 2.0297 - val_accuracy: 0.5596\n","Epoch 39/100\n","301/301 [==============================] - 5s 17ms/step - loss: 2.0680 - accuracy: 0.5342 - val_loss: 2.0042 - val_accuracy: 0.5755\n","Epoch 40/100\n","301/301 [==============================] - 5s 17ms/step - loss: 2.0482 - accuracy: 0.5400 - val_loss: 2.0777 - val_accuracy: 0.5426\n","Epoch 41/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.0692 - accuracy: 0.5386 - val_loss: 2.0586 - val_accuracy: 0.5483\n","Epoch 42/100\n","301/301 [==============================] - 5s 17ms/step - loss: 2.0368 - accuracy: 0.5353 - val_loss: 2.0672 - val_accuracy: 0.5302\n","Epoch 43/100\n","301/301 [==============================] - 5s 16ms/step - loss: 2.0299 - accuracy: 0.5404 - val_loss: 1.9922 - val_accuracy: 0.5709\n","Epoch 44/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0166 - accuracy: 0.5437 - val_loss: 2.0731 - val_accuracy: 0.5403\n","Epoch 45/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0195 - accuracy: 0.5418 - val_loss: 2.0558 - val_accuracy: 0.5406\n","Epoch 46/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0081 - accuracy: 0.5479 - val_loss: 2.0111 - val_accuracy: 0.5548\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0153 - accuracy: 0.5407 - val_loss: 2.0311 - val_accuracy: 0.5631\n","Epoch 48/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9942 - accuracy: 0.5455 - val_loss: 1.9270 - val_accuracy: 0.5757\n","Epoch 49/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9754 - accuracy: 0.5479 - val_loss: 1.9662 - val_accuracy: 0.5507\n","Epoch 50/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9906 - accuracy: 0.5445 - val_loss: 2.0113 - val_accuracy: 0.5654\n","Epoch 51/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9951 - accuracy: 0.5413 - val_loss: 1.9964 - val_accuracy: 0.5611\n","Epoch 52/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9974 - accuracy: 0.5482 - val_loss: 2.0860 - val_accuracy: 0.5115\n","Epoch 53/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9885 - accuracy: 0.5458 - val_loss: 1.9844 - val_accuracy: 0.5592\n","Epoch 54/100\n","301/301 [==============================] - 5s 16ms/step - loss: 1.9723 - accuracy: 0.5437 - val_loss: 1.9771 - val_accuracy: 0.5601\n","Epoch 55/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9667 - accuracy: 0.5411 - val_loss: 1.9721 - val_accuracy: 0.5530\n","Epoch 56/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9731 - accuracy: 0.5486 - val_loss: 2.0488 - val_accuracy: 0.5255\n","Epoch 57/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9896 - accuracy: 0.5418 - val_loss: 1.9906 - val_accuracy: 0.5559\n","Epoch 58/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9852 - accuracy: 0.5488 - val_loss: 1.9564 - val_accuracy: 0.5649\n","Epoch 59/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9642 - accuracy: 0.5495 - val_loss: 1.9992 - val_accuracy: 0.5400\n","Epoch 60/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9570 - accuracy: 0.5425 - val_loss: 1.9629 - val_accuracy: 0.5491\n","Epoch 61/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9533 - accuracy: 0.5481 - val_loss: 1.9756 - val_accuracy: 0.5573\n","Epoch 62/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9731 - accuracy: 0.5499 - val_loss: 1.9480 - val_accuracy: 0.5654\n","Epoch 63/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9567 - accuracy: 0.5458 - val_loss: 1.9753 - val_accuracy: 0.5630\n","Epoch 64/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9625 - accuracy: 0.5498 - val_loss: 1.9206 - val_accuracy: 0.5772\n","Epoch 65/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9692 - accuracy: 0.5444 - val_loss: 1.9680 - val_accuracy: 0.5671\n","Epoch 66/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9443 - accuracy: 0.5496 - val_loss: 1.9578 - val_accuracy: 0.5582\n","Epoch 67/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9557 - accuracy: 0.5456 - val_loss: 1.9337 - val_accuracy: 0.5731\n","Epoch 68/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9306 - accuracy: 0.5476 - val_loss: 1.9748 - val_accuracy: 0.5554\n","Epoch 69/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9485 - accuracy: 0.5503 - val_loss: 1.9987 - val_accuracy: 0.5517\n","Epoch 70/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9568 - accuracy: 0.5441 - val_loss: 1.9360 - val_accuracy: 0.5683\n","Epoch 71/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9461 - accuracy: 0.5510 - val_loss: 1.9704 - val_accuracy: 0.5509\n","Epoch 72/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9293 - accuracy: 0.5546 - val_loss: 1.9324 - val_accuracy: 0.5633\n","Epoch 73/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9385 - accuracy: 0.5480 - val_loss: 1.9879 - val_accuracy: 0.5418\n","Epoch 74/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9392 - accuracy: 0.5502 - val_loss: 1.9825 - val_accuracy: 0.5431\n","Epoch 75/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9333 - accuracy: 0.5517 - val_loss: 1.9314 - val_accuracy: 0.5646\n","Epoch 76/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9254 - accuracy: 0.5467 - val_loss: 1.8903 - val_accuracy: 0.5701\n","Epoch 77/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9194 - accuracy: 0.5512 - val_loss: 1.9928 - val_accuracy: 0.5332\n","Epoch 78/100\n","301/301 [==============================] - 5s 16ms/step - loss: 1.9371 - accuracy: 0.5552 - val_loss: 1.9239 - val_accuracy: 0.5802\n","Epoch 79/100\n","301/301 [==============================] - 5s 16ms/step - loss: 1.9317 - accuracy: 0.5527 - val_loss: 1.9564 - val_accuracy: 0.5595\n","Epoch 80/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9075 - accuracy: 0.5536 - val_loss: 1.9815 - val_accuracy: 0.5326\n","Epoch 81/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9256 - accuracy: 0.5503 - val_loss: 1.9474 - val_accuracy: 0.5586\n","Epoch 82/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9160 - accuracy: 0.5518 - val_loss: 1.9959 - val_accuracy: 0.5360\n","Epoch 83/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9185 - accuracy: 0.5613 - val_loss: 1.8978 - val_accuracy: 0.5733\n","Epoch 84/100\n","301/301 [==============================] - 5s 17ms/step - loss: 1.9208 - accuracy: 0.5535 - val_loss: 1.9330 - val_accuracy: 0.5559\n","Epoch 85/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9291 - accuracy: 0.5502 - val_loss: 1.8934 - val_accuracy: 0.5677\n","Epoch 86/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9121 - accuracy: 0.5559 - val_loss: 1.9944 - val_accuracy: 0.5475\n","Epoch 87/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9325 - accuracy: 0.5506 - val_loss: 1.8993 - val_accuracy: 0.5792\n","Epoch 88/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9317 - accuracy: 0.5545 - val_loss: 1.8935 - val_accuracy: 0.5683\n","Epoch 89/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9077 - accuracy: 0.5550 - val_loss: 1.9613 - val_accuracy: 0.5403\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9285 - accuracy: 0.5510 - val_loss: 1.9492 - val_accuracy: 0.5463\n","Epoch 91/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9101 - accuracy: 0.5501 - val_loss: 1.9329 - val_accuracy: 0.5556\n","Epoch 92/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9005 - accuracy: 0.5496 - val_loss: 1.9246 - val_accuracy: 0.5589\n","Epoch 93/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9333 - accuracy: 0.5527 - val_loss: 1.9071 - val_accuracy: 0.5788\n","Epoch 94/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9162 - accuracy: 0.5522 - val_loss: 1.9562 - val_accuracy: 0.5452\n","Epoch 95/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9128 - accuracy: 0.5559 - val_loss: 1.9559 - val_accuracy: 0.5573\n","Epoch 96/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9084 - accuracy: 0.5543 - val_loss: 1.9612 - val_accuracy: 0.5651\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9127 - accuracy: 0.5548 - val_loss: 1.9341 - val_accuracy: 0.5621\n","Epoch 98/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9064 - accuracy: 0.5534 - val_loss: 1.9370 - val_accuracy: 0.5533\n","Epoch 99/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9022 - accuracy: 0.5537 - val_loss: 1.9457 - val_accuracy: 0.5600\n","Epoch 100/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9055 - accuracy: 0.5535 - val_loss: 1.9461 - val_accuracy: 0.5450\n"]}]},{"cell_type":"code","source":["# adding the extra layer after dimensionality reduction might not have accomplished much?\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(8,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(160,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFmRbHHHTS_7","executionInfo":{"status":"ok","timestamp":1652125357714,"user_tz":240,"elapsed":443910,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"aac58d41-10fd-4923-96fa-9527899451a8"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 7s 17ms/step - loss: 14.4475 - accuracy: 0.3763 - val_loss: 4.4700 - val_accuracy: 0.0651\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.7216 - accuracy: 0.3944 - val_loss: 3.4773 - val_accuracy: 0.4336\n","Epoch 3/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.5307 - accuracy: 0.4162 - val_loss: 3.5221 - val_accuracy: 0.3458\n","Epoch 4/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.3035 - accuracy: 0.4422 - val_loss: 3.2213 - val_accuracy: 0.4779\n","Epoch 5/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.1065 - accuracy: 0.4524 - val_loss: 3.0775 - val_accuracy: 0.4522\n","Epoch 6/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.9652 - accuracy: 0.4682 - val_loss: 2.8675 - val_accuracy: 0.5027\n","Epoch 7/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.8877 - accuracy: 0.4798 - val_loss: 2.7538 - val_accuracy: 0.5092\n","Epoch 8/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7985 - accuracy: 0.4867 - val_loss: 2.8052 - val_accuracy: 0.4775\n","Epoch 9/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.7279 - accuracy: 0.4953 - val_loss: 2.7209 - val_accuracy: 0.4689\n","Epoch 10/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.7053 - accuracy: 0.4935 - val_loss: 2.8036 - val_accuracy: 0.4498\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6262 - accuracy: 0.4967 - val_loss: 2.4837 - val_accuracy: 0.5410\n","Epoch 12/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5604 - accuracy: 0.5046 - val_loss: 2.4795 - val_accuracy: 0.5538\n","Epoch 13/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5310 - accuracy: 0.5051 - val_loss: 2.5118 - val_accuracy: 0.5316\n","Epoch 14/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4802 - accuracy: 0.5109 - val_loss: 2.6060 - val_accuracy: 0.4905\n","Epoch 15/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4213 - accuracy: 0.5135 - val_loss: 2.5674 - val_accuracy: 0.4528\n","Epoch 16/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4122 - accuracy: 0.5176 - val_loss: 2.3512 - val_accuracy: 0.5523\n","Epoch 17/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3573 - accuracy: 0.5210 - val_loss: 2.3681 - val_accuracy: 0.5289\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3632 - accuracy: 0.5222 - val_loss: 2.3281 - val_accuracy: 0.5318\n","Epoch 19/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3189 - accuracy: 0.5229 - val_loss: 2.2924 - val_accuracy: 0.5388\n","Epoch 20/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3134 - accuracy: 0.5223 - val_loss: 2.3420 - val_accuracy: 0.5338\n","Epoch 21/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2888 - accuracy: 0.5229 - val_loss: 2.2480 - val_accuracy: 0.5364\n","Epoch 22/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2535 - accuracy: 0.5223 - val_loss: 2.2133 - val_accuracy: 0.5529\n","Epoch 23/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2434 - accuracy: 0.5286 - val_loss: 2.1561 - val_accuracy: 0.5578\n","Epoch 24/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2118 - accuracy: 0.5324 - val_loss: 2.1633 - val_accuracy: 0.5465\n","Epoch 25/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1679 - accuracy: 0.5318 - val_loss: 2.1554 - val_accuracy: 0.5437\n","Epoch 26/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1968 - accuracy: 0.5331 - val_loss: 2.1363 - val_accuracy: 0.5584\n","Epoch 27/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1557 - accuracy: 0.5338 - val_loss: 2.1233 - val_accuracy: 0.5306\n","Epoch 28/100\n","301/301 [==============================] - 5s 16ms/step - loss: 2.1437 - accuracy: 0.5376 - val_loss: 2.1674 - val_accuracy: 0.5448\n","Epoch 29/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1528 - accuracy: 0.5400 - val_loss: 2.1626 - val_accuracy: 0.5288\n","Epoch 30/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1184 - accuracy: 0.5391 - val_loss: 2.1173 - val_accuracy: 0.5449\n","Epoch 31/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1111 - accuracy: 0.5327 - val_loss: 2.1214 - val_accuracy: 0.5420\n","Epoch 32/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1102 - accuracy: 0.5380 - val_loss: 2.1258 - val_accuracy: 0.5435\n","Epoch 33/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1010 - accuracy: 0.5399 - val_loss: 2.1125 - val_accuracy: 0.5405\n","Epoch 34/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0725 - accuracy: 0.5392 - val_loss: 2.0690 - val_accuracy: 0.5361\n","Epoch 35/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0831 - accuracy: 0.5444 - val_loss: 2.0425 - val_accuracy: 0.5568\n","Epoch 36/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0587 - accuracy: 0.5419 - val_loss: 2.0014 - val_accuracy: 0.5712\n","Epoch 37/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0514 - accuracy: 0.5366 - val_loss: 1.9987 - val_accuracy: 0.5781\n","Epoch 38/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0383 - accuracy: 0.5401 - val_loss: 2.0050 - val_accuracy: 0.5659\n","Epoch 39/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0288 - accuracy: 0.5460 - val_loss: 1.9792 - val_accuracy: 0.5593\n","Epoch 40/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0256 - accuracy: 0.5412 - val_loss: 1.9914 - val_accuracy: 0.5615\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0101 - accuracy: 0.5428 - val_loss: 2.0396 - val_accuracy: 0.5452\n","Epoch 42/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0129 - accuracy: 0.5471 - val_loss: 2.1232 - val_accuracy: 0.5136\n","Epoch 43/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0097 - accuracy: 0.5426 - val_loss: 2.0407 - val_accuracy: 0.5260\n","Epoch 44/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0043 - accuracy: 0.5467 - val_loss: 1.9624 - val_accuracy: 0.5660\n","Epoch 45/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9881 - accuracy: 0.5521 - val_loss: 1.9710 - val_accuracy: 0.5758\n","Epoch 46/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9996 - accuracy: 0.5465 - val_loss: 2.0755 - val_accuracy: 0.5230\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9773 - accuracy: 0.5469 - val_loss: 1.9867 - val_accuracy: 0.5564\n","Epoch 48/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9803 - accuracy: 0.5484 - val_loss: 1.9839 - val_accuracy: 0.5573\n","Epoch 49/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9978 - accuracy: 0.5427 - val_loss: 1.9767 - val_accuracy: 0.5577\n","Epoch 50/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9869 - accuracy: 0.5449 - val_loss: 1.9662 - val_accuracy: 0.5644\n","Epoch 51/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9729 - accuracy: 0.5482 - val_loss: 1.9199 - val_accuracy: 0.5815\n","Epoch 52/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9632 - accuracy: 0.5505 - val_loss: 2.0613 - val_accuracy: 0.5130\n","Epoch 53/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9620 - accuracy: 0.5479 - val_loss: 1.9004 - val_accuracy: 0.5752\n","Epoch 54/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9483 - accuracy: 0.5438 - val_loss: 2.0339 - val_accuracy: 0.5184\n","Epoch 55/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9623 - accuracy: 0.5489 - val_loss: 2.0196 - val_accuracy: 0.5346\n","Epoch 56/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9371 - accuracy: 0.5482 - val_loss: 1.9861 - val_accuracy: 0.5460\n","Epoch 57/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9468 - accuracy: 0.5465 - val_loss: 1.9223 - val_accuracy: 0.5561\n","Epoch 58/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9292 - accuracy: 0.5505 - val_loss: 1.9829 - val_accuracy: 0.5457\n","Epoch 59/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9401 - accuracy: 0.5524 - val_loss: 1.9327 - val_accuracy: 0.5620\n","Epoch 60/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9380 - accuracy: 0.5562 - val_loss: 1.9998 - val_accuracy: 0.5420\n","Epoch 61/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9394 - accuracy: 0.5508 - val_loss: 1.9872 - val_accuracy: 0.5389\n","Epoch 62/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9195 - accuracy: 0.5559 - val_loss: 1.9945 - val_accuracy: 0.5384\n","Epoch 63/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9288 - accuracy: 0.5518 - val_loss: 2.0547 - val_accuracy: 0.5188\n","Epoch 64/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9220 - accuracy: 0.5505 - val_loss: 1.9897 - val_accuracy: 0.5328\n","Epoch 65/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9183 - accuracy: 0.5511 - val_loss: 1.9618 - val_accuracy: 0.5503\n","Epoch 66/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9365 - accuracy: 0.5497 - val_loss: 1.9318 - val_accuracy: 0.5648\n","Epoch 67/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9284 - accuracy: 0.5547 - val_loss: 1.9308 - val_accuracy: 0.5664\n","Epoch 68/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9335 - accuracy: 0.5535 - val_loss: 1.9237 - val_accuracy: 0.5723\n","Epoch 69/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9196 - accuracy: 0.5563 - val_loss: 1.9418 - val_accuracy: 0.5616\n","Epoch 70/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9137 - accuracy: 0.5560 - val_loss: 1.9452 - val_accuracy: 0.5489\n","Epoch 71/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9035 - accuracy: 0.5581 - val_loss: 1.9193 - val_accuracy: 0.5543\n","Epoch 72/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9087 - accuracy: 0.5528 - val_loss: 1.8746 - val_accuracy: 0.5812\n","Epoch 73/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9033 - accuracy: 0.5619 - val_loss: 1.9002 - val_accuracy: 0.5622\n","Epoch 74/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9212 - accuracy: 0.5540 - val_loss: 1.9296 - val_accuracy: 0.5621\n","Epoch 75/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9010 - accuracy: 0.5557 - val_loss: 1.9022 - val_accuracy: 0.5726\n","Epoch 76/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9043 - accuracy: 0.5521 - val_loss: 1.8669 - val_accuracy: 0.5786\n","Epoch 77/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9065 - accuracy: 0.5560 - val_loss: 1.9173 - val_accuracy: 0.5621\n","Epoch 78/100\n","301/301 [==============================] - 5s 16ms/step - loss: 1.9035 - accuracy: 0.5611 - val_loss: 1.9857 - val_accuracy: 0.5309\n","Epoch 79/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8931 - accuracy: 0.5572 - val_loss: 1.9480 - val_accuracy: 0.5419\n","Epoch 80/100\n","301/301 [==============================] - 5s 16ms/step - loss: 1.8784 - accuracy: 0.5604 - val_loss: 1.9813 - val_accuracy: 0.5307\n","Epoch 81/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8831 - accuracy: 0.5547 - val_loss: 1.8989 - val_accuracy: 0.5582\n","Epoch 82/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8655 - accuracy: 0.5587 - val_loss: 1.9317 - val_accuracy: 0.5417\n","Epoch 83/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8690 - accuracy: 0.5557 - val_loss: 1.8744 - val_accuracy: 0.5688\n","Epoch 84/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8574 - accuracy: 0.5580 - val_loss: 1.8858 - val_accuracy: 0.5637\n","Epoch 85/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8474 - accuracy: 0.5632 - val_loss: 1.8422 - val_accuracy: 0.5712\n","Epoch 86/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8715 - accuracy: 0.5579 - val_loss: 1.8618 - val_accuracy: 0.5734\n","Epoch 87/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8744 - accuracy: 0.5565 - val_loss: 1.8787 - val_accuracy: 0.5654\n","Epoch 88/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8753 - accuracy: 0.5540 - val_loss: 1.9524 - val_accuracy: 0.5231\n","Epoch 89/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8617 - accuracy: 0.5614 - val_loss: 1.9494 - val_accuracy: 0.5288\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8694 - accuracy: 0.5595 - val_loss: 1.8276 - val_accuracy: 0.5845\n","Epoch 91/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8568 - accuracy: 0.5607 - val_loss: 1.8724 - val_accuracy: 0.5611\n","Epoch 92/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8691 - accuracy: 0.5573 - val_loss: 1.9194 - val_accuracy: 0.5477\n","Epoch 93/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8681 - accuracy: 0.5651 - val_loss: 1.9562 - val_accuracy: 0.5367\n","Epoch 94/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8548 - accuracy: 0.5613 - val_loss: 1.8881 - val_accuracy: 0.5570\n","Epoch 95/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8721 - accuracy: 0.5605 - val_loss: 1.9212 - val_accuracy: 0.5416\n","Epoch 96/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8661 - accuracy: 0.5637 - val_loss: 1.9101 - val_accuracy: 0.5625\n","Epoch 97/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8512 - accuracy: 0.5589 - val_loss: 1.8604 - val_accuracy: 0.5772\n","Epoch 98/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8682 - accuracy: 0.5588 - val_loss: 1.8794 - val_accuracy: 0.5658\n","Epoch 99/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8422 - accuracy: 0.5643 - val_loss: 1.8605 - val_accuracy: 0.5686\n","Epoch 100/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8597 - accuracy: 0.5556 - val_loss: 1.8851 - val_accuracy: 0.5628\n"]}]},{"cell_type":"code","source":["# that probably wasn't any better. going to relax dimensionality reduction\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(160,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zey2n9efWKON","executionInfo":{"status":"ok","timestamp":1652125847963,"user_tz":240,"elapsed":443669,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"9b438bf2-d204-47bb-9576-6308b8cd81b2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 5s 14ms/step - loss: 15.0015 - accuracy: 0.3835 - val_loss: 4.9873 - val_accuracy: 0.0599\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.9656 - accuracy: 0.4005 - val_loss: 3.9194 - val_accuracy: 0.3756\n","Epoch 3/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.7555 - accuracy: 0.4267 - val_loss: 3.6284 - val_accuracy: 0.4731\n","Epoch 4/100\n","301/301 [==============================] - 4s 12ms/step - loss: 3.5494 - accuracy: 0.4404 - val_loss: 3.4915 - val_accuracy: 0.4496\n","Epoch 5/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.3602 - accuracy: 0.4560 - val_loss: 3.2182 - val_accuracy: 0.4732\n","Epoch 6/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.2066 - accuracy: 0.4693 - val_loss: 3.0821 - val_accuracy: 0.4927\n","Epoch 7/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.0942 - accuracy: 0.4768 - val_loss: 3.0514 - val_accuracy: 0.5118\n","Epoch 8/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.9696 - accuracy: 0.4883 - val_loss: 2.9452 - val_accuracy: 0.4984\n","Epoch 9/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8666 - accuracy: 0.4951 - val_loss: 2.7946 - val_accuracy: 0.5280\n","Epoch 10/100\n","301/301 [==============================] - 4s 12ms/step - loss: 2.7927 - accuracy: 0.5028 - val_loss: 2.7092 - val_accuracy: 0.5254\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7329 - accuracy: 0.4994 - val_loss: 2.7380 - val_accuracy: 0.5096\n","Epoch 12/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6751 - accuracy: 0.5104 - val_loss: 2.6067 - val_accuracy: 0.5357\n","Epoch 13/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.6320 - accuracy: 0.5106 - val_loss: 2.4857 - val_accuracy: 0.5726\n","Epoch 14/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.5450 - accuracy: 0.5227 - val_loss: 2.5380 - val_accuracy: 0.5445\n","Epoch 15/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5202 - accuracy: 0.5251 - val_loss: 2.4957 - val_accuracy: 0.5393\n","Epoch 16/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5050 - accuracy: 0.5284 - val_loss: 2.4527 - val_accuracy: 0.5502\n","Epoch 17/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4410 - accuracy: 0.5321 - val_loss: 2.4617 - val_accuracy: 0.5181\n","Epoch 18/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4100 - accuracy: 0.5264 - val_loss: 2.3860 - val_accuracy: 0.5397\n","Epoch 19/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3617 - accuracy: 0.5300 - val_loss: 2.4296 - val_accuracy: 0.5298\n","Epoch 20/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3578 - accuracy: 0.5348 - val_loss: 2.3720 - val_accuracy: 0.5526\n","Epoch 21/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3257 - accuracy: 0.5388 - val_loss: 2.2193 - val_accuracy: 0.5668\n","Epoch 22/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2725 - accuracy: 0.5409 - val_loss: 2.3609 - val_accuracy: 0.5040\n","Epoch 23/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2544 - accuracy: 0.5375 - val_loss: 2.3137 - val_accuracy: 0.5370\n","Epoch 24/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2238 - accuracy: 0.5426 - val_loss: 2.2241 - val_accuracy: 0.5513\n","Epoch 25/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2132 - accuracy: 0.5477 - val_loss: 2.2400 - val_accuracy: 0.5491\n","Epoch 26/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.1828 - accuracy: 0.5488 - val_loss: 2.2389 - val_accuracy: 0.5249\n","Epoch 27/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1832 - accuracy: 0.5491 - val_loss: 2.1576 - val_accuracy: 0.5421\n","Epoch 28/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1579 - accuracy: 0.5486 - val_loss: 2.1957 - val_accuracy: 0.5178\n","Epoch 29/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1467 - accuracy: 0.5495 - val_loss: 2.1642 - val_accuracy: 0.5665\n","Epoch 30/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1254 - accuracy: 0.5553 - val_loss: 2.1151 - val_accuracy: 0.5652\n","Epoch 31/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0993 - accuracy: 0.5545 - val_loss: 2.0926 - val_accuracy: 0.5656\n","Epoch 32/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0938 - accuracy: 0.5518 - val_loss: 2.0962 - val_accuracy: 0.5667\n","Epoch 33/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0905 - accuracy: 0.5497 - val_loss: 2.1583 - val_accuracy: 0.5374\n","Epoch 34/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0756 - accuracy: 0.5589 - val_loss: 2.0909 - val_accuracy: 0.5734\n","Epoch 35/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0673 - accuracy: 0.5594 - val_loss: 2.0983 - val_accuracy: 0.5507\n","Epoch 36/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0697 - accuracy: 0.5533 - val_loss: 2.1045 - val_accuracy: 0.5545\n","Epoch 37/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0312 - accuracy: 0.5601 - val_loss: 2.1317 - val_accuracy: 0.5330\n","Epoch 38/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0350 - accuracy: 0.5619 - val_loss: 2.0847 - val_accuracy: 0.5448\n","Epoch 39/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0304 - accuracy: 0.5598 - val_loss: 2.0337 - val_accuracy: 0.5644\n","Epoch 40/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0094 - accuracy: 0.5662 - val_loss: 2.0409 - val_accuracy: 0.5576\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0062 - accuracy: 0.5646 - val_loss: 2.0467 - val_accuracy: 0.5593\n","Epoch 42/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0035 - accuracy: 0.5624 - val_loss: 2.0987 - val_accuracy: 0.5314\n","Epoch 43/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0046 - accuracy: 0.5644 - val_loss: 2.0455 - val_accuracy: 0.5602\n","Epoch 44/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9911 - accuracy: 0.5634 - val_loss: 1.9906 - val_accuracy: 0.5695\n","Epoch 45/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9856 - accuracy: 0.5628 - val_loss: 2.1168 - val_accuracy: 0.5339\n","Epoch 46/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9680 - accuracy: 0.5677 - val_loss: 2.0961 - val_accuracy: 0.5310\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9776 - accuracy: 0.5638 - val_loss: 2.0581 - val_accuracy: 0.5358\n","Epoch 48/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9647 - accuracy: 0.5658 - val_loss: 2.0706 - val_accuracy: 0.5315\n","Epoch 49/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9563 - accuracy: 0.5639 - val_loss: 1.9792 - val_accuracy: 0.5679\n","Epoch 50/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9356 - accuracy: 0.5662 - val_loss: 1.9933 - val_accuracy: 0.5572\n","Epoch 51/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9342 - accuracy: 0.5746 - val_loss: 2.0963 - val_accuracy: 0.5199\n","Epoch 52/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9410 - accuracy: 0.5627 - val_loss: 2.0450 - val_accuracy: 0.5434\n","Epoch 53/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9412 - accuracy: 0.5620 - val_loss: 1.9969 - val_accuracy: 0.5663\n","Epoch 54/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9270 - accuracy: 0.5743 - val_loss: 1.9921 - val_accuracy: 0.5660\n","Epoch 55/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9167 - accuracy: 0.5735 - val_loss: 1.9612 - val_accuracy: 0.5738\n","Epoch 56/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9035 - accuracy: 0.5730 - val_loss: 1.9908 - val_accuracy: 0.5579\n","Epoch 57/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9252 - accuracy: 0.5754 - val_loss: 2.0552 - val_accuracy: 0.5479\n","Epoch 58/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8995 - accuracy: 0.5781 - val_loss: 2.0562 - val_accuracy: 0.5198\n","Epoch 59/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8899 - accuracy: 0.5734 - val_loss: 2.0458 - val_accuracy: 0.5321\n","Epoch 60/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9233 - accuracy: 0.5646 - val_loss: 2.0444 - val_accuracy: 0.5372\n","Epoch 61/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9066 - accuracy: 0.5761 - val_loss: 2.0330 - val_accuracy: 0.5492\n","Epoch 62/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9058 - accuracy: 0.5707 - val_loss: 1.9851 - val_accuracy: 0.5593\n","Epoch 63/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8993 - accuracy: 0.5782 - val_loss: 1.9871 - val_accuracy: 0.5610\n","Epoch 64/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8766 - accuracy: 0.5769 - val_loss: 1.9558 - val_accuracy: 0.5640\n","Epoch 65/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8855 - accuracy: 0.5749 - val_loss: 1.9509 - val_accuracy: 0.5738\n","Epoch 66/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8854 - accuracy: 0.5853 - val_loss: 2.0349 - val_accuracy: 0.5371\n","Epoch 67/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8896 - accuracy: 0.5774 - val_loss: 1.9600 - val_accuracy: 0.5607\n","Epoch 68/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8765 - accuracy: 0.5790 - val_loss: 2.0547 - val_accuracy: 0.5367\n","Epoch 69/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8763 - accuracy: 0.5744 - val_loss: 1.9981 - val_accuracy: 0.5470\n","Epoch 70/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8743 - accuracy: 0.5726 - val_loss: 1.9344 - val_accuracy: 0.5616\n","Epoch 71/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8431 - accuracy: 0.5831 - val_loss: 1.8953 - val_accuracy: 0.5725\n","Epoch 72/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8720 - accuracy: 0.5757 - val_loss: 2.0462 - val_accuracy: 0.5157\n","Epoch 73/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8625 - accuracy: 0.5809 - val_loss: 1.9618 - val_accuracy: 0.5465\n","Epoch 74/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8646 - accuracy: 0.5769 - val_loss: 1.9678 - val_accuracy: 0.5611\n","Epoch 75/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8645 - accuracy: 0.5835 - val_loss: 2.0458 - val_accuracy: 0.5383\n","Epoch 76/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8657 - accuracy: 0.5842 - val_loss: 1.9814 - val_accuracy: 0.5434\n","Epoch 77/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8454 - accuracy: 0.5809 - val_loss: 1.8684 - val_accuracy: 0.5885\n","Epoch 78/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8392 - accuracy: 0.5879 - val_loss: 2.0051 - val_accuracy: 0.5347\n","Epoch 79/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8458 - accuracy: 0.5817 - val_loss: 1.9554 - val_accuracy: 0.5565\n","Epoch 80/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8522 - accuracy: 0.5878 - val_loss: 1.9362 - val_accuracy: 0.5697\n","Epoch 81/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8325 - accuracy: 0.5852 - val_loss: 1.8724 - val_accuracy: 0.5810\n","Epoch 82/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8297 - accuracy: 0.5856 - val_loss: 1.9694 - val_accuracy: 0.5418\n","Epoch 83/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8272 - accuracy: 0.5845 - val_loss: 1.9047 - val_accuracy: 0.5711\n","Epoch 84/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8367 - accuracy: 0.5877 - val_loss: 1.9960 - val_accuracy: 0.5403\n","Epoch 85/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8447 - accuracy: 0.5814 - val_loss: 1.9551 - val_accuracy: 0.5456\n","Epoch 86/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8277 - accuracy: 0.5808 - val_loss: 1.9029 - val_accuracy: 0.5699\n","Epoch 87/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8250 - accuracy: 0.5848 - val_loss: 1.9552 - val_accuracy: 0.5469\n","Epoch 88/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8178 - accuracy: 0.5840 - val_loss: 1.8720 - val_accuracy: 0.5854\n","Epoch 89/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8271 - accuracy: 0.5862 - val_loss: 1.9498 - val_accuracy: 0.5432\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8354 - accuracy: 0.5836 - val_loss: 1.9310 - val_accuracy: 0.5698\n","Epoch 91/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8320 - accuracy: 0.5825 - val_loss: 1.8978 - val_accuracy: 0.5779\n","Epoch 92/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8296 - accuracy: 0.5861 - val_loss: 1.9379 - val_accuracy: 0.5666\n","Epoch 93/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8353 - accuracy: 0.5853 - val_loss: 1.9624 - val_accuracy: 0.5382\n","Epoch 94/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8243 - accuracy: 0.5866 - val_loss: 1.9138 - val_accuracy: 0.5670\n","Epoch 95/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8197 - accuracy: 0.5889 - val_loss: 1.9256 - val_accuracy: 0.5683\n","Epoch 96/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8190 - accuracy: 0.5964 - val_loss: 2.0117 - val_accuracy: 0.5330\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8200 - accuracy: 0.5901 - val_loss: 1.9023 - val_accuracy: 0.5726\n","Epoch 98/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8126 - accuracy: 0.5870 - val_loss: 1.9205 - val_accuracy: 0.5576\n","Epoch 99/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8122 - accuracy: 0.5928 - val_loss: 1.9472 - val_accuracy: 0.5617\n","Epoch 100/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8109 - accuracy: 0.5897 - val_loss: 1.9033 - val_accuracy: 0.5647\n"]}]},{"cell_type":"code","source":["# I think we need just a little more complexity\n","# that probably wasn't any better. going to relax dimensionality reduction\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(160,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iYnOCoy6Ydrx","executionInfo":{"status":"ok","timestamp":1652126421635,"user_tz":240,"elapsed":423827,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"3cbb15ee-7d50-4975-ea22-058a682f9ad5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 6s 16ms/step - loss: 15.5771 - accuracy: 0.3910 - val_loss: 5.1314 - val_accuracy: 0.0807\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 4.1039 - accuracy: 0.4131 - val_loss: 3.9200 - val_accuracy: 0.4833\n","Epoch 3/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.8929 - accuracy: 0.4244 - val_loss: 3.8072 - val_accuracy: 0.4528\n","Epoch 4/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.7018 - accuracy: 0.4420 - val_loss: 3.4438 - val_accuracy: 0.4835\n","Epoch 5/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.5269 - accuracy: 0.4612 - val_loss: 3.3822 - val_accuracy: 0.4917\n","Epoch 6/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.4035 - accuracy: 0.4688 - val_loss: 3.4634 - val_accuracy: 0.4305\n","Epoch 7/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.2658 - accuracy: 0.4762 - val_loss: 3.2334 - val_accuracy: 0.4715\n","Epoch 8/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.1325 - accuracy: 0.4821 - val_loss: 2.9593 - val_accuracy: 0.5632\n","Epoch 9/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.0491 - accuracy: 0.4889 - val_loss: 2.9986 - val_accuracy: 0.4921\n","Epoch 10/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9659 - accuracy: 0.4948 - val_loss: 2.8998 - val_accuracy: 0.5281\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8749 - accuracy: 0.5044 - val_loss: 2.7791 - val_accuracy: 0.5429\n","Epoch 12/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7967 - accuracy: 0.5082 - val_loss: 2.7388 - val_accuracy: 0.5299\n","Epoch 13/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.7114 - accuracy: 0.5188 - val_loss: 2.6532 - val_accuracy: 0.5602\n","Epoch 14/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6640 - accuracy: 0.5203 - val_loss: 2.5761 - val_accuracy: 0.5542\n","Epoch 15/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5987 - accuracy: 0.5156 - val_loss: 2.5907 - val_accuracy: 0.5415\n","Epoch 16/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5495 - accuracy: 0.5274 - val_loss: 2.5444 - val_accuracy: 0.5281\n","Epoch 17/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5192 - accuracy: 0.5310 - val_loss: 2.5428 - val_accuracy: 0.5258\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4613 - accuracy: 0.5326 - val_loss: 2.4342 - val_accuracy: 0.5311\n","Epoch 19/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4183 - accuracy: 0.5346 - val_loss: 2.4596 - val_accuracy: 0.5181\n","Epoch 20/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4022 - accuracy: 0.5384 - val_loss: 2.4047 - val_accuracy: 0.5332\n","Epoch 21/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3363 - accuracy: 0.5428 - val_loss: 2.4046 - val_accuracy: 0.5148\n","Epoch 22/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3174 - accuracy: 0.5465 - val_loss: 2.3636 - val_accuracy: 0.5351\n","Epoch 23/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2805 - accuracy: 0.5427 - val_loss: 2.3130 - val_accuracy: 0.5413\n","Epoch 24/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2659 - accuracy: 0.5479 - val_loss: 2.2812 - val_accuracy: 0.5592\n","Epoch 25/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2223 - accuracy: 0.5560 - val_loss: 2.1836 - val_accuracy: 0.5819\n","Epoch 26/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2270 - accuracy: 0.5516 - val_loss: 2.3380 - val_accuracy: 0.5188\n","Epoch 27/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1974 - accuracy: 0.5456 - val_loss: 2.2157 - val_accuracy: 0.5600\n","Epoch 28/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1712 - accuracy: 0.5504 - val_loss: 2.1629 - val_accuracy: 0.5786\n","Epoch 29/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1650 - accuracy: 0.5531 - val_loss: 2.2110 - val_accuracy: 0.5467\n","Epoch 30/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1545 - accuracy: 0.5585 - val_loss: 2.2082 - val_accuracy: 0.5490\n","Epoch 31/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1405 - accuracy: 0.5532 - val_loss: 2.1484 - val_accuracy: 0.5640\n","Epoch 32/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1196 - accuracy: 0.5583 - val_loss: 2.1384 - val_accuracy: 0.5682\n","Epoch 33/100\n","301/301 [==============================] - 4s 12ms/step - loss: 2.1090 - accuracy: 0.5599 - val_loss: 2.2154 - val_accuracy: 0.5394\n","Epoch 34/100\n","301/301 [==============================] - 4s 14ms/step - loss: 2.0826 - accuracy: 0.5626 - val_loss: 2.1969 - val_accuracy: 0.5343\n","Epoch 35/100\n","301/301 [==============================] - 4s 12ms/step - loss: 2.0671 - accuracy: 0.5597 - val_loss: 2.1785 - val_accuracy: 0.5493\n","Epoch 36/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0596 - accuracy: 0.5627 - val_loss: 2.2435 - val_accuracy: 0.5088\n","Epoch 37/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0550 - accuracy: 0.5632 - val_loss: 2.1462 - val_accuracy: 0.5460\n","Epoch 38/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0275 - accuracy: 0.5638 - val_loss: 2.1218 - val_accuracy: 0.5306\n","Epoch 39/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0383 - accuracy: 0.5631 - val_loss: 2.0933 - val_accuracy: 0.5678\n","Epoch 40/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0219 - accuracy: 0.5661 - val_loss: 2.1631 - val_accuracy: 0.5391\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0155 - accuracy: 0.5690 - val_loss: 2.0877 - val_accuracy: 0.5714\n","Epoch 42/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0175 - accuracy: 0.5682 - val_loss: 2.0627 - val_accuracy: 0.5533\n","Epoch 43/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9961 - accuracy: 0.5687 - val_loss: 2.1330 - val_accuracy: 0.5299\n","Epoch 44/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9867 - accuracy: 0.5727 - val_loss: 2.1705 - val_accuracy: 0.5219\n","Epoch 45/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0024 - accuracy: 0.5683 - val_loss: 2.1300 - val_accuracy: 0.5426\n","Epoch 46/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9761 - accuracy: 0.5737 - val_loss: 2.1725 - val_accuracy: 0.5274\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9966 - accuracy: 0.5726 - val_loss: 2.1219 - val_accuracy: 0.5525\n","Epoch 48/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9813 - accuracy: 0.5737 - val_loss: 2.1368 - val_accuracy: 0.5400\n","Epoch 49/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9755 - accuracy: 0.5722 - val_loss: 2.0344 - val_accuracy: 0.5715\n","Epoch 50/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9618 - accuracy: 0.5754 - val_loss: 2.0279 - val_accuracy: 0.5613\n","Epoch 51/100\n","301/301 [==============================] - 4s 12ms/step - loss: 1.9530 - accuracy: 0.5735 - val_loss: 2.0766 - val_accuracy: 0.5442\n","Epoch 52/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9673 - accuracy: 0.5768 - val_loss: 2.0882 - val_accuracy: 0.5334\n","Epoch 53/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9389 - accuracy: 0.5760 - val_loss: 2.0492 - val_accuracy: 0.5568\n","Epoch 54/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9201 - accuracy: 0.5802 - val_loss: 2.0963 - val_accuracy: 0.5215\n","Epoch 55/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9125 - accuracy: 0.5865 - val_loss: 2.0689 - val_accuracy: 0.5476\n","Epoch 56/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9205 - accuracy: 0.5783 - val_loss: 2.0384 - val_accuracy: 0.5541\n","Epoch 57/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9020 - accuracy: 0.5821 - val_loss: 2.0067 - val_accuracy: 0.5618\n","Epoch 58/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9114 - accuracy: 0.5818 - val_loss: 2.0209 - val_accuracy: 0.5597\n","Epoch 59/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9022 - accuracy: 0.5851 - val_loss: 2.0642 - val_accuracy: 0.5448\n","Epoch 60/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9153 - accuracy: 0.5818 - val_loss: 2.0428 - val_accuracy: 0.5570\n","Epoch 61/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9106 - accuracy: 0.5869 - val_loss: 2.0150 - val_accuracy: 0.5646\n","Epoch 62/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8950 - accuracy: 0.5861 - val_loss: 1.9993 - val_accuracy: 0.5702\n","Epoch 63/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8762 - accuracy: 0.5912 - val_loss: 2.0730 - val_accuracy: 0.5360\n","Epoch 64/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8850 - accuracy: 0.5872 - val_loss: 1.9601 - val_accuracy: 0.5733\n","Epoch 65/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8775 - accuracy: 0.5911 - val_loss: 2.0367 - val_accuracy: 0.5451\n","Epoch 66/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8889 - accuracy: 0.5879 - val_loss: 2.0579 - val_accuracy: 0.5494\n","Epoch 67/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8735 - accuracy: 0.5894 - val_loss: 2.1877 - val_accuracy: 0.4841\n","Epoch 68/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8645 - accuracy: 0.5864 - val_loss: 2.0304 - val_accuracy: 0.5499\n","Epoch 69/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8572 - accuracy: 0.5907 - val_loss: 2.0323 - val_accuracy: 0.5539\n","Epoch 70/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8616 - accuracy: 0.5883 - val_loss: 1.9643 - val_accuracy: 0.5638\n","Epoch 71/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8473 - accuracy: 0.5960 - val_loss: 2.0406 - val_accuracy: 0.5569\n","Epoch 72/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8486 - accuracy: 0.5973 - val_loss: 1.9778 - val_accuracy: 0.5577\n","Epoch 73/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8407 - accuracy: 0.5943 - val_loss: 2.1218 - val_accuracy: 0.5212\n","Epoch 74/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8312 - accuracy: 0.5959 - val_loss: 2.0325 - val_accuracy: 0.5398\n","Epoch 75/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8339 - accuracy: 0.5924 - val_loss: 1.9726 - val_accuracy: 0.5701\n","Epoch 76/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8107 - accuracy: 0.6009 - val_loss: 1.9911 - val_accuracy: 0.5635\n","Epoch 77/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8259 - accuracy: 0.5974 - val_loss: 2.0383 - val_accuracy: 0.5337\n","Epoch 78/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8199 - accuracy: 0.5963 - val_loss: 1.9850 - val_accuracy: 0.5533\n","Epoch 79/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8258 - accuracy: 0.5982 - val_loss: 1.9712 - val_accuracy: 0.5606\n","Epoch 80/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8397 - accuracy: 0.5974 - val_loss: 2.0880 - val_accuracy: 0.5356\n","Epoch 81/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8047 - accuracy: 0.6038 - val_loss: 2.0385 - val_accuracy: 0.5419\n","Epoch 82/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8124 - accuracy: 0.6027 - val_loss: 1.9453 - val_accuracy: 0.5766\n","Epoch 83/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8065 - accuracy: 0.6057 - val_loss: 2.0068 - val_accuracy: 0.5579\n","Epoch 84/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7977 - accuracy: 0.6029 - val_loss: 2.0751 - val_accuracy: 0.5229\n","Epoch 85/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8293 - accuracy: 0.5948 - val_loss: 2.0423 - val_accuracy: 0.5481\n","Epoch 86/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8008 - accuracy: 0.6068 - val_loss: 1.9947 - val_accuracy: 0.5499\n","Epoch 87/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8083 - accuracy: 0.6089 - val_loss: 1.9935 - val_accuracy: 0.5660\n","Epoch 88/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8076 - accuracy: 0.5992 - val_loss: 1.9746 - val_accuracy: 0.5723\n","Epoch 89/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8025 - accuracy: 0.6046 - val_loss: 2.1045 - val_accuracy: 0.5112\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7876 - accuracy: 0.6075 - val_loss: 1.9576 - val_accuracy: 0.5702\n","Epoch 91/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7803 - accuracy: 0.6107 - val_loss: 1.9529 - val_accuracy: 0.5705\n","Epoch 92/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7827 - accuracy: 0.6019 - val_loss: 2.0654 - val_accuracy: 0.5234\n","Epoch 93/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7749 - accuracy: 0.6070 - val_loss: 1.9963 - val_accuracy: 0.5556\n","Epoch 94/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7794 - accuracy: 0.6094 - val_loss: 1.9635 - val_accuracy: 0.5600\n","Epoch 95/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7681 - accuracy: 0.6126 - val_loss: 2.0586 - val_accuracy: 0.5242\n","Epoch 96/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7731 - accuracy: 0.6109 - val_loss: 2.0224 - val_accuracy: 0.5500\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7763 - accuracy: 0.6125 - val_loss: 1.9230 - val_accuracy: 0.5809\n","Epoch 98/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7614 - accuracy: 0.6141 - val_loss: 2.0513 - val_accuracy: 0.5443\n","Epoch 99/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7680 - accuracy: 0.6114 - val_loss: 1.9617 - val_accuracy: 0.5599\n","Epoch 100/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7708 - accuracy: 0.6086 - val_loss: 1.9697 - val_accuracy: 0.5628\n"]}]},{"cell_type":"code","source":["# I think we need just a little more complexity\n","# that probably wasn't any better. going to relax dimensionality reduction\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(32,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvsDeZjWaedo","executionInfo":{"status":"ok","timestamp":1652126960477,"user_tz":240,"elapsed":443685,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"8f00f4ab-e9c3-4868-8fc0-c30e98f4bf87"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 5s 14ms/step - loss: 14.9923 - accuracy: 0.3677 - val_loss: 5.1408 - val_accuracy: 0.0284\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.9200 - accuracy: 0.4014 - val_loss: 3.7859 - val_accuracy: 0.3299\n","Epoch 3/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.6857 - accuracy: 0.4280 - val_loss: 3.5884 - val_accuracy: 0.4807\n","Epoch 4/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.5330 - accuracy: 0.4444 - val_loss: 3.4619 - val_accuracy: 0.4488\n","Epoch 5/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.3951 - accuracy: 0.4561 - val_loss: 3.2565 - val_accuracy: 0.4947\n","Epoch 6/100\n","301/301 [==============================] - 4s 15ms/step - loss: 3.2895 - accuracy: 0.4613 - val_loss: 3.2873 - val_accuracy: 0.4899\n","Epoch 7/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.1820 - accuracy: 0.4778 - val_loss: 3.1048 - val_accuracy: 0.5085\n","Epoch 8/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.0949 - accuracy: 0.4838 - val_loss: 3.0431 - val_accuracy: 0.5181\n","Epoch 9/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.9968 - accuracy: 0.4914 - val_loss: 2.8793 - val_accuracy: 0.5069\n","Epoch 10/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.9085 - accuracy: 0.5003 - val_loss: 2.8565 - val_accuracy: 0.4949\n","Epoch 11/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.8442 - accuracy: 0.5018 - val_loss: 2.8402 - val_accuracy: 0.5372\n","Epoch 12/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7821 - accuracy: 0.5072 - val_loss: 2.7088 - val_accuracy: 0.5454\n","Epoch 13/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.7359 - accuracy: 0.5122 - val_loss: 2.7424 - val_accuracy: 0.5209\n","Epoch 14/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6605 - accuracy: 0.5135 - val_loss: 2.6130 - val_accuracy: 0.5305\n","Epoch 15/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.6221 - accuracy: 0.5133 - val_loss: 2.5855 - val_accuracy: 0.5457\n","Epoch 16/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.5629 - accuracy: 0.5216 - val_loss: 2.5444 - val_accuracy: 0.4906\n","Epoch 17/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5151 - accuracy: 0.5204 - val_loss: 2.4648 - val_accuracy: 0.5613\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4703 - accuracy: 0.5285 - val_loss: 2.4353 - val_accuracy: 0.5280\n","Epoch 19/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.4466 - accuracy: 0.5274 - val_loss: 2.5159 - val_accuracy: 0.5119\n","Epoch 20/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3948 - accuracy: 0.5279 - val_loss: 2.3904 - val_accuracy: 0.5331\n","Epoch 21/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3662 - accuracy: 0.5323 - val_loss: 2.2904 - val_accuracy: 0.5435\n","Epoch 22/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3402 - accuracy: 0.5313 - val_loss: 2.3162 - val_accuracy: 0.5418\n","Epoch 23/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3218 - accuracy: 0.5344 - val_loss: 2.2819 - val_accuracy: 0.5568\n","Epoch 24/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2951 - accuracy: 0.5373 - val_loss: 2.2534 - val_accuracy: 0.5519\n","Epoch 25/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2613 - accuracy: 0.5431 - val_loss: 2.2338 - val_accuracy: 0.5642\n","Epoch 26/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2501 - accuracy: 0.5401 - val_loss: 2.2592 - val_accuracy: 0.5351\n","Epoch 27/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2290 - accuracy: 0.5421 - val_loss: 2.2058 - val_accuracy: 0.5618\n","Epoch 28/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2024 - accuracy: 0.5474 - val_loss: 2.2525 - val_accuracy: 0.5184\n","Epoch 29/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1833 - accuracy: 0.5449 - val_loss: 2.1690 - val_accuracy: 0.5620\n","Epoch 30/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1695 - accuracy: 0.5466 - val_loss: 2.1566 - val_accuracy: 0.5428\n","Epoch 31/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1571 - accuracy: 0.5427 - val_loss: 2.1456 - val_accuracy: 0.5546\n","Epoch 32/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1545 - accuracy: 0.5468 - val_loss: 2.1434 - val_accuracy: 0.5563\n","Epoch 33/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1375 - accuracy: 0.5497 - val_loss: 2.2222 - val_accuracy: 0.5417\n","Epoch 34/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1274 - accuracy: 0.5528 - val_loss: 2.1164 - val_accuracy: 0.5640\n","Epoch 35/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1048 - accuracy: 0.5505 - val_loss: 2.1656 - val_accuracy: 0.5472\n","Epoch 36/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1105 - accuracy: 0.5469 - val_loss: 2.0986 - val_accuracy: 0.5636\n","Epoch 37/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0964 - accuracy: 0.5520 - val_loss: 2.1443 - val_accuracy: 0.5514\n","Epoch 38/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.0869 - accuracy: 0.5534 - val_loss: 2.1490 - val_accuracy: 0.5363\n","Epoch 39/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0912 - accuracy: 0.5476 - val_loss: 2.0579 - val_accuracy: 0.5726\n","Epoch 40/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0860 - accuracy: 0.5541 - val_loss: 2.0983 - val_accuracy: 0.5702\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0771 - accuracy: 0.5515 - val_loss: 2.0888 - val_accuracy: 0.5594\n","Epoch 42/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0539 - accuracy: 0.5542 - val_loss: 2.0275 - val_accuracy: 0.5881\n","Epoch 43/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0596 - accuracy: 0.5536 - val_loss: 2.1353 - val_accuracy: 0.5401\n","Epoch 44/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0474 - accuracy: 0.5554 - val_loss: 2.1045 - val_accuracy: 0.5520\n","Epoch 45/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.0408 - accuracy: 0.5603 - val_loss: 2.0829 - val_accuracy: 0.5486\n","Epoch 46/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0233 - accuracy: 0.5612 - val_loss: 2.1071 - val_accuracy: 0.5406\n","Epoch 47/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0197 - accuracy: 0.5586 - val_loss: 2.0095 - val_accuracy: 0.5722\n","Epoch 48/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0109 - accuracy: 0.5573 - val_loss: 2.0432 - val_accuracy: 0.5636\n","Epoch 49/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0209 - accuracy: 0.5595 - val_loss: 2.0726 - val_accuracy: 0.5564\n","Epoch 50/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9948 - accuracy: 0.5688 - val_loss: 2.0889 - val_accuracy: 0.5313\n","Epoch 51/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0113 - accuracy: 0.5571 - val_loss: 2.0360 - val_accuracy: 0.5713\n","Epoch 52/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9900 - accuracy: 0.5637 - val_loss: 2.0919 - val_accuracy: 0.5442\n","Epoch 53/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0049 - accuracy: 0.5639 - val_loss: 2.0521 - val_accuracy: 0.5585\n","Epoch 54/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0039 - accuracy: 0.5622 - val_loss: 2.0248 - val_accuracy: 0.5619\n","Epoch 55/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9821 - accuracy: 0.5635 - val_loss: 2.0779 - val_accuracy: 0.5430\n","Epoch 56/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9882 - accuracy: 0.5632 - val_loss: 2.0643 - val_accuracy: 0.5487\n","Epoch 57/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9917 - accuracy: 0.5637 - val_loss: 2.0213 - val_accuracy: 0.5739\n","Epoch 58/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9949 - accuracy: 0.5666 - val_loss: 1.9975 - val_accuracy: 0.5828\n","Epoch 59/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0012 - accuracy: 0.5617 - val_loss: 2.0877 - val_accuracy: 0.5361\n","Epoch 60/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9817 - accuracy: 0.5611 - val_loss: 1.9870 - val_accuracy: 0.5720\n","Epoch 61/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9604 - accuracy: 0.5655 - val_loss: 2.0097 - val_accuracy: 0.5673\n","Epoch 62/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9631 - accuracy: 0.5639 - val_loss: 2.0142 - val_accuracy: 0.5668\n","Epoch 63/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9435 - accuracy: 0.5704 - val_loss: 2.0274 - val_accuracy: 0.5477\n","Epoch 64/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9487 - accuracy: 0.5700 - val_loss: 2.0000 - val_accuracy: 0.5695\n","Epoch 65/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9636 - accuracy: 0.5652 - val_loss: 1.9921 - val_accuracy: 0.5651\n","Epoch 66/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9524 - accuracy: 0.5587 - val_loss: 1.9305 - val_accuracy: 0.5792\n","Epoch 67/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9556 - accuracy: 0.5643 - val_loss: 1.9910 - val_accuracy: 0.5698\n","Epoch 68/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9505 - accuracy: 0.5649 - val_loss: 1.9897 - val_accuracy: 0.5627\n","Epoch 69/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9483 - accuracy: 0.5669 - val_loss: 2.0375 - val_accuracy: 0.5580\n","Epoch 70/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9330 - accuracy: 0.5694 - val_loss: 2.0021 - val_accuracy: 0.5520\n","Epoch 71/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9203 - accuracy: 0.5714 - val_loss: 1.9159 - val_accuracy: 0.5826\n","Epoch 72/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9282 - accuracy: 0.5687 - val_loss: 2.0009 - val_accuracy: 0.5595\n","Epoch 73/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9241 - accuracy: 0.5728 - val_loss: 2.0613 - val_accuracy: 0.5372\n","Epoch 74/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9329 - accuracy: 0.5667 - val_loss: 1.9898 - val_accuracy: 0.5549\n","Epoch 75/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9235 - accuracy: 0.5713 - val_loss: 1.9296 - val_accuracy: 0.5834\n","Epoch 76/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9193 - accuracy: 0.5666 - val_loss: 2.0017 - val_accuracy: 0.5700\n","Epoch 77/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9301 - accuracy: 0.5784 - val_loss: 1.9887 - val_accuracy: 0.5641\n","Epoch 78/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9210 - accuracy: 0.5769 - val_loss: 1.9782 - val_accuracy: 0.5586\n","Epoch 79/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9199 - accuracy: 0.5712 - val_loss: 2.0096 - val_accuracy: 0.5595\n","Epoch 80/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9021 - accuracy: 0.5700 - val_loss: 1.9692 - val_accuracy: 0.5596\n","Epoch 81/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9127 - accuracy: 0.5710 - val_loss: 1.9902 - val_accuracy: 0.5652\n","Epoch 82/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9071 - accuracy: 0.5692 - val_loss: 1.9941 - val_accuracy: 0.5552\n","Epoch 83/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9038 - accuracy: 0.5740 - val_loss: 1.9631 - val_accuracy: 0.5690\n","Epoch 84/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9114 - accuracy: 0.5734 - val_loss: 1.9716 - val_accuracy: 0.5638\n","Epoch 85/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9038 - accuracy: 0.5658 - val_loss: 1.9444 - val_accuracy: 0.5832\n","Epoch 86/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9181 - accuracy: 0.5737 - val_loss: 2.0097 - val_accuracy: 0.5567\n","Epoch 87/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9134 - accuracy: 0.5733 - val_loss: 1.9803 - val_accuracy: 0.5597\n","Epoch 88/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9081 - accuracy: 0.5765 - val_loss: 1.9770 - val_accuracy: 0.5554\n","Epoch 89/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8966 - accuracy: 0.5679 - val_loss: 1.9667 - val_accuracy: 0.5590\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8944 - accuracy: 0.5713 - val_loss: 1.9923 - val_accuracy: 0.5598\n","Epoch 91/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8920 - accuracy: 0.5788 - val_loss: 1.9938 - val_accuracy: 0.5521\n","Epoch 92/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9053 - accuracy: 0.5765 - val_loss: 1.9962 - val_accuracy: 0.5446\n","Epoch 93/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8951 - accuracy: 0.5802 - val_loss: 1.9478 - val_accuracy: 0.5697\n","Epoch 94/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9156 - accuracy: 0.5720 - val_loss: 1.9699 - val_accuracy: 0.5691\n","Epoch 95/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9016 - accuracy: 0.5764 - val_loss: 1.9929 - val_accuracy: 0.5590\n","Epoch 96/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8873 - accuracy: 0.5730 - val_loss: 1.9470 - val_accuracy: 0.5786\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8699 - accuracy: 0.5764 - val_loss: 1.9621 - val_accuracy: 0.5644\n","Epoch 98/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8923 - accuracy: 0.5759 - val_loss: 1.9801 - val_accuracy: 0.5529\n","Epoch 99/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8842 - accuracy: 0.5744 - val_loss: 1.9460 - val_accuracy: 0.5700\n","Epoch 100/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8684 - accuracy: 0.5741 - val_loss: 1.9087 - val_accuracy: 0.5792\n"]}]},{"cell_type":"code","source":["# I think we need just a little more complexity\n","# that probably wasn't any better. going to relax dimensionality reduction\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(64,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zevTWB_c42-","executionInfo":{"status":"ok","timestamp":1652127591580,"user_tz":240,"elapsed":443671,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"bd9b557d-bd77-4290-c5fa-e993008e468c"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 6s 16ms/step - loss: 14.6967 - accuracy: 0.3786 - val_loss: 5.2631 - val_accuracy: 0.0288\n","Epoch 2/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.9352 - accuracy: 0.4079 - val_loss: 3.8214 - val_accuracy: 0.4128\n","Epoch 3/100\n","301/301 [==============================] - 4s 12ms/step - loss: 3.6738 - accuracy: 0.4366 - val_loss: 3.5314 - val_accuracy: 0.4795\n","Epoch 4/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.5052 - accuracy: 0.4590 - val_loss: 3.4766 - val_accuracy: 0.4291\n","Epoch 5/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.3717 - accuracy: 0.4718 - val_loss: 3.2670 - val_accuracy: 0.5191\n","Epoch 6/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.2585 - accuracy: 0.4765 - val_loss: 3.2525 - val_accuracy: 0.4868\n","Epoch 7/100\n","301/301 [==============================] - 4s 12ms/step - loss: 3.1639 - accuracy: 0.4867 - val_loss: 3.0287 - val_accuracy: 0.5444\n","Epoch 8/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.0558 - accuracy: 0.5018 - val_loss: 2.9685 - val_accuracy: 0.5563\n","Epoch 9/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9848 - accuracy: 0.5010 - val_loss: 2.8871 - val_accuracy: 0.5324\n","Epoch 10/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9056 - accuracy: 0.5073 - val_loss: 2.7983 - val_accuracy: 0.5329\n","Epoch 11/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8335 - accuracy: 0.5018 - val_loss: 2.7810 - val_accuracy: 0.5528\n","Epoch 12/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7756 - accuracy: 0.5133 - val_loss: 2.7962 - val_accuracy: 0.4981\n","Epoch 13/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7209 - accuracy: 0.5185 - val_loss: 2.7593 - val_accuracy: 0.5365\n","Epoch 14/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.6526 - accuracy: 0.5231 - val_loss: 2.6632 - val_accuracy: 0.5421\n","Epoch 15/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.6066 - accuracy: 0.5194 - val_loss: 2.4877 - val_accuracy: 0.5599\n","Epoch 16/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5408 - accuracy: 0.5316 - val_loss: 2.4628 - val_accuracy: 0.5589\n","Epoch 17/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5111 - accuracy: 0.5272 - val_loss: 2.4858 - val_accuracy: 0.5578\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4721 - accuracy: 0.5297 - val_loss: 2.4923 - val_accuracy: 0.5292\n","Epoch 19/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4251 - accuracy: 0.5333 - val_loss: 2.3719 - val_accuracy: 0.5636\n","Epoch 20/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3842 - accuracy: 0.5312 - val_loss: 2.4703 - val_accuracy: 0.5249\n","Epoch 21/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3410 - accuracy: 0.5362 - val_loss: 2.3404 - val_accuracy: 0.5367\n","Epoch 22/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3039 - accuracy: 0.5410 - val_loss: 2.3225 - val_accuracy: 0.5522\n","Epoch 23/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3023 - accuracy: 0.5392 - val_loss: 2.3134 - val_accuracy: 0.5435\n","Epoch 24/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2629 - accuracy: 0.5454 - val_loss: 2.2472 - val_accuracy: 0.5615\n","Epoch 25/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2301 - accuracy: 0.5433 - val_loss: 2.1974 - val_accuracy: 0.5552\n","Epoch 26/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2092 - accuracy: 0.5492 - val_loss: 2.1568 - val_accuracy: 0.5736\n","Epoch 27/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1993 - accuracy: 0.5452 - val_loss: 2.1818 - val_accuracy: 0.5600\n","Epoch 28/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1692 - accuracy: 0.5484 - val_loss: 2.1710 - val_accuracy: 0.5588\n","Epoch 29/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1741 - accuracy: 0.5443 - val_loss: 2.1896 - val_accuracy: 0.5576\n","Epoch 30/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1701 - accuracy: 0.5498 - val_loss: 2.2199 - val_accuracy: 0.5256\n","Epoch 31/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1276 - accuracy: 0.5547 - val_loss: 2.1843 - val_accuracy: 0.5501\n","Epoch 32/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1222 - accuracy: 0.5476 - val_loss: 2.1732 - val_accuracy: 0.5581\n","Epoch 33/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1080 - accuracy: 0.5532 - val_loss: 2.1273 - val_accuracy: 0.5557\n","Epoch 34/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0788 - accuracy: 0.5633 - val_loss: 2.0929 - val_accuracy: 0.5745\n","Epoch 35/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0829 - accuracy: 0.5565 - val_loss: 2.1055 - val_accuracy: 0.5639\n","Epoch 36/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0624 - accuracy: 0.5556 - val_loss: 2.1780 - val_accuracy: 0.5375\n","Epoch 37/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0605 - accuracy: 0.5561 - val_loss: 2.0334 - val_accuracy: 0.5762\n","Epoch 38/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0342 - accuracy: 0.5588 - val_loss: 2.0718 - val_accuracy: 0.5636\n","Epoch 39/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0422 - accuracy: 0.5576 - val_loss: 2.1230 - val_accuracy: 0.5258\n","Epoch 40/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0332 - accuracy: 0.5615 - val_loss: 2.0346 - val_accuracy: 0.5722\n","Epoch 41/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0462 - accuracy: 0.5599 - val_loss: 2.1141 - val_accuracy: 0.5397\n","Epoch 42/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0172 - accuracy: 0.5641 - val_loss: 1.9979 - val_accuracy: 0.5717\n","Epoch 43/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0015 - accuracy: 0.5631 - val_loss: 2.1099 - val_accuracy: 0.5285\n","Epoch 44/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9909 - accuracy: 0.5626 - val_loss: 2.0606 - val_accuracy: 0.5599\n","Epoch 45/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9936 - accuracy: 0.5681 - val_loss: 2.0270 - val_accuracy: 0.5678\n","Epoch 46/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0002 - accuracy: 0.5580 - val_loss: 2.0005 - val_accuracy: 0.5695\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9714 - accuracy: 0.5658 - val_loss: 2.1202 - val_accuracy: 0.5133\n","Epoch 48/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9772 - accuracy: 0.5626 - val_loss: 2.0653 - val_accuracy: 0.5404\n","Epoch 49/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9705 - accuracy: 0.5665 - val_loss: 2.0553 - val_accuracy: 0.5524\n","Epoch 50/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9541 - accuracy: 0.5654 - val_loss: 2.0221 - val_accuracy: 0.5558\n","Epoch 51/100\n","301/301 [==============================] - 4s 14ms/step - loss: 1.9671 - accuracy: 0.5660 - val_loss: 2.0420 - val_accuracy: 0.5497\n","Epoch 52/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9516 - accuracy: 0.5721 - val_loss: 2.0396 - val_accuracy: 0.5480\n","Epoch 53/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9381 - accuracy: 0.5679 - val_loss: 2.0420 - val_accuracy: 0.5373\n","Epoch 54/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9148 - accuracy: 0.5693 - val_loss: 2.0517 - val_accuracy: 0.5380\n","Epoch 55/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9344 - accuracy: 0.5693 - val_loss: 2.0439 - val_accuracy: 0.5428\n","Epoch 56/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9238 - accuracy: 0.5709 - val_loss: 2.0725 - val_accuracy: 0.5475\n","Epoch 57/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9415 - accuracy: 0.5689 - val_loss: 1.9855 - val_accuracy: 0.5642\n","Epoch 58/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9377 - accuracy: 0.5674 - val_loss: 2.0835 - val_accuracy: 0.5368\n","Epoch 59/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9369 - accuracy: 0.5707 - val_loss: 1.9729 - val_accuracy: 0.5619\n","Epoch 60/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9304 - accuracy: 0.5705 - val_loss: 2.0416 - val_accuracy: 0.5429\n","Epoch 61/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9317 - accuracy: 0.5693 - val_loss: 2.0220 - val_accuracy: 0.5460\n","Epoch 62/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9124 - accuracy: 0.5693 - val_loss: 1.9881 - val_accuracy: 0.5618\n","Epoch 63/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9143 - accuracy: 0.5734 - val_loss: 2.0253 - val_accuracy: 0.5431\n","Epoch 64/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9098 - accuracy: 0.5757 - val_loss: 1.9997 - val_accuracy: 0.5501\n","Epoch 65/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8897 - accuracy: 0.5769 - val_loss: 2.0234 - val_accuracy: 0.5348\n","Epoch 66/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8899 - accuracy: 0.5729 - val_loss: 1.9662 - val_accuracy: 0.5749\n","Epoch 67/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8980 - accuracy: 0.5791 - val_loss: 2.0284 - val_accuracy: 0.5337\n","Epoch 68/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8834 - accuracy: 0.5742 - val_loss: 2.0014 - val_accuracy: 0.5478\n","Epoch 69/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8823 - accuracy: 0.5717 - val_loss: 1.9887 - val_accuracy: 0.5580\n","Epoch 70/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8829 - accuracy: 0.5745 - val_loss: 1.9772 - val_accuracy: 0.5616\n","Epoch 71/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8797 - accuracy: 0.5765 - val_loss: 1.9453 - val_accuracy: 0.5509\n","Epoch 72/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8734 - accuracy: 0.5727 - val_loss: 2.0346 - val_accuracy: 0.5346\n","Epoch 73/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8677 - accuracy: 0.5788 - val_loss: 1.9285 - val_accuracy: 0.5702\n","Epoch 74/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8457 - accuracy: 0.5789 - val_loss: 2.0004 - val_accuracy: 0.5418\n","Epoch 75/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8519 - accuracy: 0.5788 - val_loss: 1.9454 - val_accuracy: 0.5699\n","Epoch 76/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8642 - accuracy: 0.5771 - val_loss: 1.9675 - val_accuracy: 0.5507\n","Epoch 77/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8667 - accuracy: 0.5811 - val_loss: 2.0030 - val_accuracy: 0.5489\n","Epoch 78/100\n","301/301 [==============================] - 4s 12ms/step - loss: 1.8515 - accuracy: 0.5758 - val_loss: 1.9559 - val_accuracy: 0.5487\n","Epoch 79/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8598 - accuracy: 0.5752 - val_loss: 1.9593 - val_accuracy: 0.5603\n","Epoch 80/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8675 - accuracy: 0.5765 - val_loss: 1.9774 - val_accuracy: 0.5629\n","Epoch 81/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8392 - accuracy: 0.5801 - val_loss: 1.9354 - val_accuracy: 0.5539\n","Epoch 82/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8560 - accuracy: 0.5821 - val_loss: 2.0207 - val_accuracy: 0.5379\n","Epoch 83/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8548 - accuracy: 0.5748 - val_loss: 1.9459 - val_accuracy: 0.5548\n","Epoch 84/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8410 - accuracy: 0.5832 - val_loss: 1.9521 - val_accuracy: 0.5570\n","Epoch 85/100\n","301/301 [==============================] - 4s 12ms/step - loss: 1.8456 - accuracy: 0.5865 - val_loss: 2.0466 - val_accuracy: 0.5412\n","Epoch 86/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8481 - accuracy: 0.5817 - val_loss: 1.9896 - val_accuracy: 0.5462\n","Epoch 87/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8484 - accuracy: 0.5844 - val_loss: 2.0177 - val_accuracy: 0.5318\n","Epoch 88/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8508 - accuracy: 0.5772 - val_loss: 1.9820 - val_accuracy: 0.5606\n","Epoch 89/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8318 - accuracy: 0.5841 - val_loss: 1.9360 - val_accuracy: 0.5618\n","Epoch 90/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8356 - accuracy: 0.5822 - val_loss: 1.9496 - val_accuracy: 0.5569\n","Epoch 91/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8307 - accuracy: 0.5794 - val_loss: 2.0426 - val_accuracy: 0.5308\n","Epoch 92/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8188 - accuracy: 0.5903 - val_loss: 1.9448 - val_accuracy: 0.5717\n","Epoch 93/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8259 - accuracy: 0.5849 - val_loss: 1.9210 - val_accuracy: 0.5709\n","Epoch 94/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8383 - accuracy: 0.5872 - val_loss: 1.9855 - val_accuracy: 0.5516\n","Epoch 95/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8269 - accuracy: 0.5851 - val_loss: 1.9517 - val_accuracy: 0.5666\n","Epoch 96/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8258 - accuracy: 0.5842 - val_loss: 1.9409 - val_accuracy: 0.5597\n","Epoch 97/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8067 - accuracy: 0.5871 - val_loss: 1.9378 - val_accuracy: 0.5615\n","Epoch 98/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8203 - accuracy: 0.5826 - val_loss: 2.0189 - val_accuracy: 0.5244\n","Epoch 99/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8068 - accuracy: 0.5910 - val_loss: 1.9357 - val_accuracy: 0.5529\n","Epoch 100/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8082 - accuracy: 0.5894 - val_loss: 1.9491 - val_accuracy: 0.5480\n"]}]},{"cell_type":"code","source":["# I think we need just a little more complexity\n","# that probably wasn't any better. going to relax dimensionality reduction\n","layer_list = [ #520 parameters in input\n","          layers.BatchNormalization(),\n","          #layers.Dropout(0.4),\n","          layers.Dense(520,kernel_initializer='lecun_normal',kernel_regularizer='l1'),\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(128,kernel_initializer='lecun_normal'), #let's try dimensionality reduction\n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,kernel_initializer='lecun_normal'), \n","          layers.BatchNormalization(),\n","          layers.Activation('selu'),\n","          layers.Dense(16,activation='softmax',kernel_initializer='lecun_normal'),\n","          ]\n","model = Sequential(layer_list)\n","model.compile(optimizer=keras.optimizers.Adam(\n","    learning_rate=keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,decay_steps=2000,decay_rate=0.99)),\n","    loss=\"categorical_crossentropy\", metrics=['accuracy'],)\n","history = model.fit(x_train_small, y_train_small_1hot, batch_size=32, epochs=100, validation_data=(x_val_small, y_val_small_1hot))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wemqhgb5elly","executionInfo":{"status":"ok","timestamp":1652128042201,"user_tz":240,"elapsed":443769,"user":{"displayName":"Finn Clark","userId":"13891723806369472987"}},"outputId":"e563d1b6-1676-435d-9064-fa9d81329b75"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","301/301 [==============================] - 6s 16ms/step - loss: 14.4877 - accuracy: 0.3789 - val_loss: 4.3712 - val_accuracy: 0.0515\n","Epoch 2/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.8286 - accuracy: 0.4095 - val_loss: 3.7049 - val_accuracy: 0.4897\n","Epoch 3/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.5973 - accuracy: 0.4362 - val_loss: 3.5144 - val_accuracy: 0.4884\n","Epoch 4/100\n","301/301 [==============================] - 4s 13ms/step - loss: 3.4461 - accuracy: 0.4585 - val_loss: 3.2868 - val_accuracy: 0.5066\n","Epoch 5/100\n","301/301 [==============================] - 4s 15ms/step - loss: 3.3230 - accuracy: 0.4756 - val_loss: 3.1000 - val_accuracy: 0.5170\n","Epoch 6/100\n","301/301 [==============================] - 5s 15ms/step - loss: 3.1762 - accuracy: 0.4795 - val_loss: 3.2357 - val_accuracy: 0.4526\n","Epoch 7/100\n","301/301 [==============================] - 4s 15ms/step - loss: 3.1094 - accuracy: 0.4881 - val_loss: 3.1304 - val_accuracy: 0.4822\n","Epoch 8/100\n","301/301 [==============================] - 4s 14ms/step - loss: 3.0366 - accuracy: 0.4937 - val_loss: 2.9755 - val_accuracy: 0.5330\n","Epoch 9/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.9406 - accuracy: 0.5018 - val_loss: 2.8558 - val_accuracy: 0.5510\n","Epoch 10/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.8851 - accuracy: 0.5011 - val_loss: 2.9177 - val_accuracy: 0.4908\n","Epoch 11/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.8482 - accuracy: 0.5060 - val_loss: 2.7139 - val_accuracy: 0.5348\n","Epoch 12/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.7420 - accuracy: 0.5161 - val_loss: 2.7990 - val_accuracy: 0.5508\n","Epoch 13/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.7346 - accuracy: 0.5205 - val_loss: 2.5961 - val_accuracy: 0.5465\n","Epoch 14/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.6534 - accuracy: 0.5163 - val_loss: 2.6155 - val_accuracy: 0.5291\n","Epoch 15/100\n","301/301 [==============================] - 4s 12ms/step - loss: 2.6067 - accuracy: 0.5222 - val_loss: 2.5604 - val_accuracy: 0.5606\n","Epoch 16/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5579 - accuracy: 0.5317 - val_loss: 2.5073 - val_accuracy: 0.5664\n","Epoch 17/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.5054 - accuracy: 0.5316 - val_loss: 2.4634 - val_accuracy: 0.5632\n","Epoch 18/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4572 - accuracy: 0.5392 - val_loss: 2.4461 - val_accuracy: 0.5570\n","Epoch 19/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.4319 - accuracy: 0.5372 - val_loss: 2.3602 - val_accuracy: 0.5549\n","Epoch 20/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3809 - accuracy: 0.5331 - val_loss: 2.3740 - val_accuracy: 0.5423\n","Epoch 21/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.3651 - accuracy: 0.5370 - val_loss: 2.3225 - val_accuracy: 0.5608\n","Epoch 22/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.3114 - accuracy: 0.5445 - val_loss: 2.3017 - val_accuracy: 0.5527\n","Epoch 23/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2648 - accuracy: 0.5444 - val_loss: 2.2315 - val_accuracy: 0.5662\n","Epoch 24/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.2555 - accuracy: 0.5472 - val_loss: 2.2628 - val_accuracy: 0.5686\n","Epoch 25/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.2250 - accuracy: 0.5459 - val_loss: 2.2636 - val_accuracy: 0.5361\n","Epoch 26/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.2189 - accuracy: 0.5549 - val_loss: 2.2588 - val_accuracy: 0.5471\n","Epoch 27/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1938 - accuracy: 0.5506 - val_loss: 2.1543 - val_accuracy: 0.5770\n","Epoch 28/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.1840 - accuracy: 0.5578 - val_loss: 2.2480 - val_accuracy: 0.5602\n","Epoch 29/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1421 - accuracy: 0.5599 - val_loss: 2.1387 - val_accuracy: 0.5661\n","Epoch 30/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1330 - accuracy: 0.5605 - val_loss: 2.1997 - val_accuracy: 0.5540\n","Epoch 31/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.1079 - accuracy: 0.5605 - val_loss: 2.2171 - val_accuracy: 0.5487\n","Epoch 32/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.1058 - accuracy: 0.5596 - val_loss: 2.0966 - val_accuracy: 0.5716\n","Epoch 33/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0947 - accuracy: 0.5639 - val_loss: 2.1384 - val_accuracy: 0.5526\n","Epoch 34/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0746 - accuracy: 0.5670 - val_loss: 2.1982 - val_accuracy: 0.5420\n","Epoch 35/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0724 - accuracy: 0.5696 - val_loss: 2.0787 - val_accuracy: 0.5751\n","Epoch 36/100\n","301/301 [==============================] - 5s 15ms/step - loss: 2.0341 - accuracy: 0.5728 - val_loss: 2.0661 - val_accuracy: 0.5756\n","Epoch 37/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0171 - accuracy: 0.5712 - val_loss: 2.0481 - val_accuracy: 0.5839\n","Epoch 38/100\n","301/301 [==============================] - 4s 15ms/step - loss: 2.0326 - accuracy: 0.5703 - val_loss: 2.0518 - val_accuracy: 0.5637\n","Epoch 39/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0185 - accuracy: 0.5717 - val_loss: 2.1079 - val_accuracy: 0.5538\n","Epoch 40/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0019 - accuracy: 0.5715 - val_loss: 2.0416 - val_accuracy: 0.5744\n","Epoch 41/100\n","301/301 [==============================] - 4s 13ms/step - loss: 2.0145 - accuracy: 0.5708 - val_loss: 2.1076 - val_accuracy: 0.5501\n","Epoch 42/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9897 - accuracy: 0.5771 - val_loss: 2.0645 - val_accuracy: 0.5691\n","Epoch 43/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9766 - accuracy: 0.5754 - val_loss: 2.0701 - val_accuracy: 0.5506\n","Epoch 44/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9625 - accuracy: 0.5831 - val_loss: 2.1087 - val_accuracy: 0.5474\n","Epoch 45/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9813 - accuracy: 0.5786 - val_loss: 2.0525 - val_accuracy: 0.5755\n","Epoch 46/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9668 - accuracy: 0.5808 - val_loss: 2.0357 - val_accuracy: 0.5747\n","Epoch 47/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9630 - accuracy: 0.5822 - val_loss: 2.0249 - val_accuracy: 0.5750\n","Epoch 48/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9343 - accuracy: 0.5845 - val_loss: 2.0519 - val_accuracy: 0.5760\n","Epoch 49/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9277 - accuracy: 0.5864 - val_loss: 2.0277 - val_accuracy: 0.5696\n","Epoch 50/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9254 - accuracy: 0.5852 - val_loss: 2.0277 - val_accuracy: 0.5671\n","Epoch 51/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.9239 - accuracy: 0.5819 - val_loss: 2.0158 - val_accuracy: 0.5708\n","Epoch 52/100\n","301/301 [==============================] - 4s 15ms/step - loss: 1.9142 - accuracy: 0.5856 - val_loss: 1.9885 - val_accuracy: 0.5634\n","Epoch 53/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8922 - accuracy: 0.5878 - val_loss: 2.0634 - val_accuracy: 0.5336\n","Epoch 54/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8882 - accuracy: 0.5913 - val_loss: 2.0211 - val_accuracy: 0.5683\n","Epoch 55/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.9136 - accuracy: 0.5789 - val_loss: 2.0396 - val_accuracy: 0.5640\n","Epoch 56/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8978 - accuracy: 0.5849 - val_loss: 2.0449 - val_accuracy: 0.5574\n","Epoch 57/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8783 - accuracy: 0.5933 - val_loss: 1.9320 - val_accuracy: 0.5850\n","Epoch 58/100\n","301/301 [==============================] - 4s 15ms/step - loss: 1.8759 - accuracy: 0.5945 - val_loss: 2.0577 - val_accuracy: 0.5527\n","Epoch 59/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8913 - accuracy: 0.5868 - val_loss: 2.0291 - val_accuracy: 0.5557\n","Epoch 60/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8696 - accuracy: 0.5946 - val_loss: 1.9548 - val_accuracy: 0.5778\n","Epoch 61/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8692 - accuracy: 0.5957 - val_loss: 2.0356 - val_accuracy: 0.5606\n","Epoch 62/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8896 - accuracy: 0.5915 - val_loss: 2.0225 - val_accuracy: 0.5556\n","Epoch 63/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8509 - accuracy: 0.5953 - val_loss: 2.1113 - val_accuracy: 0.5244\n","Epoch 64/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8452 - accuracy: 0.5979 - val_loss: 2.0092 - val_accuracy: 0.5582\n","Epoch 65/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8542 - accuracy: 0.5972 - val_loss: 2.0149 - val_accuracy: 0.5713\n","Epoch 66/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8540 - accuracy: 0.6011 - val_loss: 1.9962 - val_accuracy: 0.5643\n","Epoch 67/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8485 - accuracy: 0.5960 - val_loss: 1.9678 - val_accuracy: 0.5619\n","Epoch 68/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8401 - accuracy: 0.5958 - val_loss: 1.9594 - val_accuracy: 0.5745\n","Epoch 69/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8335 - accuracy: 0.6055 - val_loss: 2.0491 - val_accuracy: 0.5335\n","Epoch 70/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8063 - accuracy: 0.6006 - val_loss: 2.0320 - val_accuracy: 0.5394\n","Epoch 71/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8197 - accuracy: 0.6043 - val_loss: 2.0844 - val_accuracy: 0.5296\n","Epoch 72/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8256 - accuracy: 0.6070 - val_loss: 2.0242 - val_accuracy: 0.5540\n","Epoch 73/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8218 - accuracy: 0.6019 - val_loss: 1.9417 - val_accuracy: 0.5808\n","Epoch 74/100\n","301/301 [==============================] - 4s 15ms/step - loss: 1.8045 - accuracy: 0.6059 - val_loss: 1.9075 - val_accuracy: 0.5919\n","Epoch 75/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.8023 - accuracy: 0.6069 - val_loss: 1.9317 - val_accuracy: 0.5785\n","Epoch 76/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8125 - accuracy: 0.6030 - val_loss: 2.0340 - val_accuracy: 0.5478\n","Epoch 77/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.8022 - accuracy: 0.6073 - val_loss: 2.0705 - val_accuracy: 0.5257\n","Epoch 78/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7847 - accuracy: 0.6079 - val_loss: 1.9585 - val_accuracy: 0.5632\n","Epoch 79/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7751 - accuracy: 0.6089 - val_loss: 2.0698 - val_accuracy: 0.5278\n","Epoch 80/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7861 - accuracy: 0.6082 - val_loss: 1.9451 - val_accuracy: 0.5683\n","Epoch 81/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7722 - accuracy: 0.6101 - val_loss: 1.9205 - val_accuracy: 0.5810\n","Epoch 82/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7809 - accuracy: 0.6090 - val_loss: 2.0297 - val_accuracy: 0.5581\n","Epoch 83/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7743 - accuracy: 0.6102 - val_loss: 1.9951 - val_accuracy: 0.5587\n","Epoch 84/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7726 - accuracy: 0.6143 - val_loss: 1.9427 - val_accuracy: 0.5788\n","Epoch 85/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7752 - accuracy: 0.6053 - val_loss: 1.9009 - val_accuracy: 0.5880\n","Epoch 86/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7621 - accuracy: 0.6194 - val_loss: 2.0842 - val_accuracy: 0.5254\n","Epoch 87/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7538 - accuracy: 0.6116 - val_loss: 1.8899 - val_accuracy: 0.5933\n","Epoch 88/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7554 - accuracy: 0.6113 - val_loss: 1.9469 - val_accuracy: 0.5676\n","Epoch 89/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7493 - accuracy: 0.6153 - val_loss: 1.9121 - val_accuracy: 0.5824\n","Epoch 90/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7528 - accuracy: 0.6125 - val_loss: 2.0532 - val_accuracy: 0.5395\n","Epoch 91/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7486 - accuracy: 0.6125 - val_loss: 1.9762 - val_accuracy: 0.5471\n","Epoch 92/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7639 - accuracy: 0.6107 - val_loss: 2.1072 - val_accuracy: 0.5163\n","Epoch 93/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7608 - accuracy: 0.6177 - val_loss: 2.0854 - val_accuracy: 0.5327\n","Epoch 94/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7571 - accuracy: 0.6186 - val_loss: 1.9084 - val_accuracy: 0.5751\n","Epoch 95/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7659 - accuracy: 0.6153 - val_loss: 2.0231 - val_accuracy: 0.5546\n","Epoch 96/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7328 - accuracy: 0.6232 - val_loss: 1.8947 - val_accuracy: 0.5886\n","Epoch 97/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7374 - accuracy: 0.6215 - val_loss: 1.9526 - val_accuracy: 0.5676\n","Epoch 98/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7470 - accuracy: 0.6165 - val_loss: 2.0286 - val_accuracy: 0.5536\n","Epoch 99/100\n","301/301 [==============================] - 5s 15ms/step - loss: 1.7354 - accuracy: 0.6249 - val_loss: 1.9521 - val_accuracy: 0.5737\n","Epoch 100/100\n","301/301 [==============================] - 4s 13ms/step - loss: 1.7456 - accuracy: 0.6144 - val_loss: 2.0282 - val_accuracy: 0.5471\n"]}]}]}